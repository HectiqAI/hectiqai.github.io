{"version":3,"sources":["posts/VoxMediaPost.react.js","components/charts/EfficientNetScalingGraph.react.js","components/charts/EfficientNetGraph.react.js","components/charts/EfficientNetGraphController.react.js","posts/iOSPost.react.js","posts/posts.react.js","posts/DAMPost.react.js","posts/EfficientNetPost.react.js","posts/DecadeOfSciencePost.react.js","posts/CatherineJoinsTeamPost.react.js","components/Header.react.js","components/NavBar.react.js","components/TextCard.react.js","components/MainCard.react.js","components/Footer.react.js","components/Meta.react.js","components/RowPost.react.js","pages/HomePage.react.js","posts/metatags.react.js","routes.js","pages/PostPage.react.js","pages/errors/ErrorPage.react.js","pages/errors/404.react.js","App.react.js","index.js"],"names":["VoxMediaPost","props","react_default","a","createElement","class","href","data-fancybox","src","alt","EfficientNetScalingGraph","series","push","name","data","react_apexcharts_min_default","options","chart","id","toolbar","show","zoom","enabled","dataLabels","formatter","val","opt","i","dataPointIndex","xaxis","title","text","yaxis","grid","borderColor","lines","tooltip","type","height","defaultProps","EfficientNetGraph","_this","Object","classCallCheck","this","_super","call","state","width","depth","resolution","prevProps","setState","drawChart","svgCanvas","d3","refs","efficientnetgraph","append","attr","maxDepth","concat","_this2","selectAll","colors","resolutionScale","nodes","x","select","join","style","d","ref","React","Component","EfficientNetGraphController","widthRange","toConsumableArray","Array","keys","map","value","step","depthRange","resRange","console","log","dist_default","range","onChange","EfficientNetGraph_react","assign","iOSPost","date","summary","authors","by","content","playsinline","autoplay","muted","loop","hide","data-toggle","data-from","data-to","data-aos","data-aos-id","moreInfo","excerpt","className","EfficientNetScalingGraph_react","EfficientNetGraphController_react","frontImgUrl","frontWideImg","frontImgUrlWide","credits","dark","Header","marginTop","NavBar","boxes","react_router_dom","to","data-target","aria-controls","aria-expanded","aria-label","data-offset","LinkCard","bg","children","DivCard","TextCard","str","length","Container","containerProps","substring","MainCard","Footer","Meta","_ref","objectWithoutProperties","url","lib_default","property","metaImage","twitterCard","twitterDescription","metaImageWidth","metaImageHeight","RowPost","header","HomePage","result","filter","o","Date","now","react","NavBar_react","Header_react","slice","MainCard_react","_ref2","TextCard_react","_ref3","RowPost_react","metaTags","voxmedia","twitterImage","metaDescription","routes","path","strict","exact","isPrivate","PostPage","postId","match","params","postid","find","react_router","data-jarallax","data-speed","backgroundImage","dangerouslySetInnerHTML","__html","ErrorPage","description","Error404Page","ErrorPage_react","App","basename","process","arguments","undefined","isDemoShop","render","component","Errors","rootElement","document","getElementById","Error","ReactDOM","App_react"],"mappings":"4PA0OeA,MAvOf,SAAsBC,GACpB,OACMC,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCACTH,EAAAC,EAAAC,cAAA,gaAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,sCAGAH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,QAAT,2MAGAH,EAAAC,EAAAC,cAAA,KAAGE,KAAK,8CAA8CC,iBAAA,GACpDL,EAAAC,EAAAC,cAAA,OAAKI,IAAI,2DAA2DH,MAAM,YAAYI,IAAI,SAE5FP,EAAAC,EAAAC,cAAA,KAAGC,MAAM,QAAT,2UACqUH,EAAAC,EAAAC,cAAA,+BADrU,+TAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,0CAAT,8LAIFH,EAAAC,EAAAC,cAAA,gSAGAF,EAAAC,EAAAC,cAAA,0JAGAF,EAAAC,EAAAC,cAAA,8UAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,2BACRH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,mEAIFF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,2GAKJF,EAAAC,EAAAC,cAAA,6cAIAF,EAAAC,EAAAC,cAAA,qQAIAF,EAAAC,EAAAC,cAAA,gVACsUF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,aAAZ,8GAEtUH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,yEAIAH,EAAAC,EAAAC,cAAA,qcAIAF,EAAAC,EAAAC,cAAA,ygBAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,0CAAT,oLAIFH,EAAAC,EAAAC,cAAA,qaAIAF,EAAAC,EAAAC,cAAA,UAAQC,MAAM,oCACZH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,4CAA4CG,IAAI,iCAAiCC,IAAI,QAChGP,EAAAC,EAAAC,cAAA,cAAYC,MAAM,8BAAlB,6OAIFH,EAAAC,EAAAC,cAAA,gdAGAF,EAAAC,EAAAC,cAAA,+UAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,0CAAT,uIAIFH,EAAAC,EAAAC,cAAA,2qBAGAF,EAAAC,EAAAC,cAAA,2PAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,4CAIAH,EAAAC,EAAAC,cAAA,oMAC0LF,EAAAC,EAAAC,cAAA,iDAD1L,wFAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sBACRH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,iCAAZ,kEAAiHH,EAAAC,EAAAC,cAAA,WACjHF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,cAAZ,oOACiOH,EAAAC,EAAAC,cAAA,iDADjO,OAKJF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,iCAAZ,kDAAiGH,EAAAC,EAAAC,cAAA,WACjGF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,cAAZ,+FAKJH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,iCAAZ,kDAAiGH,EAAAC,EAAAC,cAAA,WACjGF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,cAAZ,yFAKJH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,iCAAZ,kDAAiGH,EAAAC,EAAAC,cAAA,WACjGF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,cAAZ,6HAC0HH,EAAAC,EAAAC,cAAA,8BAD1H,uEAONF,EAAAC,EAAAC,cAAA,ghBAGAF,EAAAC,EAAAC,cAAA,UAAQC,MAAM,oCACZH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,+BAA+BG,IAAI,+BAA+BC,IAAI,QACjFP,EAAAC,EAAAC,cAAA,cAAYC,MAAM,8BAAlB,uLAIFH,EAAAC,EAAAC,cAAA,8hBAIAF,EAAAC,EAAAC,cAAA,UAAQC,MAAM,gDACZH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,iDAAiDG,IAAI,gCAAgCC,IAAI,QACpGP,EAAAC,EAAAC,cAAA,cAAYC,MAAM,8BAAlB,yJAKFH,EAAAC,EAAAC,cAAA,qEAIAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,2BACRH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,8EAIFF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,oGAIFF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,UACRH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,iBAEXH,EAAAC,EAAAC,cAAA,mKAMJF,EAAAC,EAAAC,cAAA,yRAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,0CAAT,4JAKFH,EAAAC,EAAAC,cAAA,kWC3NhB,SAASM,EAAyBT,GAEhC,IAAIU,EAAS,GACbA,EAAOC,KAAK,CACVC,KAAM,eACNC,KAAM,CAAC,CAAC,IAAI,MAAO,CAAC,IAAI,MAAO,CAAC,IAAI,MAAO,CAAC,GAAG,MAAO,CAAC,GAAG,IAAO,CAAC,GAAG,MAAO,CAAC,GAAG,MAAO,CAAC,GAAG,SA4C7F,OAAQZ,EAAAC,EAAAC,cAACW,EAAAZ,EAAD,CACGa,QA1CG,CACZC,MAAO,CACLC,GAAI,wBACJC,QAAS,CACPC,MAAM,GAERC,KAAM,CACJC,SAAS,IAIbC,WAAY,CACVD,SAAS,EACTE,UAAW,SAASC,EAAKC,EAAKC,GAC1B,MAAO,IAAID,EAAIE,iBAGrBC,MAAO,CACLC,MAAO,CACLC,KAAK,oCAGTC,MAAO,CACLF,MAAO,CACLC,KAAK,gCAGTE,KAAM,CACJC,YAAa,UACbL,MAAO,CACHM,MAAO,CACHf,MAAM,KAIhBgB,QAAS,CACPd,SAAS,IAODX,OAAQA,EACR0B,KAAK,OACLC,OAAQrC,EAAMqC,SAS5B5B,EAAyB6B,aAAe,CACtCD,OAAQ,KAGK5B,odClET8B,0CAEF,SAAAA,EAAYvC,GAAM,IAAAwC,EAAA,OAAAC,OAAAC,EAAA,EAAAD,CAAAE,KAAAJ,IAChBC,EAAAI,EAAAC,KAAAF,KAAM3C,IACD8C,MAAQ,CACXC,MAAO/C,EAAM+C,MACbC,MAAOhD,EAAMgD,MACbC,WAAYjD,EAAMiD,YALJT,iEAQCU,GAEbP,KAAK3C,MAAM+C,QAAUG,EAAUH,OACjCJ,KAAKQ,SAAS,CAACJ,MAAOJ,KAAK3C,MAAM+C,QAE/BJ,KAAK3C,MAAMgD,QAAUE,EAAUF,OACjCL,KAAKQ,SAAS,CAACH,MAAOL,KAAK3C,MAAMgD,QAE/BL,KAAK3C,MAAMiD,aAAeC,EAAUD,YACtCN,KAAKQ,SAAS,CAACF,WAAYN,KAAK3C,MAAMiD,aAExCN,KAAKS,wDAQL,IALA,IAAMC,EAAYC,IAAUX,KAAKY,KAAKC,mBACjCC,OAAO,OACPC,KAAK,QAAS,QACdA,KAAK,SAAUf,KAAK3C,MAAMqC,QAEtBX,EAAI,EAAGA,EAAIiB,KAAK3C,MAAM2D,SAAUjC,IACvC2B,EAAUI,OAAV,KAAsBC,KAAK,KAA3B,OAAAE,OAAwClC,IACxC2B,EAAUI,OAAV,KAAsBC,KAAK,KAA3B,YAAAE,OAA6ClC,IAE7CiB,KAAKS,gDAMP,IAJU,IAAAS,EAAAlB,KAEJU,EAAYC,IAAUX,KAAKY,KAAKC,mBAAmBM,UAAU,OAC/DC,EAAS,CAAC,UAAU,UAAU,UAAU,UAAU,UAAU,UAAU,WACjErC,EAAI,EAAGA,EAAIiB,KAAK3C,MAAM2D,SAAUjC,IAAK,CAE5C,IAAMsC,GAAmBrB,KAAK3C,MAAMiD,WAAW,GAAG,GAE9CgB,EAAQ,CAAC,CAAClB,MAAW,GAAHrB,EAAO,EAAG,EAAI,IAAFA,EAAMiB,KAAKG,MAAMC,MACrCmB,EAAC,GAAAN,OAAO,IAAFlC,GAAOiB,KAAKG,MAAME,MAAM,GAA7B,KACDX,OAAY,GAAHX,EAAQiB,KAAK3C,MAAMqC,OAAO2B,EAAkBrB,KAAK3C,MAAMqC,OAAO2B,EAAgBtC,EAAEiB,KAAK3C,MAAMqC,OAAO2B,GAAiBrB,KAAKG,MAAME,MAAM,KAG3JK,EAAUc,OAAV,aAAAP,OAA8BlC,IAC7B+B,OAAO,QACPA,OAAO,UACP5C,KAAKoD,GACLG,KAAK,QACLV,KAAK,KALN,SAAAE,OAKqBlC,IACpBgC,KAAK,UAAW,CAAC,EAAG,EAAG,GAAI,KAC3BA,KAAK,OAAQ,GACbA,KAAK,OAAQ,IACbA,KAAK,cAAe,IACpBA,KAAK,eAAgB,IACrBA,KAAK,SAAU,sBACfD,OAAO,QACPC,KAAK,IAAKJ,MAAU,CAAC,CAAC,EAAG,GAAI,CAAC,EAAG,IAAK,CAAC,GAAI,OAC3Ce,MAAM,OAAQ,QACdX,KAAK,SAAU,QAEHL,EAAUc,OAAV,aAAAP,OAA8BlC,IAC1CoC,UAAU,QACVjD,KAAKoD,GACLG,KAAK,QACHC,MAAM,SAAU,QAChBX,KAAK,aALK,cAAAE,OAKuBlC,EALvB,MAMVgC,KAAK,KANK,GAAAE,OAMY,KAAPlC,EAAE,KAAUiB,KAAKG,MAAME,MAAM,GANlC,MAOVU,KAAK,KAAMf,KAAK3C,MAAMqC,OAAO,GAC7BqB,KAAK,KARK,GAAAE,OAQY,KAAPlC,EAAE,KAAUiB,KAAKG,MAAME,MAAM,GARlC,MASVU,KAAK,KAAMf,KAAK3C,MAAMqC,OAAO,GAGnBgB,EAAUc,OAAV,QAAAP,OAAyBlC,IACrCoC,UAAU,QACVjD,KAAKoD,GACLG,KAAK,QACHV,KAAK,QAAS,SAAAY,GAAC,OAAEA,EAAEvB,QACnBW,KAAK,SAAU,SAAAY,GAAC,OAAEA,EAAEjC,SACpBgC,MAAM,OAAQN,EAAOrC,IACrBgC,KAAK,IAAK,SAAAY,GAAC,OAAEA,EAAEJ,IACfR,KAAK,IAAK,SAAAY,GAAC,OAAGT,EAAK7D,MAAMqC,OAAO,EAAEiC,EAAEjC,OAAO,sCAMvC,OAAOpC,EAAAC,EAAAC,cAAA,OAAKoE,IAAI,6BA3FCC,IAAMC,WAyGtClC,EAAkBD,aAAe,CAC/BD,OAAQ,IACRW,MAAO,EACPW,SAAU,GACVZ,MAAO,GACPE,WAAY,GAGCV,qeChHTmC,0CAEF,SAAAA,EAAY1E,GAAM,IAAAwC,EAAA,OAAAC,OAAAC,EAAA,EAAAD,CAAAE,KAAA+B,IAChBlC,EAAAI,EAAAC,KAAAF,KAAM3C,IACD8C,MAAQ,CACXC,MAAO,EACPC,MAAO,EACPC,WAAY,GALET,uDAST,IAAAqB,EAAAlB,KAEDgC,EAAalC,OAAAmC,EAAA,EAAAnC,CAAIoC,MAAM,GAAGC,QAAQC,IAAK,SAAAT,GAAC,MAAG,CAACU,MAAMV,EAAGW,KAAK,KAC1DC,EAAazC,OAAAmC,EAAA,EAAAnC,CAAIoC,MAAM,GAAGC,QAAQC,IAAK,SAAAT,GAAC,MAAG,CAACU,MAAMV,EAAGW,KAAK,KAC1DE,EAAW1C,OAAAmC,EAAA,EAAAnC,CAAIoC,MAAM,GAAGC,QAAQC,IAAK,SAAAT,GAAC,MAAG,CAACU,MAAMV,EAAGW,KAAK,KAE9D,OADAG,QAAQC,IAAI1C,KAAKG,OACT7C,EAAAC,EAAAC,cAAA,OAAKC,MAAM,OACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,YACTH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sDAAV,SAGAH,EAAAC,EAAAC,cAACmF,EAAApF,EAAD,CACE8E,MAAOrC,KAAKG,MAAMC,MAClBwC,MAAOZ,EACPa,SAAU,SAAAR,GAAK,OAAGnB,EAAKV,SAAS,CAACJ,MAAMiC,OAEzC/E,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sDAAV,SAGAH,EAAAC,EAAAC,cAACmF,EAAApF,EAAD,CACE8E,MAAOrC,KAAKG,MAAME,MAClBuC,MAAOL,EACPM,SAAU,SAAAR,GAAK,OAAGnB,EAAKV,SAAS,CAACH,MAAMgC,OAEzC/E,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sDAAV,cAGAH,EAAAC,EAAAC,cAACmF,EAAApF,EAAD,CACE8E,MAAOrC,KAAKG,MAAMG,WAClBsC,MAAOJ,EACPK,SAAU,SAAAR,GAAK,OAAGnB,EAAKV,SAAS,CAACF,WAAW+B,QAGhD/E,EAAAC,EAAAC,cAAA,OAAKC,MAAM,oBACTH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sDAAV,SAGAH,EAAAC,EAAAC,cAACsF,EAADhD,OAAAiD,OAAA,GAAuB/C,KAAKG,MAA5B,CAAmCT,OAAQ,OAC1CpC,EAAAC,EAAAC,cAAA,KAAGC,MAAM,gCAAT,uCAjDuBoE,IAAMC,WA+DhDC,EAA4BpC,aAAe,GAI5BoC,QC4CAiB,IChCA9E,EA9EF,CACX,CACE+E,KAAM,iBACN/D,MAAO,6DACPgE,QAAS,GACTC,QAAS,CAAC,mBACVC,GAAI,+CACJC,QDZJ,SAAiBhG,GACf,OAAQC,EAAAC,EAAAC,cAAA,WACEF,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCACTH,EAAAC,EAAAC,cAAA,6HAIAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sCAAV,iBACAH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,6BACAH,EAAAC,EAAAC,cAAA,uOAGAF,EAAAC,EAAAC,cAAA,mbAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,4BACAH,EAAAC,EAAAC,cAAA,4mBAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,gBACAH,EAAAC,EAAAC,cAAA,0lBAGAF,EAAAC,EAAAC,cAAA,kMAIAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sCAAV,cACAH,EAAAC,EAAAC,cAAA,4RAIAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,0BACAH,EAAAC,EAAAC,cAAA,iaAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,4CACAH,EAAAC,EAAAC,cAAA,qeAGAF,EAAAC,EAAAC,cAAA,2OAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,6BACAH,EAAAC,EAAAC,cAAA,iaAGAF,EAAAC,EAAAC,cAAA,kjBAIAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,6CACAH,EAAAC,EAAAC,cAAA,wSAGAF,EAAAC,EAAAC,cAAA,8RAIAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,sCAAV,eACAH,EAAAC,EAAAC,cAAA,sBAGAF,EAAAC,EAAAC,cAAA,WACEF,EAAAC,EAAAC,cAAA,OAAKC,MAAM,0BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,4BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,yBACTH,EAAAC,EAAAC,cAAA,SAAO8F,aAAW,EAACC,UAAQ,EAACC,OAAK,EAACC,MAAI,EAAC7F,IAAI,2BAA2B6B,KAAK,YAAYhC,MAAM,gBAAgBI,IAAI,QACjHP,EAAAC,EAAAC,cAAA,OAAKI,IAAI,iCAAiCH,MAAM,YAAYI,IAAI,UAGpEP,EAAAC,EAAAC,cAAA,OAAKC,MAAM,4BACTH,EAAAC,EAAAC,cAAA,4BAGAF,EAAAC,EAAAC,cAAA,mDAEAF,EAAAC,EAAAC,cAAA,OAAKC,MAAM,UACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,gBAEXH,EAAAC,EAAAC,cAAA,mDAIFF,EAAAC,EAAAC,cAAA,OAAKC,MAAM,UACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,eAEXH,EAAAC,EAAAC,cAAA,6BAIFF,EAAAC,EAAAC,cAAA,OAAKC,MAAM,UACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2DACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,mBAEXH,EAAAC,EAAAC,cAAA,iDCvFxBc,GAAI,UACJoF,MAAM,GAER,CACET,KAAM,gBACN/D,MAAO,yEACPgE,QAAS,yRACTC,QAAS,CAAC,mBACVC,GAAI,+CACJC,QCrBJ,SAAiBhG,GACf,OAAQC,EAAAC,EAAAC,cAAA,WACEF,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCACTH,EAAAC,EAAAC,cAAA,wFAC8EF,EAAAC,EAAAC,cAAA,KAAGE,KAAK,sCAAR,gCAD9E,yXAGAJ,EAAAC,EAAAC,cAAA,+OASRF,EAAAC,EAAAC,cAAA,WAASC,MAAM,8BACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCAETH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,mDACqCF,EAAAC,EAAAC,cAAA,WACnCF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,mBAAZ,0BAEFH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,OACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,YACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,iCACTH,EAAAC,EAAAC,cAAA,QAAMC,MAAM,uBAAuBkG,cAAY,UAAUC,YAAU,IAAIC,UAAQ,KAAKC,YAAA,EAASC,cAAY,cAAzG,KACAzG,EAAAC,EAAAC,cAAA,QAAMC,MAAM,0BAAZ,MAEFH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,mCAAT,uFAKFH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,YACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,iCACTH,EAAAC,EAAAC,cAAA,QAAMC,MAAM,uBAAuBkG,cAAY,UAAUC,YAAU,IAAIC,UAAQ,IAAIC,YAAA,EAASC,cAAY,cAAxG,KACAzG,EAAAC,EAAAC,cAAA,QAAMC,MAAM,0BAAZ,QAEFH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,mCAAT,qFAKFH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,YACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,iCACTH,EAAAC,EAAAC,cAAA,QAAMC,MAAM,uBAAuBkG,cAAY,UAAUC,YAAU,IAAIC,UAAQ,KAAKC,YAAA,EAASC,cAAY,cAAzG,KACAzG,EAAAC,EAAAC,cAAA,QAAMC,MAAM,0BAAZ,MAEFH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,mCAAT,+DAUdH,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCACTH,EAAAC,EAAAC,cAAA,8MAGAF,EAAAC,EAAAC,cAAA,qJAC2IF,EAAAC,EAAAC,cAAA,gBAD3I,6WAGAF,EAAAC,EAAAC,cAAA,yYAMRF,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCACTH,EAAAC,EAAAC,cAAA,UAAQC,MAAM,oCACZH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uBAAuBG,IAAI,6BAA6BC,IAAI,QACvEP,EAAAC,EAAAC,cAAA,cAAYC,MAAM,8BAAlB,2WASVH,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCAETH,EAAAC,EAAAC,cAAA,oaAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,eACVH,EAAAC,EAAAC,cAAA,cAAYC,MAAM,mBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,oCAAT,oIAIFH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,eACVH,EAAAC,EAAAC,cAAA,iRAGAF,EAAAC,EAAAC,cAAA,kTD3FhBc,GAAI,UACJ0F,SAAU,yFACVN,MAAM,GAER,CACET,KAAM,gBACN/D,MAAO,+CACP+E,QAAS,qLACTf,QAAS,qLACTC,QAAS,CAAC,oBACVC,GAAI,mBACJC,QE9BJ,SAA0BhG,GACxB,OAAQC,EAAAC,EAAAC,cAAA,WACIF,EAAAC,EAAAC,cAAA,OAAK0G,UAAU,gCACb5G,EAAAC,EAAAC,cAAC2G,EAAD,CAA0BzE,OAAQ,OAEpCpC,EAAAC,EAAAC,cAAA,OAAK0G,UAAU,gCACb5G,EAAAC,EAAAC,cAAA,OAAK0G,UAAU,gCACb5G,EAAAC,EAAAC,cAAC4G,EAAD,UFwBd9F,GAAI,eACJoF,MAAM,GAER,CACET,KAAM,iBACN/D,MAAO,gFACP+E,QAAS,wPACTI,YAAa,sCACbC,aAAc,2CACdC,gBAAiB,sCACjBrB,QAAS,knBACTC,QAAS,CAAC,kBACVqB,QAAS,0IACTR,SAAU,sIACVZ,GAAI,yDACJ9E,GAAI,WACJ+E,QAASjG,GAEX,CACE6F,KAAM,oBACN/D,MAAO,iDACP+E,QAAS,wKACTI,YAAa,8BACbE,gBAAiB,mCACjBrB,QAAS,wKACTE,GAAI,yDACJD,QAAS,CAAC,kBACVE,QG7DW,SAA6BhG,GAE1C,OACIC,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCACTH,EAAAC,EAAAC,cAAA,igBAGAF,EAAAC,EAAAC,cAAA,8MACoMF,EAAAC,EAAAC,cAAA,UAAQC,MAAM,gBAAd,4BADpM,+IAGAH,EAAAC,EAAAC,cAAA,qpBAGAF,EAAAC,EAAAC,cAAA,uiBAIAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,+BAIAH,EAAAC,EAAAC,cAAA,mfACqdF,EAAAC,EAAAC,cAAA,2BADrd,6EAGAF,EAAAC,EAAAC,cAAA,4OACwNF,EAAAC,EAAAC,cAAA,2BADxN,ySAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,oBACVH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,iCAAV,cACAH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,yBAGAH,EAAAC,EAAAC,cAAA,onBAGAF,EAAAC,EAAAC,cAAA,yeAGAF,EAAAC,EAAAC,cAAA,+SAGAF,EAAAC,EAAAC,cAAA,OAAKI,IAAI,yCAAyCH,MAAM,wCAAwCI,IAAI,QACpGP,EAAAC,EAAAC,cAAA,2tBAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,0CAAT,iLAIFH,EAAAC,EAAAC,cAAA,kiBAGAF,EAAAC,EAAAC,cAAA,yYAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,oBACVH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,iCAAV,eACAH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,oBAAV,aAGAH,EAAAC,EAAAC,cAAA,ogBAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,0CAAT,2FAIFH,EAAAC,EAAAC,cAAA,sOACkNF,EAAAC,EAAAC,cAAA,KAAGE,KAAK,sDAAR,2DADlN,6SAGAJ,EAAAC,EAAAC,cAAA,OAAKI,IAAI,uCAAuCH,MAAM,sCAAsCI,IAAI,QAChGP,EAAAC,EAAAC,cAAA,qSAGAF,EAAAC,EAAAC,cAAA,8HACoHF,EAAAC,EAAAC,cAAA,qBADpH,gFACgNF,EAAAC,EAAAC,cAAA,QAAMC,MAAM,gBAAZ,iHADhN,+SAGAH,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,sCAAT,oDAKFH,EAAAC,EAAAC,cAAA,o7BAGAF,EAAAC,EAAAC,cAAA,gZACiYF,EAAAC,EAAAC,cAAA,kBADjY,uNAC4lBF,EAAAC,EAAAC,cAAA,qCAD5lB,YAGAF,EAAAC,EAAAC,cAAA,udAGAF,EAAAC,EAAAC,cAAA,wiBAC+gBF,EAAAC,EAAAC,cAAA,yDAD/gB,oHAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,oBACVH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,iCAAV,eACAH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,oBAAV,wBAGAH,EAAAC,EAAAC,cAAA,u1BAGAF,EAAAC,EAAAC,cAAA,4TAGAF,EAAAC,EAAAC,cAAA,wkBAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,2CAAT,qHAIFH,EAAAC,EAAAC,cAAA,mwBAGAF,EAAAC,EAAAC,cAAA,icAGAF,EAAAC,EAAAC,cAAA,8uBAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,oBACVH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,iCAAV,cACAH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,oBAAV,kCAGAH,EAAAC,EAAAC,cAAA,snBAGAF,EAAAC,EAAAC,cAAA,+QAGAF,EAAAC,EAAAC,cAAA,+QAGAF,EAAAC,EAAAC,cAAA,yJAGAF,EAAAC,EAAAC,cAAA,qVAGAF,EAAAC,EAAAC,cAAA,y7BAGAF,EAAAC,EAAAC,cAAA,yXAGAF,EAAAC,EAAAC,cAAA,cAAYC,MAAM,wBAChBH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,0CAAT,oIAIFH,EAAAC,EAAAC,cAAA,wXAGAF,EAAAC,EAAAC,cAAA,8RACoRF,EAAAC,EAAAC,cAAA,oEADpR,4CAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,iFAGAH,EAAAC,EAAAC,cAAA,yGAC+FF,EAAAC,EAAAC,cAAA,KAAGE,KAAK,qFAAR,wEAD/F,mIAGAJ,EAAAC,EAAAC,cAAA,4HACkHF,EAAAC,EAAAC,cAAA,2BADlH,kPACsXF,EAAAC,EAAAC,cAAA,oCADtX,8OAGAF,EAAAC,EAAAC,cAAA,0IACgIF,EAAAC,EAAAC,cAAA,sHADhI,gJAGAF,EAAAC,EAAAC,cAAA,ueACwdF,EAAAC,EAAAC,cAAA,iCADxd,kBACkgBF,EAAAC,EAAAC,cAAA,2BADlgB,iFAGAF,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yBAAV,gDAGAH,EAAAC,EAAAC,cAAA,6ZAGAF,EAAAC,EAAAC,cAAA,0nBAGAF,EAAAC,EAAAC,cAAA,oNAC0MF,EAAAC,EAAAC,cAAA,wBAD1M,uSACggBF,EAAAC,EAAAC,cAAA,wCADhgB,6CAGAF,EAAAC,EAAAC,cAAA,mUACyTF,EAAAC,EAAAC,cAAA,UAAQC,MAAM,aAAd,8BADzT,+GAGAH,EAAAC,EAAAC,cAAA,yeAGAF,EAAAC,EAAAC,cAAA,0jBAGAF,EAAAC,EAAAC,cAAA,wMH3IVc,GAAI,0BAEN,CACE2E,KAAM,qBACN/D,MAAO,kCACP+E,QAAS,uEACTM,gBAAiB,GACjBlB,QIrEW,SAAgChG,GAE7C,OAAQC,EAAAC,EAAAC,cAAA,WAASC,MAAM,qBACbH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,uCACTH,EAAAC,EAAAC,cAAA,sMAEAF,EAAAC,EAAAC,cAAA,OAAKI,IAAI,mCAAmCH,MAAM,mEAAmEI,IAAI,QACzHP,EAAAC,EAAAC,cAAA,MAAIC,MAAM,iCAAV,aACAH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,kBAAT,mcJ4DdgH,MAAM,EACNnG,GAAI,4BAEN,CACE2E,KAAM,oBACN/D,MAAO,2BACP+E,QAAS,oGACTM,gBAAiB,GACjBjG,GAAI,gBK7DOoG,MAjBf,SAAgBrH,GACd,OAAQC,EAAAC,EAAAC,cAAA,WAASC,MAAM,4CAA4CiE,MAAO,CAACiD,WAAW,KAC5ErH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,8BACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,yCACTH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,yCAAV,cAGAH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,2BAAT,0CCiBHmH,MAzBf,SAAgBvH,GACd,OAAQC,EAAAC,EAAAC,cAAA,OAAKC,MAAK,2BAAAwD,OAA8B5D,EAAMoH,KAAO,cAAe,iBAClEnH,EAAAC,EAAAC,cAAA,OAAKC,MAAQJ,EAAMwH,MAAQ,YAAa,mBACtCvH,EAAAC,EAAAC,cAACsH,EAAA,EAAD,CAAMrH,MAAM,eAAesH,GAAG,KAC5BzH,EAAAC,EAAAC,cAAA,OAAKI,IAAI,wBAAwBH,MAAM,mBAAmBI,IAAI,SAGhEP,EAAAC,EAAAC,cAAA,UAAQC,MAAM,iBAAiBgC,KAAK,SAASkE,cAAY,WAAWqB,cAAY,kBAAkBC,gBAAc,iBAAiBC,gBAAc,QAAQC,aAAW,qBAChK7H,EAAAC,EAAAC,cAAA,QAAMC,MAAM,yBAGdH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,2BAA2Ba,GAAG,kBAEvChB,EAAAC,EAAAC,cAAA,UAAQC,MAAM,iBAAiBgC,KAAK,SAASkE,cAAY,WAAWqB,cAAY,kBAAkBC,gBAAc,iBAAiBC,gBAAc,QAAQC,aAAW,qBAChK7H,EAAAC,EAAAC,cAAA,KAAGC,MAAM,aAGXH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,mDAAmDC,KAAK,yBAAyBiG,cAAY,gBAAgByB,cAAY,KAAlI,eCZhB,SAASC,EAAShI,GAChB,OAAQC,EAAAC,EAAAC,cAACsH,EAAA,EAAD,CAAMrH,MAAK,0CAAAwD,OAA4C5D,EAAMiI,IAAMP,GAAI1H,EAAM0H,IAC1E1H,EAAMkI,UAGnB,SAASC,EAAQnI,GACf,OAAQC,EAAAC,EAAAC,cAAA,OAAKC,MAAK,0CAAAwD,OAA4C5D,EAAMiI,KACzDjI,EAAMkI,UA2CJE,MAtCf,SAAkBpI,GAChB,IAAI+D,EAAS,CACXjC,KAAM,aACNmG,GAAI,WACJpG,MAAO,aAGL7B,EAAMoH,OACRrD,EAAS,CACPjC,KAAM,gBACNmG,GAAI,eACJpG,MAAO,eAIX,IA/BgBwG,EAAKC,EA+BjBC,EAAYJ,EACZK,EAAiB,CAACP,GAAGlE,EAAOkE,IAQhC,OAPIjI,EAAMgG,UACRuC,EAAYP,EACZQ,EAAiB,CAACP,GAAGlE,EAAOkE,GAAIP,GAAE,UAAA9D,OAAW5D,EAAMiB,MAK7ChB,EAAAC,EAAAC,cAAA,OAAKC,MAAM,mCACTH,EAAAC,EAAAC,cAACoI,EAAcC,EACXxI,EAAMgH,YACN/G,EAAAC,EAAAC,cAAA,OAAKC,MAAM,yBAAyBG,IAAMP,EAAMgH,YAAcxG,IAAI,QAClE,KACFP,EAAAC,EAAAC,cAAA,OAAKC,MAAM,qBACTH,EAAAC,EAAAC,cAAA,MAAIC,MAAK,uBAAAwD,OAAyBG,EAAOjC,OAAS9B,EAAM4F,MACxD3F,EAAAC,EAAAC,cAAA,MAAIC,MAAK,aAAAwD,OAAeG,EAAOlC,QAAW7B,EAAM6B,OAChD5B,EAAAC,EAAAC,cAAA,KAAGC,MAAK,QAAAwD,OAAUG,EAAOjC,OAAzB,KAhDEuG,EAgD2CrI,EAAM4G,SAAW5G,EAAM6F,QAhD7DyC,EAgDuE,IA/CnFD,EAAIC,OAASA,EAASD,EAAII,UAAU,EAAGH,EAAO,GAAK,MAAQD,QC+BvDK,MA5Bf,SAAkB1I,GAChB,IANgBqI,EAAKC,EAMjBvE,EAAS,CACXjC,KAAM,aACNmG,GAAI,WACJpG,MAAO,aAUT,OAPI7B,EAAMoH,OACRrD,EAAS,CACPjC,KAAM,gBACNmG,GAAI,eACJpG,MAAO,eAGH5B,EAAAC,EAAAC,cAAA,OAAKC,MAAM,UACTH,EAAAC,EAAAC,cAACsH,EAAA,EAAD,CAAMrH,MAAK,0CAAAwD,OAA4CG,EAAOkE,IAAMP,GAAE,UAAA9D,OAAY5D,EAAMiB,KACpFjB,EAAMgH,YACN/G,EAAAC,EAAAC,cAAA,OAAKC,MAAM,yBAAyBG,IAAMP,EAAMgH,YAAcxG,IAAI,QAClE,KACFP,EAAAC,EAAAC,cAAA,OAAKC,MAAM,qBACTH,EAAAC,EAAAC,cAAA,MAAIC,MAAK,uBAAAwD,OAAyBG,EAAOjC,OAAS9B,EAAM4F,MACxD3F,EAAAC,EAAAC,cAAA,MAAIC,MAAK,aAAAwD,OAAeG,EAAOlC,QAAW7B,EAAM6B,OAChD5B,EAAAC,EAAAC,cAAA,KAAGC,MAAK,QAAAwD,OAAUG,EAAOjC,OAAzB,KA3BEuG,EA2B2CrI,EAAM4G,SAAW5G,EAAM6F,QA3B7DyC,EA2BsE,IA1BlFD,EAAIC,OAASA,EAASD,EAAII,UAAU,EAAGH,EAAO,GAAK,MAAQD,QCDvD,SAASM,EAAO3I,GAC7B,OACMC,EAAAC,EAAAC,cAAA,UAAQC,MAAM,+CACZH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,6CACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,aACTH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,SACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,sBAAT,wBAKFH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,iBACTH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,iEACRH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,0CACRH,EAAAC,EAAAC,cAAA,KAAGE,KAAK,+BAA+BD,MAAM,wBAC3CH,EAAAC,EAAAC,cAAA,OAAKI,IAAI,uCAAuCH,MAAM,mBAAmBI,IAAI,UAGjFP,EAAAC,EAAAC,cAAA,MAAIC,MAAM,0CACRH,EAAAC,EAAAC,cAAA,KAAGE,KAAK,6CAA6CD,MAAM,wBACzDH,EAAAC,EAAAC,cAAA,OAAKI,IAAI,wCAAwCH,MAAM,mBAAmBI,IAAI,oCCnBrF,SAASoI,EAATC,GAAgC,IAAjBhH,EAAiBgH,EAAjBhH,MAAU7B,EAAOyC,OAAAqG,EAAA,EAAArG,CAAAoG,EAAA,WACzCE,EAAG,gCAAAnF,OAAmC5D,EAAMiB,IAEhD,OAAQhB,EAAAC,EAAAC,cAAC6I,EAAA9I,EAAD,KACED,EAAAC,EAAAC,cAAA,aAAQ0B,GACR5B,EAAAC,EAAAC,cAAA,QAAM8I,SAAS,eAAejD,QAAQ,mBACtC/F,EAAAC,EAAAC,cAAA,QAAM8I,SAAS,UAAUjD,QAAQ,YACjC/F,EAAAC,EAAAC,cAAA,QAAM8I,SAAS,WAAWjD,QAASnE,IACnC5B,EAAAC,EAAAC,cAAA,QAAM8I,SAAS,WAAWjD,QAAShG,EAAMkJ,YACzCjJ,EAAAC,EAAAC,cAAA,QAAM8I,SAAS,SAASjD,QAAS+C,IACjC9I,EAAAC,EAAAC,cAAA,QAAMS,KAAK,eAAeoF,QAAShG,EAAMmJ,cACzClJ,EAAAC,EAAAC,cAAA,QAAMS,KAAK,gBAAgBoF,QAASnE,IACpC5B,EAAAC,EAAAC,cAAA,QAAMS,KAAK,sBAAsBoF,QAAShG,EAAMoJ,qBAChDnJ,EAAAC,EAAAC,cAAA,QAAMS,KAAK,cAAcoF,QAAS+C,IAClC9I,EAAAC,EAAAC,cAAA,QAAMS,KAAK,eAAeoF,QAAQ,cAClC/F,EAAAC,EAAAC,cAAA,QAAM8I,SAAS,iBAAiBjD,QAAShG,EAAMqJ,iBAC/CpJ,EAAAC,EAAAC,cAAA,QAAM8I,SAAS,kBAAkBjD,QAAShG,EAAMsJ,mBCc7CC,MA1Bf,SAAiBvJ,GACf,IANgBqI,EAAKC,EAajBC,EAAY,MAMhB,OALIvI,EAAMgG,UACRuC,EAAY,KAINtI,EAAAC,EAAAC,cAAA,OAAKC,MAAM,6CACXH,EAAAC,EAAAC,cAAA,OAAKC,MAAM,WACRH,EAAAC,EAAAC,cAAA,MAAIC,MAAM,kCAAmCJ,EAAMwJ,QAClDvJ,EAAAC,EAAAC,cAACoI,EAAD,CAAWlI,KAAI,UAAAuD,OAAY5D,EAAMiB,KAAMhB,EAAAC,EAAAC,cAAA,MAAIC,MAAM,QAASJ,EAAM6B,QAChE5B,EAAAC,EAAAC,cAAA,KAAGC,MAAK,QAAAwD,OAhBZ,eAgBI,KAvBIyE,EAuByCrI,EAAM4G,QAvB1C0B,EAuBmD,IAtB/DD,EAAIC,OAASA,EAASD,EAAII,UAAU,EAAGH,EAAO,GAAK,MAAQD,KAwB5DpI,EAAAC,EAAAC,cAAA,OAAKC,MAAM,oCACTH,EAAAC,EAAAC,cAAA,KAAGC,MAAM,6BAA8BJ,EAAM4F,6cCvBnD6D,+JAGF,IAAMC,EAAS7I,EAAK8I,OAAO,SAAAC,GAAC,OAAKA,EAAEvD,OAAMsD,OAAO,SAAAC,GAE9C,OADc,IAAIC,KAAKD,EAAEhE,MACViE,KAAKC,QAEtB,OAAQC,EAAA,yBACEA,EAAA,cAACC,EAAD,CAAQxC,OAAK,EAACJ,MAAI,IAChB2C,EAAA,cAACE,EAAD,MACAF,EAAA,yBAAS3J,MAAM,kCACb2J,EAAA,qBAAK3J,MAAM,aACT2J,EAAA,qBAAK3J,MAAM,OACPsJ,EAAOQ,MAAM,EAAE,GAAGnF,IAAI,SAAA8D,GAAoC,IAAlCjD,EAAkCiD,EAAlCjD,KAAM/D,EAA4BgH,EAA5BhH,MAAO+E,EAAqBiC,EAArBjC,QAAY5G,EAASyC,OAAAqG,EAAA,EAAArG,CAAAoG,EAAA,4BAC1D,OAAQkB,EAAA,cAACI,EAAD1H,OAAAiD,OAAA,CAAUE,KAAMA,EAAM/D,MAAOA,EAAO+E,QAASA,GAAa5G,MAElE0J,EAAOQ,MAAM,EAAE,GAAGnF,IAAI,SAAAqF,EAAmC1I,GAAI,IAArCkE,EAAqCwE,EAArCxE,KAAM/D,EAA+BuI,EAA/BvI,MAAO+E,EAAwBwD,EAAxBxD,QAAY5G,EAAYyC,OAAAqG,EAAA,EAAArG,CAAA2H,EAAA,4BAC7D,OAAQL,EAAA,cAACM,EAAD5H,OAAAiD,OAAA,CAAUE,KAAMA,EAAM/D,MAAOA,EAAO+E,QAASA,GAAa5G,OAItE+J,EAAA,qBAAK3J,MAAM,oCACPsJ,EAAOQ,MAAM,GAAGnF,IAAI,SAAAuF,EAAmC5I,GAAI,IAArCkE,EAAqC0E,EAArC1E,KAAM/D,EAA+ByI,EAA/BzI,MAAO+E,EAAwB0D,EAAxB1D,QAAY5G,EAAYyC,OAAAqG,EAAA,EAAArG,CAAA6H,EAAA,4BAC3D,OAAQP,EAAA,cAACQ,EAAD9H,OAAAiD,OAAA,CAAUE,KAAMA,EAAM/D,MAAOA,EAAO+E,QAASA,GAAa5G,SAM5E+J,EAAA,cAACpB,EAAD,cA7BSnE,aCMRgG,EAZE,CACfC,SAAU,CACRvB,UAAW,gDACXG,eAAgB,MAChBC,gBAAiB,MACjBH,YAAa,sBACbuB,aAAc,gDACdtB,mBAAoB,wQACpBuB,gBAAiB,8sBCANC,EAJA,CACb,CAAEC,KAAM,IAAKjK,KAAM,OAAQkK,QAAQ,EAAOC,OAAM,EAAMC,WAAU,EAAMvG,UFmCzDgF,GElCb,CAAEoB,KAAM,iBAAkBjK,KAAM,OAAQkK,QAAQ,EAAOC,OAAM,EAAMC,WAAU,EAAMvG,kDCInF,SAAAwG,EAAYjL,GAAM,IAAAwC,EAAA,OAAAC,OAAAC,EAAA,EAAAD,CAAAE,KAAAsI,IAChBzI,EAAAI,EAAAC,KAAAF,KAAM3C,IACDkL,OAAS1I,EAAKxC,MAAMmL,MAAMC,OAAOC,OACtC7I,EAAK3B,KAAQA,EAAKyK,KAAK,SAAA1B,GAAC,OAAEA,EAAE3I,KAAOuB,EAAK0I,SAHxB1I,uDAMhB,OAAgB,MAAXG,KAAK9B,MAAiC,MAAnB8B,KAAK9B,KAAKmF,QACxB+D,EAAA,cAACwB,EAAA,EAAD,CAAU7D,GAAG,SAEfqC,EAAA,yBACEA,EAAA,cAACnB,EAADnG,OAAAiD,OAAA,CAAM7D,MAAOc,KAAK9B,KAAKgB,OAAW2I,EAAS7H,KAAK9B,KAAKI,MAErD8I,EAAA,cAACC,EAAD,CAAQxC,OAAK,IAEX7E,KAAK9B,KAAKoG,aACV8C,EAAA,yBAASyB,iBAAA,EAAcC,aAAW,KAAKrL,MAAM,2CAA2CiE,MAAO,CAACqH,gBAAe,OAAA9H,OAASjB,KAAK9B,KAAKoG,aAAnB,QAC7G,KAGJ8C,EAAA,yBAAS3J,MAAM,sBACb2J,EAAA,qBAAK3J,MAAM,aACT2J,EAAA,qBAAK3J,MAAM,8BACT2J,EAAA,qBAAK3J,MAAM,uCACT2J,EAAA,oBAAI3J,MAAM,kCAAkCuC,KAAK9B,KAAK+E,MACtDmE,EAAA,oBAAI3J,MAAM,8BACNuC,KAAK9B,KAAKgB,OAEZc,KAAK9B,KAAKkF,GAAKgE,EAAA,mBAAG3J,MAAM,oBAAT,MAAgCuC,KAAK9B,KAAKkF,IAAU,KACrEgE,EAAA,mBAAG3J,MAAM,uBAAuBuL,wBAAyB,CAACC,OAAQjJ,KAAK9B,KAAKgF,WAE5EkE,EAAA,oBAAI3J,MAAM,mBAKjBuC,KAAK9B,KAAKmF,UAGX+D,EAAA,yBAAS3J,MAAM,sBACb2J,EAAA,qBAAK3J,MAAM,cACa,MAAnBuC,KAAK9B,KAAKiF,SAAoC,MAAnBnD,KAAK9B,KAAKsG,SAAqC,MAApBxE,KAAK9B,KAAK8F,SAAkBoD,EAAA,oBAAI3J,MAAM,oBAAsB,KACrH2J,EAAA,qBAAK3J,MAAM,+DACPuC,KAAK9B,KAAKiF,QACViE,EAAA,qBAAK3J,MAAM,OACT2J,EAAA,qBAAK3J,MAAM,8BACT2J,EAAA,mBAAG3J,MAAM,cACLuC,KAAK9B,KAAKiF,QAAQwC,OAAO,EAAI,UAAW,WAG9CyB,EAAA,qBAAK3J,MAAM,+BACT2J,EAAA,mBAAG3J,MAAM,4BACNuC,KAAK9B,KAAKiF,QAAQ1B,KAAK,SAGtB,KAENzB,KAAK9B,KAAKsG,QACV4C,EAAA,qBAAK3J,MAAM,4BACT2J,EAAA,qBAAK3J,MAAM,8BACT2J,EAAA,mBAAG3J,MAAM,eAAT,YAIF2J,EAAA,qBAAK3J,MAAM,+BACT2J,EAAA,mBAAG3J,MAAM,+BAA+BuL,wBAAyB,CAACC,OAAQjJ,KAAK9B,KAAKsG,aAGhF,KAERxE,KAAK9B,KAAK8F,SACVoD,EAAA,qBAAK3J,MAAM,4BACT2J,EAAA,qBAAK3J,MAAM,8BACT2J,EAAA,mBAAG3J,MAAM,eAAT,eAIF2J,EAAA,qBAAK3J,MAAM,+BACT2J,EAAA,mBAAG3J,MAAM,+BAA+BuL,wBAAyB,CAACC,OAAQjJ,KAAK9B,KAAK8F,cAGhF,QAKlBoD,EAAA,cAACpB,EAAD,cAtFSnE,mdC2BRqH,+JA9BX,OAAQ9B,EAAA,qBAAK3J,MAAM,gCACT2J,EAAA,qBAAK3J,MAAM,uEACT2J,EAAA,qBAAK3J,MAAM,0CAET2J,EAAA,oBAAI3J,MAAM,0CACPuC,KAAK3C,MAAM6B,OAGdkI,EAAA,mBAAG3J,MAAM,+BACNuC,KAAK3C,MAAM8L,aAGd/B,EAAA,qBAAK3J,MAAM,eACT2J,EAAA,mBAAG3J,MAAM,kBAAkBC,KAAK,KAAhC,6BAhBImE,aCSTuH,MARf,SAAsB/L,GAIpB,OAAQ+J,EAAA,cAACiC,EAAD,CAAWnK,MAHP,0BAGqBiK,YAFf,keCyBLG,0CArBX,SAAAA,EAAYjM,GAAO,IAAAwC,EAAA,OAAAC,OAAAC,EAAA,EAAAD,CAAAE,KAAAsJ,IACjBzJ,EAAAI,EAAAC,KAAAF,KAAM3C,IACD8C,MAAQ,GAFIN,uDAOjB,OAAQvC,EAAAC,EAAAC,cAACsH,EAAA,EAAD,CAAeyE,SAAUC,IACvBlM,EAAAC,EAAAC,cAACoL,EAAA,EAAD,KACGX,EAAO7F,IAAI,WAAqE,IAAA8D,EAAAuD,UAAA9D,OAAA,QAAA+D,IAAAD,UAAA,GAAAA,UAAA,GAAX,GAAxDvB,EAAmEhC,EAAnEgC,KAAkBC,GAAiDjC,EAA5DmC,UAA4DnC,EAAjDiC,QAAQC,EAAyClC,EAAzCkC,MAAmBtG,GAAsBoE,EAAlCyD,WAAkCzD,EAAtBpE,WAAsB2H,UAAA9D,OAAA,GAAA8D,UAAA,GAC/E,OAAQnM,EAAAC,EAAAC,cAACoL,EAAA,EAAD,CAAOR,MAAOA,EAAOD,OAAQA,EAAQD,KAAMA,EAAM0B,OAAQ,SAACvM,GAClD,OAASC,EAAAC,EAAAC,cAACsE,EAAczE,QAE1CC,EAAAC,EAAAC,cAACoL,EAAA,EAAD,CAAOiB,UAAWC,aAflBjI,IAAMC,WCClBiI,UAAcC,SAASC,eAAe,SAE5C,IAAIF,EAGF,MAAM,IAAIG,MAAM,4CAFhBC,IAASP,OAAOtM,EAAAC,EAAAC,cAAC4M,EAAD,MAAQL","file":"static/js/main.4c964a77.chunk.js","sourcesContent":["import React from \"react\";\n\n\nfunction VoxMediaPost(props){\n  return (\n        <section class=\"pt-0 pt-md-0 pb-0\">\n          <div class=\"container\">\n            <div class=\"row justify-content-center\">\n              <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                <p>\n                  Have you ever felt like someone is spying on you all over the web? You shop for a pair of shoes online... and curiously enough Facebook Ads suggest that specific pair of shoes 10 min later. Not any pair of shoes: The EXACT same pair. What a coincidence! Turns out it’s not a coincidence at all. Welcome to the wonderful world of third-party cookies, real time bidding and data management platforms.\n                </p>\n                <h2 class=\"font-weight-bold mt-8\">\n                  Third-party vs first-party cookies\n                </h2>\n                <p class=\"mb-6\">\n                  If you have never heard of cookies, I would recommend that you watch this excellent video from Vox. It explains what a third-party cookie is and how it has been the core of the Ad industry for years.\n                </p>\n                <a href=\"https://www.youtube.com/watch?v=HFyaW50GFOs\" data-fancybox >\n                  <img src=\"https://img.youtube.com/vi/HFyaW50GFOs/maxresdefault.jpg\" class=\"img-fluid\" alt=\"...\"/>\n                </a>\n                <p class=\"mt-6\">\n                  A cookie is a little piece of data stored somewhere on your browser's device. Cookies were first invented to act as small and useful short term memory for a specific website. When you add an item to your cart, you’re expecting the site to remember it when you come back the next day. That short term memory is handled by a <i>first-party cookie</i>. It can only be accessed by the website itself. A third-party cookie can be set and accessed by a different website than the one it was created on. They are usually managed by ad networks or analytics tools.  And that’s exactly how they know you have placed a pair of yellow shoes in an online cart yesterday. \n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “A first-party cookie can only be accessed by the website that has set it. A third-party cookie can be set and accessed by a different website than the one it was created on.”\n                  </p>\n                </blockquote>\n                <p>\n                  That little piece of information (you shopping for a pair of shoes) is actually quite valuable. Ad networks collect all that data about you for ad retargeting. The next time they see you online, they can show you an ad specially tailored for you (spoiler: it involves shoes). \n                </p>\n                <p>\n                  Usage of third-party cookies raises serious concerns for user privacy and the good news is that the days of third-party cookies are numbered.\n                </p>\n                <p>\n                  With new regulation in place like General Data Protection Regulation in Europe (GPDR) and the new California Consumer Privacy Act (CCPA), using a third-party cookie while staying fully compliant will become more and more difficult. One of the key aspects of these regulations is the right to be forgotten. More crucially:\n                </p>\n                <ul class=\"list-unstyled mb-6 mt-7\">\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      Users can now refuse the sale of their personal data.\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                       Users are able to request deletion of their personal information at any time from a website. \n                    </p>\n                  </li>\n                </ul>\n                <p>\n                  Fulfilling those two simple requirements is nearly impossible with third-party data. For instance, when an ad network tracks your movements on your favorite shoes website, that information is automatically broadcasted and shared among a multitude of other systems. From that point on, the shoes website no longer controls where this data flows. Even if requested, it becomes practically impossible to delete this data or even to find where it went.\n                </p>\n\n                <p>\n                  Those issues have been known for some years now and most browsers (Edge, Chrome, Firefox) had the option to retire third-party cookies. Others like Apple, whom unsurprisingly doesn't own any Ad network, even disabled third-party cookies by default.\n                </p>\n\n                <p>\n                  Recently, Google announced their intentions to completely phase out third-party cookies in Chrome within two years. This announcement puts the final nail in the coffin for third-party cookies. Marketers and Brands now need to find other ways to make their marketing just as effective while respecting the new privacy rules. <span class=\"text-info\">Companies that still have not invested in a forward-looking plan to manage audience will be left behind. </span>\n                </p>\n                <h2 class=\"font-weight-bold mt-8\">\n                  Learning to know your audience using first-party contextual data only\n                </h2>\n\n                <p>\n                  Vox Media is one of the biggest and most respected digital media companies in the US. They have properties like The Verge, Vox, Eater, Polygon, Curbed, SB Nation, etc. They are receiving an incredible amount of visitors every day. Their system makes a great deal of decisions with every click. What content should they show the user? What ad content is the most relevant? And they need to answer those questions a thousand times per second.\n                </p>\n\n                <p>\n                  Months ago, Vox Media approached us with an interesting challenge. Would it be possible to leverage the latest AI methods to build a more privacy-friendly audience segmentation technology without relying on any third-party data? Instead, the prediction could be based on actions users took on their properties, or in other words, the first party-data. It could be the title of the clicked articles, the type of concent within the article, the time of the day, the level of engagement, the user's device, etc.\n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “ Would it be possible to leverage the latest AI methods to build a more privacy-friendly audience segmentation technology without relying on any third-party data? ”\n                  </p>\n                </blockquote>\n                <p>\n                  Ad retargeting by tracking users all over the web is similar to fishing using bottom trawling. It might work, but you’re gathering a whole lot of other data in the process which is highly problematic for a user's privacy. It turns out that if you want to learn more about your audience by using only first-party data, you need to be much smarter about how you’re using the few data points you have. \n                </p>\n\n                <figure class=\"figure pt-8 pb-8 pl-lg-8 pr-lg-8\">\n                  <img class=\"figure-img img-fluid rounded lift lift-lg\" src=\"/assets/img/photos/fishing.png\" alt=\"...\"/>\n                  <figcaption class=\"figure-caption text-center\">\n                    Doing ad retargeting by tracking users all over the web is similar to fishing using bottom trawling. It might work, but you’re gathering a whole lot of other data in the process which is highly problematic for a user's privacy.\n                  </figcaption>\n                </figure>\n                <p>\n                  Most of the data you have about the audience is how they are interacting with the content you placed on your website. For digital media companies, like Vox Media, this content is mostly texts, images, and videos. Thanks to the latest advance in deep learning, we are now much better at analyzing the content of images and texts than just a few years ago. Using Convolutional Neural Network, we can now really get a sense of what an image is all about. \n                </p>\n                <p>\n                  The same type of analysis can be achieved on texts using state-of-the-art Transformer models (like BERT, ELMO, GPT2, etc.) trained on huge amounts of data. Assuming that you can describe in great detail the nature of the content, the challenge is to turn users interactions with your content into interests and intentions.\n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “ As a content provider, the beauty of this approach is that the deep learning models are powered by your own content. ”\n                  </p>\n                </blockquote>\n                <p>\n                  We can illustrate this task using a classic example. You have a restaurant and a new client comes in. You have essentially two ways to know more about him. You can track him everywhere and note every restaurant he visits. This would be quite creepy and the equivalent of using a third-party cookie. Or you can use the first-party data approach and keep track of what he orders and likes in your own restaurant. If you have a deep understanding of all the meals served in your restaurant, you can do a pretty good job of predicting his interests. You can even add up other variables like the day and hour of the week the client visited to push further your understanding. \n                </p>\n                <p>\n                  As a content provider, the beauty of this approach is that the deep learning models are powered by your own content. As the amount of content increases in volume and diversity, the more refined your knowledge of your own audience will be.\n                </p>\n                <h2 class=\"font-weight-bold mt-8\">\n                  Toward a first-party segmentation system\n                </h2>\n\n                <p>\n                  A first-party segmentation system is designed to describe the audience interests and intents based on its viewed content. An example of these segments could be something specific like <i>Camera Enthusiast Currently Shopping</i>. In order to be effective, this system is required to fulfill a number of features: \n                </p>\n                <ul class=\"list-unstyled mb-7\">\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The model needs to learn from large volume of historical data.</span><br/>\n                      <span class=\"text-muted\">\n                      It must find and learn interesting patterns and \"natural\" segments in the data. For instance, it may find that a group of users visit the same content because of a similar interest. It will then be labelled as something like <i>Camera Enthusiast Currently Shopping</i>. \n                      </span>\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The predictions must be computed in real time.</span><br/>\n                      <span class=\"text-muted\">\n                      It must handle a high volume of requests (> 1k/sec) within a reasonable latency (~100ms). \n                      </span>\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The model should understand all content types.</span><br/>\n                      <span class=\"text-muted\">\n                      Content may come in any form like images, texts, numerical attributes, labels, etc.\n                      </span>\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The prediction should be valid on unseen data.</span><br/>\n                      <span class=\"text-muted\">\n                      For instance, if a user clicks on a recently published article about a new camera, the model should be able to assign the <i>Camera Enthusiast</i> segment even if the article is absent from the historical data. \n                      </span>\n                    </p>\n                  </li>\n                </ul>\n\n                <p>\n                  A natural way to model the interactions of the users with a set of content is by using the concept of graphs. In this setup we have two types of components: The users and the content. A user and a piece of content are connected if the user clicked on the content. For each content component, we convert raw data (texts and images) into compact attributes (a big vector), using recent deep learning models, and store them for later use. The interaction itself (the pageviews) may also have its own set of attributes.\n                </p>\n                <figure class=\"figure pt-8 pb-8 pl-lg-8 pr-lg-8\">\n                  <img class=\"figure-img img-fluid rounded\" src=\"/assets/img/photos/graph.png\" alt=\"...\"/>\n                  <figcaption class=\"figure-caption text-center\">\n                    A natural representation of the Vox Media dataset. Users are connected with contents they have seen. The model is trained to predict users segments based on this graph structure.\n                  </figcaption>\n                </figure>\n                <p>\n                  In this graph framework, the model task is to assign segments to users. This is done by crunching all the component features and even propagating information from users with similar records. All those attributes need to work together in a cohesive way to inform the final decision. These models, called Graph Neural Networks, are known to be extremely effective on a large variety of machine learning tasks. That's why state-of-the-art Graph Neural Networks are at the core the Vox Media’s first-party segmentation platform.\n                </p>\n\n                <figure class=\"figure pt-8 pb-8 pt-8 pb-8 pl-lg-10 pr-lg-10\">\n                  <img class=\"figure-img img-fluid rounded pl-lg-10 pr-lg-10\" src=\"/assets/img/photos/graph2.png\" alt=\"...\"/>\n                  <figcaption class=\"figure-caption text-center\">\n                    User segments are computed using the attributes of the contents they have seen. Higher level attributes can be built by looking deeper in the graph.\n                  </figcaption>\n                </figure>\n\n                <p>\n                  These models fulfill all the requirements and even more: \n                </p>\n\n                <ul class=\"list-unstyled mb-6 mt-7\">\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      The prediction does not require the actual identity of the user.\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      It is powered by the content owned by the platform, not the demographic user features.\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      The segments are also perfectly inductive, meaning that the model is trained on historical data but the segments extend to unseen pieces of content.\n                    </p>\n                  </li>\n                </ul>\n            \n                <p>\n                  At the end, those first-party predictions are much more effective for the brand and more privacy-friendly than third-party cookies. And all this is happening many hundreds of times per sec; every time a user is interacting with a piece of content on Vox Media Network!\n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “        At the end, those contextual predictions are much more effective for the brand and more privacy-friendly than third-party cookies.\n                    ”\n                  </p>\n                </blockquote>\n                <p>\n                  Vox Media first-party segmentation platform is an impressive piece of technology and we’re are really proud, at Hectiq.AI, to have participated in its generation. Brands using Vox Media's new platform can now connect effectively to their audience and more importantly, they can do it in a privacy-friendly way.\n                </p>\n              </div>\n            </div> \n          </div> \n        </section>)\n}\n\n\nexport default VoxMediaPost;","import React from \"react\";\nimport PropTypes from 'prop-types';\nimport Chart from \"react-apexcharts\";\n\n\nfunction EfficientNetScalingGraph(props){\n\n  var series = [];\n  series.push({\n    name: \"EfficientNet\",\n    data: [[5.3,77.3], [7.8,79.2], [9.2,80.3], [12,81.7], [19,83.0], [30,83.7], [43,84.2], [66,84.4]]\n  })\n\n  let options = {\n    chart: {\n      id: \"efficient-net-scaling\",\n      toolbar: {\n        show: false\n      },\n      zoom: {\n        enabled: false\n      }\n      \n    },\n    dataLabels: {\n      enabled: true,\n      formatter: function(val, opt, i) {\n          return \"B\"+opt.dataPointIndex;\n      },\n    },\n    xaxis: {\n      title: {\n        text:\"Number of parameters (Millions)\"\n      }\n    },\n    yaxis: {\n      title: {\n        text:\"Imagenet Top-1 Accuracy (%)\"\n      }\n    },\n    grid: {\n      borderColor: '#e7e7e7',\n      xaxis: {\n          lines: {\n              show: true\n          }\n      },\n    },\n    tooltip: {\n      enabled: false\n    }\n  };\n\n\n  return (<Chart\n             options={options}\n              series={series}\n              type=\"line\"\n              height={props.height}\n          />)\n}\n\n\nEfficientNetScalingGraph.propTypes = {\n  height: PropTypes.number\n};\n\nEfficientNetScalingGraph.defaultProps = {\n  height: 300\n};\n\nexport default EfficientNetScalingGraph;","import React from \"react\";\nimport PropTypes from 'prop-types';\n\nimport * as d3 from \"d3\";\n\nclass EfficientNetGraph extends React.Component {\n\n    constructor(props){\n      super(props)\n      this.state = {\n        width: props.width,\n        depth: props.depth,\n        resolution: props.resolution,\n      }\n    }\n    componentDidUpdate(prevProps) {\n      // Typical usage (don't forget to compare props):\n      if (this.props.width !== prevProps.width) {\n        this.setState({width: this.props.width})\n      }\n      if (this.props.depth !== prevProps.depth) {\n        this.setState({depth: this.props.depth})\n      }\n      if (this.props.resolution !== prevProps.resolution) {\n        this.setState({resolution: this.props.resolution})\n      }\n      this.drawChart()\n    }\n    componentDidMount() {\n      const svgCanvas = d3.select(this.refs.efficientnetgraph)\n          .append('svg')\n          .attr('width', \"100%\")\n          .attr('height', this.props.height);\n\n      for (var i = 0; i < this.props.maxDepth; i++) {\n        svgCanvas.append(`g`).attr(\"id\", `d3-g${i}`);\n        svgCanvas.append(`g`).attr(\"id\", `d3-link-g${i}`);\n      }\n        this.drawChart()\n    }\n    drawChart() {\n\n      const svgCanvas = d3.select(this.refs.efficientnetgraph).selectAll(\"svg\")\n      let colors = ['#c6dbef','#9ecae1','#6baed6','#4292c6','#2171b5','#08519c','#08306b'];\n      for (var i = 0; i < this.props.maxDepth; i++) {\n\n        const resolutionScale = (this.props.resolution+4)/10;\n\n        let nodes = [{width: (i==0)? 8: 8+i*1.2*this.state.width,\n                      x: `${i*100/(this.state.depth+2)}%`,\n                      height: (i==0)? (this.props.height*resolutionScale): this.props.height*resolutionScale-i*this.props.height*resolutionScale/(this.state.depth+2)}]\n\n\n        svgCanvas.select(`#d3-link-g${i}`)\n        .append(\"defs\")\n        .append('marker')\n        .data(nodes)\n        .join('defs')\n        .attr('id', `arrow-${i}`)\n        .attr('viewBox', [0, 0, 20, 20])\n        .attr('refX', 0)\n        .attr('refY', 10)\n        .attr('markerWidth', 10)\n        .attr('markerHeight', 10)\n        .attr('orient', 'auto-start-reverse')\n        .append('path')\n        .attr('d', d3.line()([[0, 0], [0, 20], [20, 10]]))\n        .style('fill', '#aaa')\n        .attr('stroke', '#aaa');\n\n        const link = svgCanvas.select(`#d3-link-g${i}`)\n        .selectAll(\"line\")\n        .data(nodes)\n        .join(\"line\")\n          .style(\"stroke\", \"#aaa\")\n          .attr('marker-end', `url(#arrow-${i})`)\n          .attr(\"x1\", `${(i+0.4)*100/(this.state.depth+2)}%`)\n          .attr(\"y1\", this.props.height/2)\n          .attr(\"x2\", `${(i+0.7)*100/(this.state.depth+2)}%`)\n          .attr(\"y2\", this.props.height/2);\n\n\n        const node = svgCanvas.select(`#d3-g${i}`)\n        .selectAll(\"rect\")\n        .data(nodes)\n        .join(\"rect\")\n          .attr(\"width\", d=>d.width)\n          .attr(\"height\", d=>d.height)\n          .style(\"fill\", colors[i])\n          .attr(\"x\", d=>d.x )\n          .attr(\"y\", d=> this.props.height/2-d.height/2,);\n\n       \n      }\n      // Add nodes\n    }\n    render() { return <div ref=\"efficientnetgraph\"></div> }\n}\n\n\n\nEfficientNetGraph.propTypes = {\n  height: PropTypes.number,\n  depth: PropTypes.number,\n  maxDepth: PropTypes.number,\n  width: PropTypes.number,\n  resolution: PropTypes.number,\n\n};\n\nEfficientNetGraph.defaultProps = {\n  height: 100,\n  depth: 4,\n  maxDepth: 10,\n  width: 10,\n  resolution: 4,\n};\n\nexport default EfficientNetGraph;","import React from \"react\";\nimport PropTypes from 'prop-types';\n\nimport StepRangeSlider from 'react-step-range-slider'\nimport EfficientNetGraph from \"./EfficientNetGraph.react\";\n\nclass EfficientNetGraphController extends React.Component {\n\n    constructor(props){\n      super(props)\n      this.state = {\n        width: 4,\n        depth: 3,\n        resolution: 5\n      }\n    }\n\n    render() { \n\n      const widthRange = [...Array(6).keys()].map((d=>({value:d, step:1})));\n      const depthRange = [...Array(6).keys()].map((d=>({value:d, step:1})));\n      const resRange = [...Array(6).keys()].map((d=>({value:d, step:1})));\n      console.log(this.state)\n      return (<div class=\"row\">\n                <div class=\"col-md-4\"> \n                  <h6 class=\"font-weight-bold text-uppercase text-gray-700 mt-3\">\n                    Width\n                  </h6>\n                  <StepRangeSlider \n                    value={this.state.width} \n                    range={widthRange} \n                    onChange={value =>this.setState({width:value})}\n                  />\n                  <h6 class=\"font-weight-bold text-uppercase text-gray-700 mt-3\">\n                    Depth\n                  </h6>\n                  <StepRangeSlider \n                    value={this.state.depth} \n                    range={depthRange} \n                    onChange={value =>this.setState({depth:value})}\n                  />\n                  <h6 class=\"font-weight-bold text-uppercase text-gray-700 mt-3\">\n                    Resolution\n                  </h6>\n                  <StepRangeSlider \n                    value={this.state.resolution} \n                    range={resRange} \n                    onChange={value =>this.setState({resolution:value})}\n                  />\n                </div>\n                <div class=\"col-md-8 my-auto\"> \n                  <h6 class=\"font-weight-bold text-uppercase text-gray-700 mt-3\">\n                    Model\n                  </h6>\n                  <EfficientNetGraph {...this.state} height={130}/>\n                   <p class=\"font-size-lg text-muted mb-6\">\n                    Boxes are the tensor used. \n                  </p>\n                </div>\n              </div>)\n    }\n}\n\n\n\nEfficientNetGraphController.propTypes = {\n\n};\n\nEfficientNetGraphController.defaultProps = {\n\n};\n\nexport default EfficientNetGraphController;","import React from \"react\";\n\nfunction iOSPost(props){\n  return (<div>\n            <section class=\"pt-0 pt-md-0 pb-0\">\n              <div class=\"container\">\n                <div class=\"row justify-content-center\">\n                  <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                    <p>\n                    As cellphones are getting more performant, more companies are developing solutions directly on the user devices.\n                    </p>\n\n                    <h2 class=\"pt-6 font-weight-bold text-primary\">Opportunities</h2>\n                    <h3 class=\"pt-4 font-weight-bold\">Saving computational cost</h3>\n                    <p>\n                      When deploying machine learning in the cloud, companies are often surprised how expensive it can be. To be performant and effective, models are often deployed on expensive GPU instances rather than cheap CPU instances. \n                    </p>\n                    <p>\n                      User devices offer an interesting alternative to reduce your computational cost. The models are deployed on the devices and the computation is done locally. Sometimes, the machine learning results can be sent to and stored on a cloud server for future use. Hence, not only you no longer have to maintain GPUs instances in the cloud, but it can also reduces the bandwidth of information sent between users and your servers. \n                    </p>\n                    <h3 class=\"pt-4 font-weight-bold\">Improved user experience</h3>\n                    <p>\n                    The overall user experience is improved because the interaction between the user and the machine learning results will be made available faster. Compared with cloud solutions, users no longer have to wait for the server to respond with results. Unfortunately, it can also create a disparity in the experience from one device to the other, due to difference in phones performance. If you carefuly optimize the models, this difference will faint, even more as new devices are designed for running machine learning. Our team at Hectiq can help you optimize and sustain a uniform user experience on all devices.\n                    </p>\n                    <h3 class=\"pt-4 font-weight-bold\">New products</h3>\n                    <p>\n                      The flagship of mobile machine learning is all the new applications it opens to. Vision based models can be heavily exploited as camera are getting better and overused. For instance, Apple recently showcased how a pose estimation model can be used to count push-ups and squats made by a user. It seems a promising avenue for gym-related apps but imagine if used in a cooking app to guide users into a recipe, or in a DIY-app for step-by-step image validation and advices. Fashion businesses can also be seriously advantaged by developing body measurements solutions or try-it-on AR models.\n                    </p>\n                    <p>\n                      Language models also benefits from being deployed on mobiles. Classifying a text, extracting the entities, or answering questions are all typical tasks performed by language models. \n                    </p>\n\n                    <h2 class=\"pt-6 font-weight-bold text-primary\">Challenges</h2>\n                    <p>\n                      Developing mobile solutions comes with many challenges. Fortunately, this pathway has been cleared by strong players such as Facebook, Apple, and Google. They have spotted the difficulties and developed reliable solutions. We present what we think are the top challenges.\n                    </p>\n\n                    <h3 class=\"pt-4 font-weight-bold\">Developing fast models</h3>\n                    <p>\n                      Nobody will enjoy the experience if the app is slow. Your models prediction has to be relatively fast, depending on the usage. If it’s an one-time inference, it probably can take at most 10 seconds. But if the user interacts with the prediction, then your model should run in the milliseconds frame. In vision, 50ms per inference is the reference threshold to achieve a live capture 18fps inference.\n                    </p>\n                    <h3 class=\"pt-4 font-weight-bold\">Validating the prediction on the devices</h3>\n                    <p>\n                      Models run differently on devices compared with computers. The reason is models are deployed and compiled using custom mobile libraries but are trained and tested using other libraries on your computer. It is likely that the conversion modifies certain aspects that impact the inference results. Make sure to validate the model performance on the devices. Document and characterize the differences between the mobile and computer-based models to organize your future work.\n                    </p>\n                    <p>\n                      During conversion, most mobile librairies suggest to reduce the models using 16-bits or 8-bits encoding. While this is sometimes essential for running on devices, keep it mind that it can impact the accuracy of your model. \n                    </p>\n                    <h3 class=\"pt-4 font-weight-bold\">Keeping models up-to-date</h3>\n                    <p>\n                    It is a good practice to update the models according to their usage and their performance in real-world datasets. You should always periodically fine-tuned your models with failed cases and in-the-wild data. This continuous integration can be hard to maintain as you will need to make sure they don't degrade the models performance and that they don’t interfere with existing components of your app.\n                    </p>\n                    <p>\n                    Pushing updated models to devices is a tricky task. A bad practice would be to include the model into your app and push app updates on the App Store whenever you update the model weights. A better practice is to include a routine in the app that checks whether a model update is available and downloads the new weights. The best practice is to use the recently introduced model manager by Apple. It organizes your models into collections (such as bundles of models) and the devices automatically update their models according to the release version.\n                    </p>\n\n                    <h3 class=\"pt-4 font-weight-bold\">Have a fallback option if the models fail</h3>\n                    <p>\n                      Even with a 98% accuracy, a model is wrong in 2% cases. These 2% cases are critical as you need a backup plan so that the app does not crash. You should expect that the incorrect predictions are hard to detect so you need extra caution to validate on-device any outputs of the model. \n                    </p>\n                    <p>\n                      You will also need a backup plan if the model is incapable of making a prediction (due to invalid input or software error). One solution is to send and process the information in the cloud on cheap CPU instances, or a design that lets the user manually set the information.\n                    </p>\n\n                    <h2 class=\"pt-6 font-weight-bold text-primary\">Performance</h2>\n                    <p>\n                      Wondering\n                    </p>\n                    <div>\n                      <div class=\"row align-items-center\">\n                        <div class=\"col-12 col-md-6 col-lg-7\">\n                          <div class=\"device device-iphonex\">\n                            <video playsinline autoplay muted loop src=\"/assets/img/ios/test.mp4\" type=\"video/mp4\" class=\"device-screen\" alt=\"...\"/>\n                            <img src=\"/assets/img/shapes/iphonex.svg\" class=\"img-fluid\" alt=\"...\"/>\n                          </div>\n                        </div>\n                        <div class=\"col-12 col-md-6 col-lg-5\">\n                          <h2>\n                            Our mobile lab\n                          </h2>\n                          <p>We have developed an internal iOS app.\n                          </p>\n                          <div class=\"d-flex\">\n                            <div class=\"badge badge-rounded-circle badge-primary-soft mt-1 mr-4\">\n                              <i class=\"fe fe-play\"></i>\n                            </div>\n                            <p>\n                              Benchmark models and monitor activity\n                            </p>\n                          </div>\n                          <div class=\"d-flex\">\n                            <div class=\"badge badge-rounded-circle badge-primary-soft mt-1 mr-4\">\n                              <i class=\"fe fe-cpu\"></i>\n                            </div>\n                            <p>\n                              Optimize models\n                            </p>\n                          </div>\n                          <div class=\"d-flex\">\n                            <div class=\"badge badge-rounded-circle badge-primary-soft mt-1 mr-4\">\n                              <i class=\"fe fe-package\"></i>\n                            </div>\n                            <p>\n                              Test deployment strategies\n                            </p>\n                          </div>\n                        </div>\n                      </div>\n                    </div>\n                  </div>\n                </div>\n              </div>\n            </section>\n          </div>)\n}\n\n\nexport default iOSPost;","import VoxMediaPost from \"./VoxMediaPost.react\"\nimport DecadeOfSciencePost from \"./DecadeOfSciencePost.react\";\nimport CatherineJoinsTeamPost from \"./CatherineJoinsTeamPost.react\";\nimport EfficientNetPost from \"./EfficientNetPost.react\";\nimport DAMPost from \"./DAMPost.react\";\nimport iOSPost from \"./iOSPost.react\";\n\nconst data = [\n  {\n    date: \"August 5, 2020\",\n    title: \"Opportunities and challenge of machine learning on devices\",\n    summary: \"\",\n    authors: [\"Edward Laurence\"],\n    by: \"Edward Laurence, Data Scientist at Hectiq.AI\",\n    content: iOSPost,\n    id: \"iosPost\",\n    hide: true,\n  },\n  {\n    date: \"July 30, 2020\",\n    title: \"Helping a nonprofit organization in their fight against school dropout\",\n    summary: \"The Diplome avant la Médaille is a nonprofit organization that needs to pair hundred students and volunteers according to their availability and preferences. We provided a pairing program that solved their problem so that they can focus on helping students get their diploma.\",\n    authors: [\"Edward Laurence\"],\n    by: \"Edward Laurence, Data Scientist at Hectiq.AI\",\n    content: DAMPost,\n    id: \"dampost\",\n    moreInfo: \"Check and support DAM's program <a href='https://diplomeavantlamedaille.org'>here</a>.\",\n    hide: true,\n  },\n  {\n    date: \"June 10, 2020\",\n    title: \"EfficientNet1D: A beast among the miniatures\",\n    excerpt: \"In our efforts to make machine learning accessible for everyone,  we are releasing our 1D implementation of the fairly recent family of models: EfficientNet based on the PyTorch.\",\n    summary: \"In our efforts to make machine learning accessible for everyone,  we are releasing our 1D implementation of the fairly recent family of models: EfficientNet based on the PyTorch.\",\n    authors: [\"Catherine Paulin\"],\n    by: \"Catherine Paulin\",\n    content: EfficientNetPost,\n    id: \"efficientnet\",\n    hide: true,\n  },\n  {\n    date: \"March 30, 2020\",\n    title: \"Vox Media: Building a privacy-friendly first-party data segmentation platform\",\n    excerpt: \"Hectiq.AI completes an exciting collaboration with Vox Media. We have developed an Ai model to help Vox Media better understand their audience. The challenge was tough as the model needed to be privacy-friendly, crazy fast, and highly effective.\",\n    frontImgUrl: '/assets/img/photos/voxmediapost.svg',\n    frontWideImg: \"/assets/img/photos/voxmediapost-wide.svg\",\n    frontImgUrlWide: \"/assets/img/photos/voxmediapost.svg\",\n    summary: \"Hectiq.AI completes an exciting collaboration with Vox Media. We have developed an Ai model to help Vox Media better understand their audience. The challenge was tough as the model needed to be privacy-friendly, crazy fast, and highly effective. In this post, we explain what motivated this project, namely developing an alternative to 'creepy' third-party data, and we discuss how we achieved the task by using state-of-the-art deep learning models. You can also read Vox Media's recent <a href='https://www.voxmedia.com/2020/2/26/21155010/its-not-who-you-are-but-what-you-are-consuming'>white paper</a> about the project.\",\n    authors: [\"Martin Laprise\"],\n    credits: \"Bottom trawling illustration from <i>Cold-water Coral Reefs: out of sight - no longer out of mind. UNEP-WCMC Biodiversity Series 22</i>\",\n    moreInfo: \"<a href='https://www.voxmedia.com/2020/2/26/21155010/its-not-who-you-are-but-what-you-are-consuming'>Vox Media announces Forte.</a>\",\n    by: \"Martin Laprise, Chief Scientist & Founder of Hectiq.AI\",\n    id: \"voxmedia\",\n    content: VoxMediaPost,\n  },\n  {\n    date: \"February 19, 2020\",\n    title: \"A decade of Data Science hypes: The Good Parts\",\n    excerpt: \"In this post, I look back at some of the trends & hypes that punctuated the Data Science world in the last years and take some notes on the lasting impacts they had.\",\n    frontImgUrl: \"/assets/img/photos/idea.jpg\",\n    frontImgUrlWide: \"/assets/img/photos/idea-wide.jpg\",\n    summary: \"In this post, I look back at some of the trends & hypes that punctuated the Data Science world in the last years and take some notes on the lasting impacts they had.\",\n    by: \"Martin Laprise, Chief Scientist & Founder of Hectiq.ai\",\n    authors: [\"Martin Laprise\"],\n    content: DecadeOfSciencePost,\n    id: \"decade-of-data-science\",\n  },\n  {\n    date: \"September 30, 2019\",\n    title: \"Hectiq.ai welcomes a new member\",\n    excerpt: \"Catherine Paulin is joining hectiq.ai to bring balance to the force.\",\n    frontImgUrlWide: \"\",\n    content: CatherineJoinsTeamPost,\n    dark: true,\n    id: \"catherine-joins-the-team\",\n  },\n  {\n    date: \"September 1, 2019\",\n    title: \"Hectiq.ai has a new blog\",\n    excerpt: \"We are proud to announce this new blog website! It will feature our recent achievements and news.\",\n    frontImgUrlWide: \"\",\n    id: \"new-website\",\n  }\n]\n\nexport default data;","import React from \"react\";\n\n\nfunction DAMPost(props){\n  return (<div>\n            <section class=\"pt-0 pt-md-0 pb-0\">\n              <div class=\"container\">\n                <div class=\"row justify-content-center\">\n                  <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                    <p>\n                      At our coworking space, we meet inspiring organizations. One of them is the <a href=\"https://diplomeavantlamedaille.org\">Diplome avant la médaille</a> nonprofit organization. Since its creation, it is making a huge difference in the fight against school dropout in Quebec city: 9 of 10 students participating in their program have been able to finish high school while 84% of the teachers witness academic improvement! Just like Coach Carter, they use sports to bring students into classes and they organize study sessions.\n                    </p>\n                    <p>\n                      A critical facet of the program is the tutoring. Volunteers enroll to assist students once a week during study sessions. In 2019, almost 200 students were engaged in the program. In 2020, they expect to double this number.                       \n                    </p>\n                  </div>\n                </div>\n              </div>\n            </section>\n\n\n            <section class=\"pt-0 pt-md-0 my-6 bg-light\">\n              <div class=\"container\">\n                <div class=\"row justify-content-center\">\n                  <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n\n                    <div class=\"pt-6 pb-2\">\n                      <h2>\n                        Diplome avant la médaille's stats  <br/>\n                        <span class=\"text-primary h3\">2018-2019 school year</span>\n                      </h2>\n                      <div class=\"row\">\n                        <div class=\"col-md-4\">\n                          <div class=\"d-flex justify-content-center\">\n                            <span class=\"price display-2 mb-0\" data-toggle=\"countup\" data-from=\"0\" data-to=\"96\" data-aos data-aos-id=\"countup:in\">0</span>\n                            <span class=\"h2 align-self-end mb-1\">%</span>\n                          </div>\n                          <p class=\"font-size-lg text-gray-700 mb-6\">\n                            Ratio of students participating in their program have been able to pass their year \n                          </p>\n                        </div>\n\n                        <div class=\"col-md-4\">\n                          <div class=\"d-flex justify-content-center\">\n                            <span class=\"price display-2 mb-0\" data-toggle=\"countup\" data-from=\"0\" data-to=\"8\" data-aos data-aos-id=\"countup:in\">0</span>\n                            <span class=\"h2 align-self-end mb-1\">/10</span>\n                          </div>\n                          <p class=\"font-size-lg text-gray-700 mb-6\">\n                            Students with grade retention that succeeded to pass their year with the program\n                          </p>\n                        </div>\n\n                        <div class=\"col-md-4\">\n                          <div class=\"d-flex justify-content-center\">\n                            <span class=\"price display-2 mb-0\" data-toggle=\"countup\" data-from=\"0\" data-to=\"84\" data-aos data-aos-id=\"countup:in\">0</span>\n                            <span class=\"h2 align-self-end mb-1\">%</span>\n                          </div>\n                          <p class=\"font-size-lg text-gray-700 mb-6\">\n                            Fraction of teachers witnessing academic improvement\n                          </p>\n                        </div>\n                      </div>\n                    </div>\n                  </div>\n                </div>\n              </div>\n            </section>\n            <section class=\"pt-0 pt-md-0 pb-0\">\n              <div class=\"container\">\n                <div class=\"row justify-content-center\">\n                  <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                    <p>\n                      The success of the project comes with the practical problem of pairing tutors and students. Since now, they used to match people by hand. This solution becomes impractical for the coming years. \n                    </p>\n                    <p>\n                      Pairing is notoriously hard and can be framed as an integer constraint problem. Let say we have N students and N tutors. You can define N<sup>2</sup> unique pairs of students and tutors. Each pair has a value characterizing how the student's and tutor's preferences (subject, availability, and schools) are alike. The optimization task consists in selecting the most number of pairs, at most N, but verifying that each tutor and student are selected only once and maximizing the values over the selected pairs. \n                    </p>\n                    <p>For 10 students and 10 tutors, we count 100 possible pairs and over 3 million possible pairing solutions (it grows as the factorial of the number of students). Even for so few students, solving it hand will likely get you not the best possible pairings. We must rely on computers to deal with such complexity and find valuable pairing solutions. This is where Hectiq can help. \n                    </p>\n                  </div>\n                </div>\n              </div>\n            </section>\n            <section class=\"pt-0 pt-md-0 pb-0\">\n              <div class=\"container\">\n                <div class=\"row justify-content-center\">\n                  <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                    <figure class=\"figure pt-8 pb-8 pl-lg-8 pr-lg-8\">\n                      <img class=\"figure-img img-fluid\" src=\"/assets/img/photos/dam.png\" alt=\"...\"/>\n                      <figcaption class=\"figure-caption text-center\">\n                        All 24 possible pairing solutions for 4 students and 4 tutors. The problem for DAM has another layer of complexity: We must select the pairings that respect the most of the preferences of the students and tutors. Due to people availability, it may not even exist a solution where all students are paired so that we also check other pairing possibilities.\n                      </figcaption>\n                    </figure>\n                  </div>\n                </div>\n              </div>\n            </section>\n\n            <section class=\"pt-0 pt-md-0 pb-0\">\n              <div class=\"container\">\n                <div class=\"row justify-content-center\">\n                  <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n\n                    <p>\n                      We have been building a script for automatically finding pairings. Each student fills a form indicating its weak/strong subjects, availability, and school location. For tutors, they specify subjects they are comfortable to teach, their availability, and the schools they can commute to. Our code maximizes the number of pairings between students and tutors while optimizing on the people’s preferences. \n                    </p>\n                    <hr class=\"hr-md mb-7\"/>\n                    <blockquote class=\"blockquote mb-7\">\n                      <p class=\"h3 mb-0 text-center text-primary\">\n                        “Our code maximizes the number of pairings between students and tutors while optimizing on the people's preferences.”\n                      </p>\n                    </blockquote>\n                    <hr class=\"hr-md mb-7\"/>\n                    <p>\n                      We'll also call present in the future to help them add new features. For instance, the pandemic forces them to set up online tutoring. It is a small adjustment to the script, but it makes a huge difference as the program can continue through social distancing.\n                    </p>\n                    <p>\n                    Le Diplome avant la médaille was facing an important issue in their path to expanding their tutoring offer. We solved it by providing an optimization code. We are proud to be able to deal with these practical engineering tasks and let them focus on helping students get their diploma.\n                    </p>\n                  </div>\n                </div>\n              </div>\n            </section>\n          </div>)\n}\n\n\nexport default DAMPost;","import React from \"react\";\n\nimport {EfficientNetScalingGraph, EfficientNetGraphController} from \"../components/charts\"\n\n\n\nfunction EfficientNetPost(props){\n  return (<div>\n              <div className=\"container-fluid container-xs\">\n                <EfficientNetScalingGraph height={450}/>\n              </div>\n              <div className=\"container-fluid bg-light p-8\">\n                <div className=\"container-fluid container-lg\">\n                  <EfficientNetGraphController/>\n                </div>\n              </div>\n          </div>)\n}\n\n\nexport default EfficientNetPost;","import React from \"react\";\n\n\nexport default function DecadeOfSciencePost(props){\n\n  return (\n      <section class=\"pt-0 pt-md-0 pb-0\">\n        <div class=\"container\">\n          <div class=\"row justify-content-center\">\n            <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n              <p>\n                It’s the beginning of 2020 and like every new year, we see an abundance of the usual “year-in-review” blog posts. Looking back is not completely meaningless, but there is something deeply repetitive about those articles. This year I felt it was slightly different - because of the new decade, we saw a bunch of reviews of the previous one. Not all of them are great of course, but I think there is actually something more interesting that results from a look back over the past decade. \n              </p>\n              <p>\n                Ten years is long enough to see trends happening but still short enough to be experienced and remembered by a bunch of people. I thought it would be fun to go through some of the trends & hypes <strong class=\"text-primary\">I personally experienced</strong> as a Data Scientist / Data Engineer / CTO / Algo Guy or Machine Learning Engineer or whatever you want to call it over the past ten years.\n              </p>\n              <p>\n                Coincidently and on a more personal note, I also realized lately that I started doing data science fulltime in 2010. As a Physics PhD student for almost 5 years at that time (yes it's way too long !!!), I was eager to start something new, something more in line with my true passion for physics, numerical computation, software, the web..., a bunch of things that seem totally orthogonal to each other back then. At the time, working on web-related projects was not really viewed as hardcore software engineering and most importantly, no serious physicists were working on software. At least this opinion was part of the common wisdom of that time. \n              </p>\n              <p>\n                To be honest, I felt like my interest profile was a little bit off for a physicist. But then around 2008 something interesting started happening... for some reason people started searching for  people like me. They were looking for a strange mix of skills: software engineering, web tech knowledge, statistics, machine learning, etc. I was not really aware of this at that time, but an increasing amount of companies had started gathering a significant amount of data and they were looking for ways to leverage this in some form or other.\n              </p>\n\n              <h2 class=\"font-weight-bold mt-8\">\n                The infamous Data Scientist\n              </h2>\n\n              <p>\n                Naming something is often a good first step toward understanding something and a decade ago, some software engineers & product managers started realizing that they had a new type of “problem”. They were accumulating a lot of data and they started hiring people to handle & analyze this data, create statistical experiments, build data products, etc. It turns out that it’s really handy to have a job title when you’re looking to hire people, so they coined the term: <i>Data Scientist</i> with all it’s glorious ambiguity on purpose. That’s all it is.\n              </p>\n              <p>\n                When I first started working in startups, I was not really aware of this term nor was my employer, which made the “market” much less efficient. One of the trends I noticed pretty early one is the rise of this term <i>Data Scientist</i> as a job title to a point of almost laughable hype. Nevertheless, I think overall it was a good thing as it crystallized a genuine set of skills which lead to a much more efficient job market. People interested in those things can now find interesting jobs much more easily than a decade ago.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">Early 2009</h5>\n              <h2 class=\"font-weight-bold mt-0\">\n                Recommendation System\n              </h2>\n              <p>\n                From my perspective, recommendation systems were one the first big trends and big wins in Data Science. On September 18th, 2009, Netflix announced the winner of the Netflix prize. The challenge, announced 3 years earlier, was one of the first kaggle-like machine learning challenges and as such, it had a huge influence on the industry. Machine Learning is all about optimizing parameters: The parameters of models, the parameters of training, etc. The idea of treating the “model builder” just as another parameter in the process was a relatively new idea at the time, at least in a non-academic setting.  \n              </p>\n              <p>\n                The open nature of this challenge (and the $1M Grand Prize attached to it !) also lead to interesting contributions from a different set of people not normally engaged with this kind of work. That challenge was also where methods based on regularized Matrix Factorization (“MF”) were introduced for the first time in the context of collaborative filtering. All the winning methods (mostly big ensemble models) involved one or many MF models at some point or another. \n              </p>\n              <p>\n                Those methods are now part of most machine learning libraries and to this day, they are still the backbone of many recommendation engines. MF models were also one of the first to popularize the notion of embedding in a different context than in text analysis where they were initially used.\n              </p>\n              <img src=\"/assets/img/photos/recommandation.jpeg\" class=\"img-fluid pt-8 pb-8 pl-lg-11 pr-lg-11\" alt=\"...\"/>\n              <p>\n                In a way, recommendation models were (and still are) the perfect candidate to kick off the Data Science effort in a company. For many managers, the fact that a successful company would be willing to give $1M to improve their recommendation by 10% was an eye opener. It turns out that if you have a long tail of items to “sell” to a big audience, that audience's  discovery will be imperfect; if you have some basic analytics it’s pretty easy to quantify the financial impact of having better discovery by introducing a recommendation algorithm in the loop. In that sense, it was the perfect project to sell a new Data Science effort to executives. They could easily see a potential return on investment. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center text-primary-desat\">\n                  “Most successful projects involving machine learning essentially generate their own labels by some clever process, UX mechanic or by leveraging existing datasets”\n                </p>\n              </blockquote>\n              <p>\n                Another interesting aspect was the unsupervised nature of the models. As we are now discovering with modern deep learning models, the requirement for a huge volume of labeled data to train a supervised model can be a big problem for a project. “Classic” Machine Learning used back then was less “greedy” but it was nevertheless problematic. Having to manually label data in order to train and deploy a model in production can put a stop to a DS project in a spectacular fashion if it’s not carefully planned. \n              </p>\n              <p>\n                In my experience, most successful projects involving machine learning essentially “generate their own labels” by some clever process, UX mechanic or by leveraging existing datasets. Unsupervised recommendation models didn’t have that problem. From a product perspective it was “magic”: you fed in your historical data and the model takes care of the rest.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">2013 - 2015</h5>\n              <h2 class=\"font-weight-bold\">\n                Big Data!\n              </h2>\n              <p>\n                A long time ago, before companies like Google, Facebook or LinkedIn even existed,  data meant a very specific thing for a data analyst. If you would ask them back then what was “data”,  they would probably describe something similar to a row in a SQL database. Data is a bunch of rows, with some specific typed field that we can export to a beautiful CSV file and import it in Excel to do some kind of analysis with it. Then came companies like Google, who literally transform data into money. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center text-primary-desat\">\n                  “Then came companies like Google, who literally transform data into money. ”\n                </p>\n              </blockquote>\n              <p>\n                Needless to say that if you’re transforming data into money you’ll probably have a very different perspective of what data is and how to handle it. In 2004, Google published a hugely influential paper titled <a href=\"https://dl.acm.org/doi/abs/10.1145/1327452.1327492\">MapReduce: Simplified Data Processing on Large Clusters</a>. In this paper, Jeffrey Dean and Sanjay Ghemawat described in length the programming model used in one of the earlier versions of the data pipeline used in Google clusters. Technicality aside, the paper was also one of the first sneak peeks into their infrastructure and the philosophy behind it. \n              </p>\n              <img src=\"/assets/img/photos/solvecartoon.jpeg\" class=\"img-fluid pt-8 pb-8 mx-auto d-block\" alt=\"...\"/>\n              <p>\n                The “machine” they described was a completely different beast than most database specialists were using at the time. It was running on a bunch of distributed & cheap computers and the raw data (like web requests logs, crawled documents) were at the center of everything.\n              </p>\n              <p>\n                Apache Hadoop, one of the first open source implementations of the MapReduce approach was the poster child of the <i>Big Data</i> marketing craze at the time.  Around 2013-2015, the hype was in full swing. <span class=\"text-primary\">Exactly like Data Scientist, the term Big Data was a loosely defined term used mostly by non technical people</span>, but like other buzzword it was also useful to promote some interesting ideas. A lot of definitions of Big Data (because there are a lot !)  seem fixated on the volume of the data, but in my mind the core idea behind the concept and the main takeaway is what I call the Diogenes Syndrome Principle:\n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h3 mb-0 text-center text-secondary\">\n                  \"Log everything and, never, ever, delete data.\"\n                </p>\n              </blockquote>\n\n              <p>\n                The idea is that you never know the analysis that you’ll want to do in the future, so you save everything and structure the data later. Another more modern way to articulate this idea is what some refer to as the “log-oriented” architecture or the lambda architecture (which is similar in essence). In log-oriented architecture, like the one popularized by Jay Kreps (the creator of Apache Kafka), we acknowledge that the metrics or the objects we store in a database are in fact the result of a series of events. Those events, if they are saved somewhere and replayed to the system can effectively rebuild all the metrics from scratch. In that sense, the log events are a more fundamental piece of data then the aggregate metrics that we store in the db, so building the data pipeline around them make sense. Most modern data pipelines are now built using this principle with tools like Apache Kafka, Spark, Storm, etc. \n              </p>\n              <p>\n                Of course, for most sizable companies, this principle will lead to a significant volume and variety of data (raw event, text, images, etc..) which will have a significant impact on the engineering aspects... most SQL databases at the time were not equipped to deal with this. That’s why the number of different databases pretty much exploded at the same time. If you google the term <i>NoSQL</i> (yes … another buzzword) you will probably find a bunch of articles written during that inflation era. Most software engineering in the 90s would work mostly with a single type database, a big central SQL (<i>MySQL, Postgres, Oracle,</i> etc..). \n              </p>\n              <p>\n                Now most engineers and data scientists need to handle a wide variety of databases: Full text search databases for text, document-based  databases for documents, and so on covering key-values, time series, columnar, graphs, etc.  A side effect of this explosion of possibilities is what I would call database-FOMO (Fear Of Missing Out) where a bunch of curious engineers in a team can’t help themself…. They need to try the latest db tech out there. \n              </p>\n              <p>\n                There are a lot of things that can slowdown a software project and choosing an exotic database just for the sake of it is a really effective one. And if you want to slow down your project even more… mix them up ! A tip that I’ve found to work in many situations: when in doubt, simply use a good old Postgres database. However in the end, having all those databases for different types of data is obviously a good thing. Fortunately, it looks like we’re back to a more classic SQL way of thinking and lot of those databases (<i>Elasticsearch, Cassandra, Redshift, BigQuery</i>, etc..), even though they are using widely different technologies, are now offering an SQL-like querying layer.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">2010 - 2012</h5>\n              <h2 class=\"font-weight-bold\">\n                GPU Computing & CUDA\n              </h2>\n              <p>\n                My first contact with Data Science was related to GPU Computing. GPU Computing (and CUDA specifically) is all the rage these days, but people would be surprised to learn how \"old\" it is. I started poking around with CUDA around 2009 in order to accelerate a critical computation in my PhD project. In order to validate an hypothesis we had about the behavior of a really intense & short pulse of light propagating in a laser, we needed to numerically solve something called the nonlinear Schrödinger equation. There were well-established methods to do that, but it turns out that solving that equation on a 2009 CPU was pretty time consuming especially if it's part of an optimization process where you need to solve this equation a TON of times. Long story short, reimplementing our code to PyCUDA gave us a HUGE performance improvement. \n              </p>\n              <p>\n                During my journey into this GPU rabbit hole I started doing some consulting around it. My first client was a financial startup in San Diego looking for some help to optimize their computation pipeline using GPU. I decided to put on hold my thesis to work on this... and ... never went back of course :).\n              </p>\n              <p>\n                Going back to CUDA a little bit. GPU computing really had a huge impact on machine learning in the last years, it's essentially the technological backbone that allowed researchers to unlock the power of deep learning. Around 2007, I remember watching a talk by Geoffrey Hinton about restricted Boltzmann machines and deep autoencoders (on Google Video ... Youtube was still mostly about cat videos at the time). I was really impressed, but it became clear pretty quickly that this kind of model was really CPU intensive and worked mostly with tiny images in toy datasets. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center  text-primary-desat\">\n                  \"GPUs are essentially the technological backbone that allowed researchers to unlock the power of deep learning.\"\n                </p>\n              </blockquote>\n              <p>\n                In 2010, I also started talking with a research group at Columbia University using something called a \"convolutional neural network\" to detect houses in satellite imagery. Their model, trained on CPU, was taking weeks to train. We discussed the possibility of maybe accelerating this training using this new thing called CUDA but to be honest it was slightly over my head at the time. But this idea of using a GPU to train CNN turns out to be a great one. Most deep learning researchers and practitioners point out the beginning of the current \"deep learning craze\" to the year 2012, when Krizhevsky and Sutskever, two students in Hinton's research group, published their paper about AlexNet: a GPU-trained CNN that  became the State-of-the-art on Imagenet.\n              </p>\n              <p>\n                Fast forward to today, some people are slightly nervous about the fact that  most implementations of deep learning models are working exclusively on top of proprietary software like CUDA. To this day, there is no serious support for open technology like OpenCL in the most popular DL framework and when we read discussions in the GitHub issues of Tensorflow and PyTorch, it looks like it's not at all a priority for a number of reasons. \n              </p>\n              <p>\n                It's a bummer but we have to give Nvidia some credit. A decade ago, Deep Learning was not a thing yet and most of the other companies like AMD and Intel didn't care about scientific computing, at least in their consumer offering easily accessible to graduate students. When CUDA was first released, AMD didn't have any serious equivalent and Intel certainly had their MKL library but it was not freely available. So Nvidia made a huge bet at the time and they also shifted their product really quickly to answer this new emerging market around Deep Learning. Today, an increasing number of libraries used in machine learning are GPU accelerated and it is fair to say that it’s one of the big shifts that happened in the last decade.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">2012 - Now</h5>\n              <h2 class=\"font-weight-bold\">\n                The rise of Deep Learning & AI\n              </h2>\n              <p>\n                People into machine learning will know that a lot of the fundamental works and ideas currently used in Deep Learning had their root in the 2000s or even in the 90s. But I think it’s fair to say the last decade was really where things exploded and where DL had the most impact for the DS community. In 2013-2014, right after the first splash made by AlexNet, we could already feel the excitement, but DL was still an academic affair. Just installing Theano or Caffee, the two most popular DL libraries at the time, was not a straightforward task and training a model from scratch on ImageNet was a feat in itself.\n              </p>\n              <p>\n                It was definitely working, but the engineering aspect was so hazardous that it was really hard to see how you could leverage technologies like this in a production environment.  With the first release of Tensorflow (and Keras), things changed pretty quickly. \n              </p>\n              <p>\n                It was now possible to experiment with exotic neural network architecture more easily and more quickly. More importantly, deep learning researchers started using Tensorflow massively and this had a big impact for the commercial applications of deep learning. \n              </p>\n              <p>\n                It meant that researchers and machine learning engineers, implementing those solutions into real products, now had the same common language. \n              </p>\n              <p>\n                To this day, the rapidity at which ideas flow from research directly to commercial applications is one of the distinctive aspects of the Deep Learning / AI community. Part of it comes from the common tools (Tensorflow, PyTorch, Scikit-Learn, etc.) used by the researchers and the machine learning engineers working in companies.\n              </p>\n              <p>\n                This point reminds me of the value of good software engineering. For a Data Scientist, it’s often tempting to define yourself as “not an engineer”... I tried that in the past and it didn’t end well. It’s true that the main job of a DS is not always to put software directly in production, but in years of doing this I never encountered a successful machine learning or data science  project without having the DS themself taking care of a bunch of the engineering aspects. A training or an inference pipeline is an intricate process that can be broken in millions of different ways.The only way to move a project forward, into a real world application, is with the original Data Science or Machine Learning team taking care of the entire loop at least once. There is no such thing as “doing research” and passing it on to the engineering team… I never encountered this successfully in my entire life.\n              </p>\n              <p>\n                Another big trend in the last decade was the rise and the dominance of the PyData stack. Years ago using Python to do your scientific computation was more a matter of taste (or even faith!) than a logical decision. The main appeal was the language itself, the numpy/scipy libraries and the ability to bind legacy code (in Fortran or C) directly in a Python module. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center text-primary-desat\">\n                  \"Years ago using Python to do your scientific computation was more a matter of taste (or even faith!) than a logical decision.\"\n                </p>\n              </blockquote>\n              <p>\n                Fast forward today, Python is THE dominant language in Data Science and Machine Learning and by a big margin. The entire PyData stack is a big ecosystem of libraries developed mostly independently by a bunch of people. Each one of them is the result of a team (or often a single person!) who decided to build something that could be used reliably by other people. \n              </p>\n              <p>\n                Building a sophisticated model or analysis is really cool, but when you can really trust it and use it in another software, model or analysis, then there is a huge compounding effect. I think this is the lessons we can learn from all those components we are using everyday: <i>numpy, scipy, pandas, scikit-learn, tensorflow, pytorch</i>, etc. Good software engineering is key.\n              </p>\n              <h3 class=\"font-weight-bold mt-8\">\n                Ok… but what’s the difference between Deep Learning and AI again???\n              </h3>\n              <p>\n                When I was still an undergraduate, one of the first books I read about the history of AI was <a href=\"https://www.amazon.ca/Ai-Tumultuous-History-Artificial-Intelligence/dp/0465029973\">AI: The Tumultuous History of the Search for Artificial Intelligence</a> by Daniel Crevier. This is a truly excellent book about the history of what we now call the “Good old Fashioned AI”. \n              </p>\n              <p>\n                Starting with the Dartmouth Workshop, the book goes through the birth of the discipline, Newell & Simon work on <i>Logic Theorist</i>, the perceptron, symbolic systems, expert systems etc. The book explains in great detail the tumultuous history of the AI field punctuated by a number of hype and disillusionment phases. After reading this you can understand why the word <i>artificial intelligence</i> was pretty much a taboo word for a number of years in many academic circles. It was tainted by a lot of unrealistic promises so most researchers decided to depart a little bit from that grand vision of automating complex human tasks. \n              </p>\n              <p>\n                During those years, researchers were essentially continuing doing all sorts of interesting things, but using different names: <i>computer vision, operational research, optimization, natural language processing, reinforcement learning,</i> etc. Before 2015, you would very rarely hear the term AI. But for some reason, the success of deep learning brought back that word to life. \n              </p>\n              <p>\n                It was actually quite strange to witness but I think it simply reflected the optimism of researchers and  machine learning practitioners that maybe this new approach, used in conjunction with the arsenal of modern computer science, could really lead to the automation of tasks that were impossible to do previously, and it turns out it was the case. This inflection didn’t happen immediately. But when deep learning started to be applied to a broader range of problems (<i>sentence-to-sentence</i> model in NLP, <i>Deep Q Network</i> in reinforcement learning, etc.) we clearly started to see the shift happen.\n              </p>\n              <h3 class=\"font-weight-bold mt-8\">\n                So what does this all mean for Data Science?\n              </h3>\n              <p>\n                There is common misconception in non-technical audiences (and even some technical audiences for that matter), about the current state of AI and the role played by deep learning models in those systems. Current deep learning models are really really good in perception tasks and pattern recognition. In most applications, deep learning models are used for detection, perception or pattern recognition.\n              </p>\n              <p>\n                More broadly speaking, they are really good at learning useful representations. After the first “perception” step, a simple prediction is usually done (classify an image, turn a wheel, find the most likely words, etc.) and then the traditional arsenal of modern computer science usually kicks-in. In most cases, statistical modelling is also required. This is true for self-driving cars, AI applied to health, fraud detection, etc. Hopefully, it turns out that there is a LOT of tasks where the automation is actually limited by this perception layer. So in those cases, current AI systems are really effective.\n              </p>\n              <p>\n                Current deep learning models require a ton of data. In a sense, they are the perfect complement to modern big data systems: They can digest an enormous amount of data and make sense of it. The latest <i>Transformer</i> models are the best example of this process. Language models, like BERT, ELMo, and GPT2 seem to have an infinite appetite for data. Every month we see  another paper on arxiv about a new transformer model trained on an even bigger dataset and the trend is pretty clear: The more the better! <i>Convolution Neural Networks</i> seem to have the same infinite appetite.\n              </p>\n              <p>\n                So does it mean we can only use deep learning models when we have a ton of labeled data? Well not exactly. Yes the vision model embedded in a Tesla car requires a ton of pre-annotated images to effectively detect cars, pedestrians and dogs. But once this model is trained and deployed into the wild it actually <strong class=\"text-info\">generates data and metrics</strong> that would have been impossible to gather otherwise. And the same is true for a lot of other applications.\n              </p>\n              <p>\n                For example, once you have an effective pose estimation model for humans, you can use it to gather a ton of other metrics in the live feed of a soccer game. This is the area of application where current neural networks are really good, and this is where there is the greatest misconception. Current neural networks models do not make complex human-like decisions, they make simple decisions and actually generate new data that can be analysed to make better complex decisions.\n              </p>\n              <p>\n                Exactly like the big data systems of some years ago gave us a new definition of data, new AI systems expand our definition of what is “data”. A raw feed of  log events in the 80s was not really a useful source of data because no system was powerful enough to ingest it into a simple row in a database. Modern data pipelines are now entirely based on log events. A raw video feed in the 90s was not a useful source of data for the same reason, but now we have models that give us the confidence that we can transform this video into useful metrics.\n              </p>\n              <p>\n                So AI, like all the other buzzwords before it, might fade into disuse, but the concepts and techniques it has brought forth into daily usage will remain with us for a very long time.\n              </p>\n            </div>\n          </div> \n        </div> \n      </section>)\n}","import React from \"react\";\n\n\nexport default function CatherineJoinsTeamPost(props){\n\n  return (<section class=\"pt-0 pt-md-0 pb-0\">\n            <div class=\"container\">\n              <div class=\"row justify-content-center\">\n                <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                  <p>Hectiq.ai team is happy to announce the arrival of Catherine as an Ai Research Scientist. She will work along us to pursue new industrial challenges while bringing balance to the force.\n                  </p>\n                  <img src=\"/assets/img/photos/catherine.jpg\" class=\"img-fluid rounded lift lift-lg mt-8 mb-8 mx-auto d-block w-lg-25\" alt=\"...\"/>\n                  <h5 class=\"text-uppercase mt-8 text-info\">Short Bio</h5>\n                  <p class=\"text-secondary\">\n                    Catherine completed her postgraduate studies at the University de Moncton, specializing in AI-based steganalysis. After acquiring her degree, Catherine worked as an AI researcher in different R&D departments (INO, Zilia), broadening and sharpening her proficiency with multiple frameworks. Her passion brought her naturally to hectiq.ai, where she gets to apply and adapt innovative methods in machine learning toward industrial problematics.\n                  </p>\n                </div>\n              </div> \n            </div> \n          </section>)\n\n}","import React from 'react';\n\n\nfunction Header(props){\n  return (<section class=\"pt-12 pt-md-14 pb-12 pb-md-15 bg-gray-900\" style={{marginTop:-83}}>\n            <div class=\"container\">\n              <div class=\"row justify-content-center\">\n                <div class=\"col-12 col-md-10 col-lg-7 text-center\">\n                  <h1 class=\"display-2 font-weight-bold text-white\">\n                    Blog posts\n                  </h1>\n                  <p class=\"lead text-white-75 mb-4\">\n                    Because we have something to say. \n                  </p>\n                </div>\n              </div>\n            </div>\n          </section>)\n}\n\nexport default Header;\n","import React from 'react';\nimport { Link } from \"react-router-dom\";\n\nfunction NavBar(props){\n  return (<nav class={`navbar navbar-expand-lg ${(props.dark)? \"navbar-dark\": \"navbar-light\" }`}>\n            <div class={(props.boxes)? \"container\": \"container-fuild\"}>\n              <Link class=\"navbar-brand\" to=\"/\">\n                <img src=\"/assets/img/brand.png\" class=\"navbar-brand-img\" alt=\"...\"/>\n              </Link>\n\n              <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n                <span class=\"navbar-toggler-icon\"></span>\n              </button>\n\n              <div class=\"collapse navbar-collapse\" id=\"navbarCollapse\">\n\n                <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n                  <i class=\"fe fe-x\"></i>\n                </button>\n\n                <a class=\"navbar-btn btn btn-sm btn-secondary lift ml-auto\" href=\"https://www.hectiq.ai/\" data-toggle=\"smooth-scroll\" data-offset=\"0\">\n                  Contact\n                </a> \n              </div>\n            </div>\n          </nav>)\n}\n\nexport default NavBar;","import React from \"react\";\nimport { Link } from \"react-router-dom\";\n\n\nfunction truncate(str, length) {\n    return str.length > length ? str.substring(0, length-3) + \"...\" : str;\n}\n\nfunction LinkCard(props){\n  return (<Link class={`card lift lift-lg shadow-light-lg mb-7 ${props.bg}`} to={props.to}>\n            {props.children}\n          </Link>)\n}\nfunction DivCard(props){\n  return (<div class={`card lift lift-lg shadow-light-lg mb-7 ${props.bg}`}>\n            {props.children}\n          </div>)\n}\n\n\nfunction TextCard(props){\n  var colors = {\n    text: \"text-muted\",\n    bg: \"bg-white\",\n    title: \"text-dark\"\n\n  }\n  if (props.dark){\n    colors = {\n      text: \"text-white-50\",\n      bg: \"bg-secondary\",\n      title: \"text-light\"\n    }\n  }\n\n  var Container = DivCard;\n  var containerProps = {bg:colors.bg}\n  if (props.content){\n    Container = LinkCard; \n    containerProps = {bg:colors.bg, to:`/posts/${props.id}`}\n\n  }\n\n\n  return (<div class=\"col-12 col-sm-4 col-md-4 d-flex\">\n            <Container {...containerProps} >\n              {(props.frontImgUrl)?\n                <img class=\"card-img-top mb-0 mt-0\" src={ props.frontImgUrl } alt=\"...\"/>\n              : null} \n              <div class=\"card-body my-auto\">\n                <h6 class={`text-uppercase mb-1 ${colors.text}`}>{props.date}</h6>\n                <h4 class={`mb-0 mb-2 ${colors.title}`}>{ props.title }</h4>\n                <p class={`mb-0 ${colors.text}`}> { truncate(props.excerpt || props.summary , 100) }</p>\n              </div>\n            </Container>\n          </div>);\n}\n\nexport default TextCard;","import React from \"react\";\nimport { Link } from \"react-router-dom\";\n\nfunction truncate(str, length) {\n    return str.length > length ? str.substring(0, length-3) + \"...\" : str;\n}\n\n\nfunction MainCard(props){\n  var colors = {\n    text: \"text-muted\",\n    bg: \"bg-white\",\n    title: \"text-dark\"\n\n  }\n  if (props.dark){\n    colors = {\n      text: \"text-white-50\",\n      bg: \"bg-secondary\",\n      title: \"text-light\"\n    }\n  }\n  return (<div class=\"col-12\">\n            <Link class={`card lift lift-lg shadow-light-lg mb-7 ${colors.bg}`} to={`/posts/${props.id}`}>\n              {(props.frontImgUrl)?\n                <img class=\"card-img-top mb-0 mt-0\" src={ props.frontImgUrl } alt=\"...\"/>\n              : null} \n              <div class=\"card-body my-auto\">\n                <h6 class={`text-uppercase mb-1 ${colors.text}`}>{props.date}</h6>\n                <h4 class={`mb-0 mb-2 ${colors.title}`}>{ props.title }</h4>\n                <p class={`mb-0 ${colors.text}`}> { truncate(props.excerpt || props.summary, 100) }</p>\n              </div>\n            </Link>\n          </div>);\n}\n\nexport default MainCard;","import React from \"react\";\n\n\nexport default function Footer(props){\n  return (\n        <footer class=\"py-0 py-md-0 pb-10 {{ includes.classList }}\">\n          <div class=\"container border-top border-gray-300 pt-8\">\n            <div class=\"row  mb-4\">\n              <div class=\"col-6\">\n                <p class=\"text-gray-700 mb-2\">\n                  A blog by Hectiq.ai\n                </p>\n              </div>\n\n              <div class=\"col-6 ml-auto\">\n                <ul class=\"list-unstyled list-inline list-social mb-6 mb-md-0 text-right\">\n                  <li class=\"list-inline-item list-social-item mr-3\">\n                    <a href=\"https://twitter.com/HectiqAI\" class=\"text-decoration-none\">\n                      <img src=\"/assets/img/icons/social/twitter.svg\" class=\"list-social-icon\" alt=\"...\"/>\n                    </a>\n                  </li>\n                  <li class=\"list-inline-item list-social-item mr-3\">\n                    <a href=\"https://www.linkedin.com/company/10141301/\" class=\"text-decoration-none\">\n                      <img src=\"/assets/img/icons/social/linkedin.svg\" class=\"list-social-icon\" alt=\"...\"/>\n                    </a>\n                  </li>\n                </ul>\n              </div>\n\n            </div>\n          </div>\n        </footer>)\n}","import React from \"react\";\nimport MetaTags from 'react-meta-tags';\n\n\nexport default function Meta({title, ...props}){\n  let url = `https://blog.hectiq.ai/posts/${props.id}`\n\n  return (<MetaTags>\n            <title>{title}</title>\n            <meta property=\"og:site_name\" content=\"Hectiq.AI Blog\"/>\n            <meta property=\"og:type\" content=\"article\"/>\n            <meta property=\"og:title\" content={title}/>\n            <meta property=\"og:image\" content={props.metaImage}/>\n            <meta property=\"og:url\" content={url}/>\n            <meta name=\"twitter:card\" content={props.twitterCard}/>\n            <meta name=\"twitter:title\" content={title}/>\n            <meta name=\"twitter:description\" content={props.twitterDescription}/>\n            <meta name=\"twitter:url\" content={url}/>\n            <meta name=\"twitter:site\" content=\"@HectiqAI\"/>\n            <meta property=\"og:image:width\" content={props.metaImageWidth}/>\n            <meta property=\"og:image:height\" content={props.metaImageHeight}/>\n          </MetaTags>)\n}","import React from \"react\";\n\n\nfunction truncate(str, length) {\n    return str.length > length ? str.substring(0, length-3) + \"...\" : str;\n}\n\n\nfunction RowPost(props){\n  var colors = {\n    text: \"text-muted\",\n    bg: \"bg-white\",\n    title: \"text-dark\"\n\n  }\n\n  var Container = \"div\";\n  if (props.content){\n    Container = \"a\";\n  }\n\n\n  return (<div class=\"list-group-item d-flex align-items-center\">\n          <div class=\"mr-auto\">\n             <h6 class=\"text-uppercase mb-1 text-muted\">{ props.header }</h6>\n              <Container href={`/posts/${props.id}`}><h4 class=\"mb-0\">{ props.title }</h4></Container>\n              <p class={`mb-0 ${colors.text}`}> { truncate(props.excerpt, 120) }</p>\n          </div>\n          <div class=\"badge text-wrap ml-10 text-right\">\n            <p class=\"text-uppercase text-muted\">{ props.date }</p>\n          </div>\n        </div>);\n}\n\nexport default RowPost;","import * as React from \"react\";\n\nimport data from \"../posts/posts.react\";\n\nimport {NavBar, Header, TextCard, MainCard, RowPost, Footer} from \"../components\";\n\nclass HomePage extends React.Component {\n\n  render() {\n    const result = data.filter(o => !o.hide).filter(o => {\n      let dateObj = new Date(o.date);\n      return dateObj<Date.now()\n    });\n    return (<div>\n              <NavBar boxes dark/>\n                <Header/>\n                <section class=\"py-8 py-md-11 mt-n10 mt-md-n14\">\n                  <div class=\"container\">\n                    <div class=\"row\" >\n                      {(result.slice(0,1).map(({date, title, excerpt, ...props})=>{\n                        return (<MainCard date={date} title={title} excerpt={excerpt} {...props} />)\n                      }))}\n                      {(result.slice(1,4).map(({date, title, excerpt, ...props}, i)=>{\n                        return (<TextCard date={date} title={title} excerpt={excerpt} {...props} />)\n                      }))}\n                    </div>\n\n                    <div class=\"list-group list-group-flush mt-6\">  \n                      {(result.slice(4).map(({date, title, excerpt, ...props}, i)=>{\n                        return (<RowPost  date={date} title={title} excerpt={excerpt} {...props} />)\n                      }))}\n                    </div>\n\n                  </div>\n                </section>\n              <Footer/>\n            </div>)\n  }\n}\n\nexport default HomePage;","const metaTags = {\n  voxmedia: {\n    metaImage: \"/assets/img/photos/voxmediacollab-twitter.png\",\n    metaImageWidth: \"915\",\n    metaImageHeight: \"484\",\n    twitterCard: \"summary_large_image\",\n    twitterImage: \"/assets/img/photos/voxmediacollab-twitter.png\",\n    twitterDescription: \"Hectiq.AI completes an exciting collaboration with Vox Media for better understanding their audience. In this post, we discuss what motivated this project, and we explain how we developed a privacy-friendly, crazy fast, and highly effective deep learning model.\",\n    metaDescription: \"Hectiq.AI completes an exciting collaboration with Vox Media for better understanding their audience. In this post, we discuss what motivated this project, and we explain how we developed a privacy-friendly, crazy fast, and highly effective deep learning model.\",\n  },\n}\n\nexport default metaTags;","import {HomePage, PostPage} from \"./pages\"\n\n\n// Define your routes here for breadcrumb\nconst routes = [\n  { path: \"/\", name: \"Home\", strict: false, exact:true, isPrivate:true, Component: HomePage },\n  { path: \"/posts/:postid\", name: \"Home\", strict: false, exact:true, isPrivate:true, Component: PostPage },\n];\nexport default routes;","import * as React from \"react\";\nimport { Redirect } from \"react-router-dom\";\n\n\nimport data from \"../posts/posts.react\";\nimport metaTags from \"../posts/metatags.react\";\n\nimport {NavBar, Footer, Meta} from \"../components\";\n\nclass PostPage extends React.Component {\n  constructor(props){\n    super(props)\n    this.postId = this.props.match.params.postid\n    this.data  = data.find(o=>o.id === this.postId)\n  }\n  render() {\n    if ((this.data==null)||(this.data.content==null)){\n      return (<Redirect to=\"/404\"/>)\n    }\n    return (<div>\n              <Meta title={this.data.title} {...metaTags[this.data.id]}/>\n\n              <NavBar boxes/>\n\n              {(this.data.frontWideImg)? \n                <section data-jarallax data-speed=\".8\" class=\"py-12 py-md-15 py-sm-0 bg-cover jarallax\" style={{backgroundImage: `url(${this.data.frontWideImg})`}}></section>\n                : null}\n\n              {/*Content*/}\n              <section class=\"pt-8 pt-md-11 pb-0\">\n                <div class=\"container\">\n                  <div class=\"row justify-content-center\">\n                    <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                      <h6 class=\"text-uppercase mb-1 text-muted\">{this.data.date}</h6>\n                      <h1 class=\"display-4 font-weight-bold\">\n                        { this.data.title }\n                      </h1>\n                      {(this.data.by)? <p class=\"text-secondary  \">By {this.data.by}</p> : null}\n                      <p class=\"lead mb-7 text-muted\" dangerouslySetInnerHTML={{__html: this.data.summary}}>\n                      </p>\n                      <hr class=\"hr-md mb-7\"/>\n                    </div>\n                  </div>\n                </div>\n              </section>\n              {this.data.content()}\n\n              {/*Post footer*/}\n              <section class=\"pt-2 pt-md-11 pb-8\">\n                <div class=\"container \">\n                  {((this.data.authors!=null)||(this.data.credits!=null)||(this.data.moreInfo!=null))? <hr class=\"border-gray-300\"/> : null}\n                  <div class=\"list-group list-group-flush border-0 mb-1 mr-lg-12 ml-md-4 \">\n                    {(this.data.authors)? \n                      <div class=\"row\">\n                        <div class=\"col-sm-4 col-md-4 col-lg-2\">\n                          <p class=\"text-muted\">\n                            {(this.data.authors.length>1)? \"Authors\": \"Author\"}\n                          </p>\n                        </div>\n                        <div class=\"col-sm-8 col-md-8 col-lg-10\">\n                          <p class=\"font-size-sm text-muted \">\n                            {this.data.authors.join(', ')}\n                          </p>\n                        </div>\n                      </div>: null}\n\n                      {(this.data.credits)? \n                        <div class=\"row border-top pt-4 pb-4\">\n                          <div class=\"col-sm-4 col-md-4 col-lg-2\">\n                            <p class=\"text-muted \">\n                              Credits\n                            </p>\n                          </div>\n                          <div class=\"col-sm-8 col-md-8 col-lg-10\">\n                            <p class=\"font-size-sm text-muted mb-0\" dangerouslySetInnerHTML={{__html: this.data.credits}}>\n                            </p>\n                          </div>\n                        </div>: null}\n\n                      {(this.data.moreInfo)? \n                        <div class=\"row border-top pt-4 pb-4\">\n                          <div class=\"col-sm-4 col-md-4 col-lg-2\">\n                            <p class=\"text-muted \">\n                              Learn more\n                            </p>\n                          </div>\n                          <div class=\"col-sm-8 col-md-8 col-lg-10\">\n                            <p class=\"font-size-sm text-muted mb-0\" dangerouslySetInnerHTML={{__html: this.data.moreInfo}}>\n                            </p>\n                          </div>\n                        </div>: null}\n                  </div>\n                </div>\n              </section>\n\n              <Footer/>\n\n            </div>)\n  }\n}\n\nexport default PostPage;","import * as React from \"react\";\nimport PropTypes from 'prop-types';\n\nclass ErrorPage extends React.Component {\n\n  render() {\n    return (<div class=\"container d-flex flex-column\">\n              <div class=\"row align-items-center justify-content-center no-gutters min-vh-100\">\n                <div class=\"col-12 col-md-5 col-lg-4 py-8 py-md-11\">\n                  \n                  <h1 class=\"display-3 font-weight-bold text-center\">\n                    {this.props.title}\n                  </h1>\n\n                  <p class=\"mb-5 text-center text-muted\">\n                    {this.props.description}\n                  </p>\n\n                  <div class=\"text-center\">\n                    <a class=\"btn btn-primary\" href=\"/\">\n                      Back to safety\n                    </a>\n                  </div>\n\n                </div>\n              </div>\n            </div>)\n  }\n}\n\nErrorPage.propTypes = {\n  title: PropTypes.string.isRequired,\n  description: PropTypes.string.isRequired,\n}\n\n\nexport default ErrorPage;","import * as React from \"react\";\nimport ErrorPage from \"./ErrorPage.react\"\n\n\nfunction Error404Page(props){\n  let title = \"You should not be here.\",\n      description = \"This page does not exist\";\n\n  return (<ErrorPage title={title} description={description}/>)\n}\n\n\nexport default Error404Page;","import React from 'react';\n\nimport { BrowserRouter, Route, Switch } from \"react-router-dom\";\nimport routes from \"./routes.js\";\n\nimport * as Errors from \"./pages/errors\"\n\n\nclass App extends React.Component {\n\n    constructor(props) {  \n      super(props);\n      this.state = {}\n    };\n\n    render() {\n      \n      return (<BrowserRouter basename={process.env.PUBLIC_URL}>\n                <Switch>\n                  {routes.map(({path,  isPrivate, strict, exact, isDemoShop, Component}={}, key)=> {\n                    return (<Route exact={exact} strict={strict} path={path} render={(props) => {\n                                    return ( <Component {...props} />)}} />)\n                  })}\n                  <Route component={Errors.Error404} />\n                </Switch>\n              </BrowserRouter>\n      );\n    }\n}\n\n\nexport default App;","// @flow\n\nimport React from \"react\";\nimport ReactDOM from \"react-dom\";\n\nimport App from \"./App.react\";\n\nimport \"./index.scss\";\n\nconst rootElement = document.getElementById(\"root\");\n\nif (rootElement) {\n  ReactDOM.render(<App/>, rootElement);\n} else {\n  throw new Error(\"Could not find root element to mount to!\");\n}\n"],"sourceRoot":""}