{"version":3,"sources":["posts/EfficientNetPost.react.js","posts/posts.react.js","posts/VoxMediaPost.react.js","posts/DecadeOfSciencePost.react.js","posts/CatherineJoinsTeamPost.react.js","components/Header.react.js","components/NavBar.react.js","components/TextCard.react.js","components/MainCard.react.js","components/Footer.react.js","components/Meta.react.js","components/RowPost.react.js","pages/HomePage.react.js","posts/metatags.react.js","routes.js","pages/PostPage.react.js","pages/errors/ErrorPage.react.js","pages/errors/404.react.js","history.js","App.react.js","index.js"],"names":["EfficientNetPost","data","date","title","excerpt","summary","authors","by","content","props","react_default","a","createElement","id","hide","frontImgUrl","frontWideImg","frontImgUrlWide","credits","moreInfo","class","href","data-fancybox","src","alt","dark","Header","style","marginTop","NavBar","concat","boxes","type","data-toggle","data-target","aria-controls","aria-expanded","aria-label","data-offset","TextCard","colors","text","bg","str","length","Container","substring","MainCard","Footer","Meta","_ref","Object","objectWithoutProperties","url","lib_default","property","metaImage","name","twitterCard","twitterDescription","metaImageWidth","metaImageHeight","RowPost","header","HomePage","result","filter","o","react","NavBar_react","Header_react","slice","map","MainCard_react","assign","_ref2","i","TextCard_react","_ref3","RowPost_react","React","metaTags","voxmedia","twitterImage","metaDescription","routes","path","strict","exact","isPrivate","Component","PostPage","_this","classCallCheck","this","_super","call","postId","match","params","postid","find","react_router","to","data-jarallax","data-speed","backgroundImage","dangerouslySetInnerHTML","__html","join","ErrorPage","description","Error404Page","ErrorPage_react","createBrowserHistory","App","state","history","arguments","undefined","isDemoShop","render","component","Errors","rootElement","document","getElementById","Error","ReactDOM","App_react"],"mappings":"4NAOeA,ICuDAC,EAzDF,CACX,CACEC,KAAM,gBACNC,MAAO,+CACPC,QAAS,qLACTC,QAAS,qLACTC,QAAS,CAAC,oBACVC,GAAI,mBACJC,QDXJ,SAA0BC,GACxB,OAAQC,EAAAC,EAAAC,cAAA,aCWNC,GAAI,eACJC,MAAM,GAER,CACEZ,KAAM,iBACNC,MAAO,gFACPC,QAAS,wPACTW,YAAa,sCACbC,aAAc,2CACdC,gBAAiB,sCACjBZ,QAAS,knBACTC,QAAS,CAAC,kBACVY,QAAS,0IACTC,SAAU,sIACVZ,GAAI,yDACJM,GAAI,WACJL,QC3BJ,SAAsBC,GACpB,OACMC,EAAAC,EAAAC,cAAA,WAASQ,MAAM,qBACbV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,aACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,8BACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,uCACTV,EAAAC,EAAAC,cAAA,gaAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yBAAV,sCAGAV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,QAAT,2MAGAV,EAAAC,EAAAC,cAAA,KAAGS,KAAK,8CAA8CC,iBAAA,GACpDZ,EAAAC,EAAAC,cAAA,OAAKW,IAAI,2DAA2DH,MAAM,YAAYI,IAAI,SAE5Fd,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,QAAT,2UACqUV,EAAAC,EAAAC,cAAA,+BADrU,+TAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,0CAAT,8LAIFV,EAAAC,EAAAC,cAAA,gSAGAF,EAAAC,EAAAC,cAAA,0JAGAF,EAAAC,EAAAC,cAAA,8UAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,2BACRV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,mEAIFF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,2GAKJF,EAAAC,EAAAC,cAAA,6cAIAF,EAAAC,EAAAC,cAAA,qQAIAF,EAAAC,EAAAC,cAAA,gVACsUF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,aAAZ,8GAEtUV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yBAAV,yEAIAV,EAAAC,EAAAC,cAAA,qcAIAF,EAAAC,EAAAC,cAAA,ygBAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,0CAAT,oLAIFV,EAAAC,EAAAC,cAAA,qaAIAF,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,oCACZV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,4CAA4CG,IAAI,iCAAiCC,IAAI,QAChGd,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,8BAAlB,6OAIFV,EAAAC,EAAAC,cAAA,gdAGAF,EAAAC,EAAAC,cAAA,+UAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,0CAAT,uIAIFV,EAAAC,EAAAC,cAAA,2qBAGAF,EAAAC,EAAAC,cAAA,2PAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yBAAV,4CAIAV,EAAAC,EAAAC,cAAA,oMAC0LF,EAAAC,EAAAC,cAAA,iDAD1L,wFAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,sBACRV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,iCAAZ,kEAAiHV,EAAAC,EAAAC,cAAA,WACjHF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,cAAZ,oOACiOV,EAAAC,EAAAC,cAAA,iDADjO,OAKJF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,iCAAZ,kDAAiGV,EAAAC,EAAAC,cAAA,WACjGF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,cAAZ,+FAKJV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,iCAAZ,kDAAiGV,EAAAC,EAAAC,cAAA,WACjGF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,cAAZ,yFAKJV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,iCAAZ,kDAAiGV,EAAAC,EAAAC,cAAA,WACjGF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,cAAZ,6HAC0HV,EAAAC,EAAAC,cAAA,8BAD1H,uEAONF,EAAAC,EAAAC,cAAA,ghBAGAF,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,oCACZV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,+BAA+BG,IAAI,+BAA+BC,IAAI,QACjFd,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,8BAAlB,uLAIFV,EAAAC,EAAAC,cAAA,8hBAIAF,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,gDACZV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,iDAAiDG,IAAI,gCAAgCC,IAAI,QACpGd,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,8BAAlB,yJAKFV,EAAAC,EAAAC,cAAA,qEAIAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,2BACRV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,8EAIFF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,oGAIFF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,UACRV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2DACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,iBAEXV,EAAAC,EAAAC,cAAA,mKAMJF,EAAAC,EAAAC,cAAA,yRAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,0CAAT,4JAKFV,EAAAC,EAAAC,cAAA,8UDhMd,CACEV,KAAM,oBACNC,MAAO,iDACPC,QAAS,wKACTW,YAAa,8BACbE,gBAAiB,mCACjBZ,QAAS,wKACTE,GAAI,yDACJD,QAAS,CAAC,kBACVE,QEtCW,SAA6BC,GAE1C,OACIC,EAAAC,EAAAC,cAAA,WAASQ,MAAM,qBACbV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,aACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,8BACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,uCACTV,EAAAC,EAAAC,cAAA,igBAGAF,EAAAC,EAAAC,cAAA,8MACoMF,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,gBAAd,4BADpM,+IAGAV,EAAAC,EAAAC,cAAA,qpBAGAF,EAAAC,EAAAC,cAAA,uiBAIAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yBAAV,+BAIAV,EAAAC,EAAAC,cAAA,mfACqdF,EAAAC,EAAAC,cAAA,2BADrd,6EAGAF,EAAAC,EAAAC,cAAA,4OACwNF,EAAAC,EAAAC,cAAA,2BADxN,ySAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,oBACVV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,iCAAV,cACAV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yBAAV,yBAGAV,EAAAC,EAAAC,cAAA,onBAGAF,EAAAC,EAAAC,cAAA,yeAGAF,EAAAC,EAAAC,cAAA,+SAGAF,EAAAC,EAAAC,cAAA,OAAKW,IAAI,yCAAyCH,MAAM,wCAAwCI,IAAI,QACpGd,EAAAC,EAAAC,cAAA,2tBAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,0CAAT,iLAIFV,EAAAC,EAAAC,cAAA,kiBAGAF,EAAAC,EAAAC,cAAA,yYAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,oBACVV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,iCAAV,eACAV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,oBAAV,aAGAV,EAAAC,EAAAC,cAAA,ogBAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,0CAAT,2FAIFV,EAAAC,EAAAC,cAAA,sOACkNF,EAAAC,EAAAC,cAAA,KAAGS,KAAK,sDAAR,2DADlN,6SAGAX,EAAAC,EAAAC,cAAA,OAAKW,IAAI,uCAAuCH,MAAM,sCAAsCI,IAAI,QAChGd,EAAAC,EAAAC,cAAA,qSAGAF,EAAAC,EAAAC,cAAA,8HACoHF,EAAAC,EAAAC,cAAA,qBADpH,gFACgNF,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,gBAAZ,iHADhN,+SAGAV,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,sCAAT,oDAKFV,EAAAC,EAAAC,cAAA,o7BAGAF,EAAAC,EAAAC,cAAA,gZACiYF,EAAAC,EAAAC,cAAA,kBADjY,uNAC4lBF,EAAAC,EAAAC,cAAA,qCAD5lB,YAGAF,EAAAC,EAAAC,cAAA,udAGAF,EAAAC,EAAAC,cAAA,wiBAC+gBF,EAAAC,EAAAC,cAAA,yDAD/gB,oHAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,oBACVV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,iCAAV,eACAV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,oBAAV,wBAGAV,EAAAC,EAAAC,cAAA,u1BAGAF,EAAAC,EAAAC,cAAA,4TAGAF,EAAAC,EAAAC,cAAA,wkBAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,2CAAT,qHAIFV,EAAAC,EAAAC,cAAA,mwBAGAF,EAAAC,EAAAC,cAAA,icAGAF,EAAAC,EAAAC,cAAA,8uBAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,oBACVV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,iCAAV,cACAV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,oBAAV,kCAGAV,EAAAC,EAAAC,cAAA,snBAGAF,EAAAC,EAAAC,cAAA,+QAGAF,EAAAC,EAAAC,cAAA,+QAGAF,EAAAC,EAAAC,cAAA,yJAGAF,EAAAC,EAAAC,cAAA,qVAGAF,EAAAC,EAAAC,cAAA,y7BAGAF,EAAAC,EAAAC,cAAA,yXAGAF,EAAAC,EAAAC,cAAA,cAAYQ,MAAM,wBAChBV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,0CAAT,oIAIFV,EAAAC,EAAAC,cAAA,wXAGAF,EAAAC,EAAAC,cAAA,8RACoRF,EAAAC,EAAAC,cAAA,oEADpR,4CAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yBAAV,iFAGAV,EAAAC,EAAAC,cAAA,yGAC+FF,EAAAC,EAAAC,cAAA,KAAGS,KAAK,qFAAR,wEAD/F,mIAGAX,EAAAC,EAAAC,cAAA,4HACkHF,EAAAC,EAAAC,cAAA,2BADlH,kPACsXF,EAAAC,EAAAC,cAAA,oCADtX,8OAGAF,EAAAC,EAAAC,cAAA,0IACgIF,EAAAC,EAAAC,cAAA,sHADhI,gJAGAF,EAAAC,EAAAC,cAAA,ueACwdF,EAAAC,EAAAC,cAAA,iCADxd,kBACkgBF,EAAAC,EAAAC,cAAA,2BADlgB,iFAGAF,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yBAAV,gDAGAV,EAAAC,EAAAC,cAAA,6ZAGAF,EAAAC,EAAAC,cAAA,0nBAGAF,EAAAC,EAAAC,cAAA,oNAC0MF,EAAAC,EAAAC,cAAA,wBAD1M,uSACggBF,EAAAC,EAAAC,cAAA,wCADhgB,6CAGAF,EAAAC,EAAAC,cAAA,mUACyTF,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,aAAd,8BADzT,+GAGAV,EAAAC,EAAAC,cAAA,yeAGAF,EAAAC,EAAAC,cAAA,0jBAGAF,EAAAC,EAAAC,cAAA,wMFlKVC,GAAI,0BAEN,CACEX,KAAM,qBACNC,MAAO,kCACPC,QAAS,uEACTa,gBAAiB,GACjBT,QG9CW,SAAgCC,GAE7C,OAAQC,EAAAC,EAAAC,cAAA,WAASQ,MAAM,qBACbV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,aACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,8BACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,uCACTV,EAAAC,EAAAC,cAAA,sMAEAF,EAAAC,EAAAC,cAAA,OAAKW,IAAI,mCAAmCH,MAAM,mEAAmEI,IAAI,QACzHd,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,iCAAV,aACAV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,kBAAT,mcHqCdK,MAAM,EACNZ,GAAI,4BAEN,CACEX,KAAM,oBACNC,MAAO,2BACPC,QAAS,oGACTa,gBAAiB,GACjBJ,GAAI,gBItCOa,MAjBf,SAAgBjB,GACd,OAAQC,EAAAC,EAAAC,cAAA,WAASQ,MAAM,4CAA4CO,MAAO,CAACC,WAAW,KAC5ElB,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,aACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,8BACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,yCACTV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,yCAAV,cAGAV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,2BAAT,0CCgBHS,MAzBf,SAAgBpB,GACd,OAAQC,EAAAC,EAAAC,cAAA,OAAKQ,MAAK,2BAAAU,OAA8BrB,EAAMgB,KAAO,cAAe,iBAClEf,EAAAC,EAAAC,cAAA,OAAKQ,MAAQX,EAAMsB,MAAQ,YAAa,mBACtCrB,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,eAAeC,KAAK,KAC3BX,EAAAC,EAAAC,cAAA,OAAKW,IAAI,wBAAwBH,MAAM,mBAAmBI,IAAI,SAGhEd,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,iBAAiBY,KAAK,SAASC,cAAY,WAAWC,cAAY,kBAAkBC,gBAAc,iBAAiBC,gBAAc,QAAQC,aAAW,qBAChK3B,EAAAC,EAAAC,cAAA,QAAMQ,MAAM,yBAGdV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,2BAA2BP,GAAG,kBAEvCH,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,iBAAiBY,KAAK,SAASC,cAAY,WAAWC,cAAY,kBAAkBC,gBAAc,iBAAiBC,gBAAc,QAAQC,aAAW,qBAChK3B,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,aAGXV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,mDAAmDC,KAAK,yBAAyBY,cAAY,gBAAgBK,cAAY,KAAlI,eCsBDC,MAlCf,SAAkB9B,GAChB,IAAI+B,EAAS,CACXC,KAAM,aACNC,GAAI,WACJvC,MAAO,aAGLM,EAAMgB,OACRe,EAAS,CACPC,KAAM,gBACNC,GAAI,eACJvC,MAAO,eAIX,IApBgBwC,EAAKC,EAoBjBC,EAAY,MAKhB,OAJIpC,EAAMD,UACRqC,EAAY,KAGNnC,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,mCACTV,EAAAC,EAAAC,cAACiC,EAAD,CAAWzB,MAAK,0CAAAU,OAA4CU,EAAOE,IAAMrB,KAAI,UAAAS,OAAYrB,EAAMI,KAC3FJ,EAAMM,YACNL,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,yBAAyBG,IAAMd,EAAMM,YAAcS,IAAI,QAClE,KACFd,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,qBACTV,EAAAC,EAAAC,cAAA,MAAIQ,MAAK,uBAAAU,OAAyBU,EAAOC,OAAShC,EAAMP,MACxDQ,EAAAC,EAAAC,cAAA,MAAIQ,MAAK,aAAAU,OAAeU,EAAOrC,QAAWM,EAAMN,OAChDO,EAAAC,EAAAC,cAAA,KAAGQ,MAAK,QAAAU,OAAUU,EAAOC,OAAzB,KAjCEE,EAiC2ClC,EAAML,QAjC5CwC,EAiCqD,IAhCjED,EAAIC,OAASA,EAASD,EAAIG,UAAU,EAAGF,EAAO,GAAK,MAAQD,QC2BvDI,MA3Bf,SAAkBtC,GAChB,IAAI+B,EAAS,CACXC,KAAM,aACNC,GAAI,WACJvC,MAAO,aAUT,OAPIM,EAAMgB,OACRe,EAAS,CACPC,KAAM,gBACNC,GAAI,eACJvC,MAAO,eAGHO,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,UACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAK,0CAAAU,OAA4CU,EAAOE,IAAMrB,KAAI,UAAAS,OAAYrB,EAAMI,KACnFJ,EAAMM,YACNL,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,yBAAyBG,IAAMd,EAAMM,YAAcS,IAAI,QAClE,KACFd,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,qBACTV,EAAAC,EAAAC,cAAA,MAAIQ,MAAK,uBAAAU,OAAyBU,EAAOC,OAAShC,EAAMP,MACxDQ,EAAAC,EAAAC,cAAA,MAAIQ,MAAK,aAAAU,OAAeU,EAAOrC,QAAWM,EAAMN,WCrBjD,SAAS6C,EAAOvC,GAC7B,OACMC,EAAAC,EAAAC,cAAA,UAAQQ,MAAM,+CACZV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,6CACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,aACTV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,SACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,sBAAT,wBAKFV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,iBACTV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,iEACRV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,0CACRV,EAAAC,EAAAC,cAAA,KAAGS,KAAK,+BAA+BD,MAAM,wBAC3CV,EAAAC,EAAAC,cAAA,OAAKW,IAAI,uCAAuCH,MAAM,mBAAmBI,IAAI,UAGjFd,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,0CACRV,EAAAC,EAAAC,cAAA,KAAGS,KAAK,6CAA6CD,MAAM,wBACzDV,EAAAC,EAAAC,cAAA,OAAKW,IAAI,wCAAwCH,MAAM,mBAAmBI,IAAI,oCCnBrF,SAASyB,EAATC,GAAgC,IAAjB/C,EAAiB+C,EAAjB/C,MAAUM,EAAO0C,OAAAC,EAAA,EAAAD,CAAAD,EAAA,WACzCG,EAAG,gCAAAvB,OAAmCrB,EAAMI,IAEhD,OAAQH,EAAAC,EAAAC,cAAC0C,EAAA3C,EAAD,KACED,EAAAC,EAAAC,cAAA,aAAQT,GACRO,EAAAC,EAAAC,cAAA,QAAM2C,SAAS,eAAe/C,QAAQ,mBACtCE,EAAAC,EAAAC,cAAA,QAAM2C,SAAS,UAAU/C,QAAQ,YACjCE,EAAAC,EAAAC,cAAA,QAAM2C,SAAS,WAAW/C,QAASL,IACnCO,EAAAC,EAAAC,cAAA,QAAM2C,SAAS,WAAW/C,QAASC,EAAM+C,YACzC9C,EAAAC,EAAAC,cAAA,QAAM2C,SAAS,SAAS/C,QAAS6C,IACjC3C,EAAAC,EAAAC,cAAA,QAAM6C,KAAK,eAAejD,QAASC,EAAMiD,cACzChD,EAAAC,EAAAC,cAAA,QAAM6C,KAAK,gBAAgBjD,QAASL,IACpCO,EAAAC,EAAAC,cAAA,QAAM6C,KAAK,sBAAsBjD,QAASC,EAAMkD,qBAChDjD,EAAAC,EAAAC,cAAA,QAAM6C,KAAK,cAAcjD,QAAS6C,IAClC3C,EAAAC,EAAAC,cAAA,QAAM6C,KAAK,eAAejD,QAAQ,cAClCE,EAAAC,EAAAC,cAAA,QAAM2C,SAAS,iBAAiB/C,QAASC,EAAMmD,iBAC/ClD,EAAAC,EAAAC,cAAA,QAAM2C,SAAS,kBAAkB/C,QAASC,EAAMoD,mBCc7CC,MA1Bf,SAAiBrD,GACf,IANgBkC,EAAKC,EAajBC,EAAY,MAMhB,OALIpC,EAAMD,UACRqC,EAAY,KAINnC,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,6CACXV,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,WACRV,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,kCAAmCX,EAAMsD,QAClDrD,EAAAC,EAAAC,cAACiC,EAAD,CAAWxB,KAAI,UAAAS,OAAYrB,EAAMI,KAAMH,EAAAC,EAAAC,cAAA,MAAIQ,MAAM,QAASX,EAAMN,QAChEO,EAAAC,EAAAC,cAAA,KAAGQ,MAAK,QAAAU,OAhBZ,eAgBI,KAvBIa,EAuByClC,EAAML,QAvB1CwC,EAuBmD,IAtB/DD,EAAIC,OAASA,EAASD,EAAIG,UAAU,EAAGF,EAAO,GAAK,MAAQD,KAwB5DjC,EAAAC,EAAAC,cAAA,OAAKQ,MAAM,oCACTV,EAAAC,EAAAC,cAAA,KAAGQ,MAAM,6BAA8BX,EAAMP,6cCvBnD8D,+JAGF,IAAMC,EAAShE,EAAKiE,OAAO,SAAAC,GAAC,OAAKA,EAAErD,OACnC,OAAQsD,EAAA,yBACEA,EAAA,cAACC,EAAD,CAAQtC,OAAK,EAACN,MAAI,IAClB2C,EAAA,cAACE,EAAD,MACAF,EAAA,yBAAShD,MAAM,kCACbgD,EAAA,qBAAKhD,MAAM,aACTgD,EAAA,qBAAKhD,MAAM,OACP6C,EAAOM,MAAM,EAAE,GAAGC,IAAI,SAAAtB,GAAoC,IAAlChD,EAAkCgD,EAAlChD,KAAMC,EAA4B+C,EAA5B/C,MAAOC,EAAqB8C,EAArB9C,QAAYK,EAAS0C,OAAAC,EAAA,EAAAD,CAAAD,EAAA,4BAC1D,OAAQkB,EAAA,cAACK,EAADtB,OAAAuB,OAAA,CAAUxE,KAAMA,EAAMC,MAAOA,EAAOC,QAASA,GAAaK,MAElEwD,EAAOM,MAAM,EAAE,GAAGC,IAAI,SAAAG,EAAmCC,GAAI,IAArC1E,EAAqCyE,EAArCzE,KAAMC,EAA+BwE,EAA/BxE,MAAOC,EAAwBuE,EAAxBvE,QAAYK,EAAY0C,OAAAC,EAAA,EAAAD,CAAAwB,EAAA,4BAC7D,OAAQP,EAAA,cAACS,EAAD1B,OAAAuB,OAAA,CAAUxE,KAAMA,EAAMC,MAAOA,EAAOC,QAASA,GAAaK,OAItE2D,EAAA,qBAAKhD,MAAM,oCACP6C,EAAOM,MAAM,GAAGC,IAAI,SAAAM,EAAmCF,GAAI,IAArC1E,EAAqC4E,EAArC5E,KAAMC,EAA+B2E,EAA/B3E,MAAOC,EAAwB0E,EAAxB1E,QAAYK,EAAY0C,OAAAC,EAAA,EAAAD,CAAA2B,EAAA,4BAC3D,OAAQV,EAAA,cAACW,EAAD5B,OAAAuB,OAAA,CAAUxE,KAAMA,EAAMC,MAAOA,EAAOC,QAASA,GAAaK,SAS1E2D,EAAA,cAACpB,EAAD,cA7BSgC,aCMRC,EAZE,CACfC,SAAU,CACR1B,UAAW,gDACXI,eAAgB,MAChBC,gBAAiB,MACjBH,YAAa,sBACbyB,aAAc,gDACdxB,mBAAoB,wQACpByB,gBAAiB,8sBCANC,EAJA,CACb,CAAEC,KAAM,IAAK7B,KAAM,OAAQ8B,QAAQ,EAAOC,OAAM,EAAMC,WAAU,EAAMC,UFmCzD1B,GElCb,CAAEsB,KAAM,iBAAkB7B,KAAM,OAAQ8B,QAAQ,EAAOC,OAAM,EAAMC,WAAU,EAAMC,kDCInF,SAAAC,EAAYlF,GAAM,IAAAmF,EAAA,OAAAzC,OAAA0C,EAAA,EAAA1C,CAAA2C,KAAAH,IAChBC,EAAAG,EAAAC,KAAAF,KAAMrF,IACDwF,OAASL,EAAKnF,MAAMyF,MAAMC,OAAOC,OACtCR,EAAK3F,KAAQA,EAAKoG,KAAK,SAAAlC,GAAC,OAAEA,EAAEtD,KAAO+E,EAAKK,SAHxBL,uDAMhB,OAAgB,MAAXE,KAAK7F,MAAiC,MAAnB6F,KAAK7F,KAAKO,SAAiBsF,KAAK7F,KAAKa,KACnDsD,EAAA,cAACkC,EAAA,EAAD,CAAUC,GAAG,SAEfnC,EAAA,yBACEA,EAAA,cAACnB,EAADE,OAAAuB,OAAA,CAAMvE,MAAO2F,KAAK7F,KAAKE,OAAW8E,EAASa,KAAK7F,KAAKY,MAErDuD,EAAA,cAACC,EAAD,CAAQtC,OAAK,IAEX+D,KAAK7F,KAAKe,aACVoD,EAAA,yBAASoC,iBAAA,EAAcC,aAAW,KAAKrF,MAAM,2CAA2CO,MAAO,CAAC+E,gBAAe,OAAA5E,OAASgE,KAAK7F,KAAKe,aAAnB,QAC7G,KAGJoD,EAAA,yBAAShD,MAAM,sBACbgD,EAAA,qBAAKhD,MAAM,aACTgD,EAAA,qBAAKhD,MAAM,8BACTgD,EAAA,qBAAKhD,MAAM,uCACTgD,EAAA,oBAAIhD,MAAM,kCAAkC0E,KAAK7F,KAAKC,MACtDkE,EAAA,oBAAIhD,MAAM,8BACN0E,KAAK7F,KAAKE,OAEZ2F,KAAK7F,KAAKM,GAAK6D,EAAA,mBAAGhD,MAAM,oBAAT,MAAgC0E,KAAK7F,KAAKM,IAAU,KACrE6D,EAAA,mBAAGhD,MAAM,uBAAuBuF,wBAAyB,CAACC,OAAQd,KAAK7F,KAAKI,WAE5E+D,EAAA,oBAAIhD,MAAM,mBAKjB0E,KAAK7F,KAAKO,UAGX4D,EAAA,yBAAShD,MAAM,sBACbgD,EAAA,qBAAKhD,MAAM,cACa,MAAnB0E,KAAK7F,KAAKK,SAAoC,MAAnBwF,KAAK7F,KAAKiB,SAAqC,MAApB4E,KAAK7F,KAAKkB,SAAkBiD,EAAA,oBAAIhD,MAAM,oBAAsB,KACrHgD,EAAA,qBAAKhD,MAAM,+DACP0E,KAAK7F,KAAKK,QACV8D,EAAA,qBAAKhD,MAAM,OACTgD,EAAA,qBAAKhD,MAAM,8BACTgD,EAAA,mBAAGhD,MAAM,cACL0E,KAAK7F,KAAKK,QAAQsC,OAAO,EAAI,UAAW,WAG9CwB,EAAA,qBAAKhD,MAAM,+BACTgD,EAAA,mBAAGhD,MAAM,4BACN0E,KAAK7F,KAAKK,QAAQuG,KAAK,SAGtB,KAENf,KAAK7F,KAAKiB,QACVkD,EAAA,qBAAKhD,MAAM,4BACTgD,EAAA,qBAAKhD,MAAM,8BACTgD,EAAA,mBAAGhD,MAAM,eAAT,YAIFgD,EAAA,qBAAKhD,MAAM,+BACTgD,EAAA,mBAAGhD,MAAM,+BAA+BuF,wBAAyB,CAACC,OAAQd,KAAK7F,KAAKiB,aAGhF,KAER4E,KAAK7F,KAAKkB,SACViD,EAAA,qBAAKhD,MAAM,4BACTgD,EAAA,qBAAKhD,MAAM,8BACTgD,EAAA,mBAAGhD,MAAM,eAAT,eAIFgD,EAAA,qBAAKhD,MAAM,+BACTgD,EAAA,mBAAGhD,MAAM,+BAA+BuF,wBAAyB,CAACC,OAAQd,KAAK7F,KAAKkB,cAGhF,QAKlBiD,EAAA,cAACpB,EAAD,cAtFSgC,mdC2BR8B,+JA9BX,OAAQ1C,EAAA,qBAAKhD,MAAM,gCACTgD,EAAA,qBAAKhD,MAAM,uEACTgD,EAAA,qBAAKhD,MAAM,0CAETgD,EAAA,oBAAIhD,MAAM,0CACP0E,KAAKrF,MAAMN,OAGdiE,EAAA,mBAAGhD,MAAM,+BACN0E,KAAKrF,MAAMsG,aAGd3C,EAAA,qBAAKhD,MAAM,eACTgD,EAAA,mBAAGhD,MAAM,kBAAkBC,KAAK,KAAhC,6BAhBI2D,aCSTgC,MARf,SAAsBvG,GAIpB,OAAQ2D,EAAA,cAAC6C,EAAD,CAAW9G,MAHP,0BAGqB4G,YAFf,qCCJLG,odC8BAC,0CArBX,SAAAA,EAAY1G,GAAO,IAAAmF,EAAA,OAAAzC,OAAA0C,EAAA,EAAA1C,CAAA2C,KAAAqB,IACjBvB,EAAAG,EAAAC,KAAAF,KAAMrF,IACD2G,MAAQ,GAFIxB,uDAOjB,OAAQlF,EAAAC,EAAAC,cAAC0F,EAAA,EAAD,CAAQe,QAASA,GACf3G,EAAAC,EAAAC,cAAC0F,EAAA,EAAD,KACGjB,EAAOb,IAAI,WAAqE,IAAAtB,EAAAoE,UAAA1E,OAAA,QAAA2E,IAAAD,UAAA,GAAAA,UAAA,GAAX,GAAxDhC,EAAmEpC,EAAnEoC,KAAkBC,GAAiDrC,EAA5DuC,UAA4DvC,EAAjDqC,QAAQC,EAAyCtC,EAAzCsC,MAAmBE,GAAsBxC,EAAlCsE,WAAkCtE,EAAtBwC,WAAsB4B,UAAA1E,OAAA,GAAA0E,UAAA,GAC/E,OAAQ5G,EAAAC,EAAAC,cAAC0F,EAAA,EAAD,CAAOd,MAAOA,EAAOD,OAAQA,EAAQD,KAAMA,EAAMmC,OAAQ,SAAChH,GAClD,OAASC,EAAAC,EAAAC,cAAC8E,EAAcjF,QAE1CC,EAAAC,EAAAC,cAAC0F,EAAA,EAAD,CAAOoB,UAAWC,aAflB3C,IAAMU,WCAlBkC,EAAcC,SAASC,eAAe,QAE5C,IAAIF,EAGF,MAAM,IAAIG,MAAM,4CAFhBC,IAASP,OAAO/G,EAAAC,EAAAC,cAACqH,EAAD,MAAQL","file":"static/js/main.f4654ca4.chunk.js","sourcesContent":["import React from \"react\";\n\nfunction EfficientNetPost(props){\n  return (<div></div>)\n}\n\n\nexport default EfficientNetPost;","import VoxMediaPost from \"./VoxMediaPost.react\"\nimport DecadeOfSciencePost from \"./DecadeOfSciencePost.react\";\nimport CatherineJoinsTeamPost from \"./CatherineJoinsTeamPost.react\";\nimport EfficientNetPost from \"./EfficientNetPost.react\";\n\nconst data = [\n  {\n    date: \"June 10, 2020\",\n    title: \"EfficientNet1D: A beast among the miniatures\",\n    excerpt: \"In our efforts to make machine learning accessible for everyone,  we are releasing our 1D implementation of the fairly recent family of models: EfficientNet based on the PyTorch.\",\n    summary: \"In our efforts to make machine learning accessible for everyone,  we are releasing our 1D implementation of the fairly recent family of models: EfficientNet based on the PyTorch.\",\n    authors: [\"Catherine Paulin\"],\n    by: \"Catherine Paulin\",\n    content: EfficientNetPost,\n    id: \"efficientnet\",\n    hide: true,\n  },\n  {\n    date: \"March 30, 2020\",\n    title: \"Vox Media: Building a privacy-friendly first-party data segmentation platform\",\n    excerpt: \"Hectiq.AI completes an exciting collaboration with Vox Media. We have developed an Ai model to help Vox Media better understand their audience. The challenge was tough as the model needed to be privacy-friendly, crazy fast, and highly effective.\",\n    frontImgUrl: '/assets/img/photos/voxmediapost.svg',\n    frontWideImg: \"/assets/img/photos/voxmediapost-wide.svg\",\n    frontImgUrlWide: \"/assets/img/photos/voxmediapost.svg\",\n    summary: \"Hectiq.AI completes an exciting collaboration with Vox Media. We have developed an Ai model to help Vox Media better understand their audience. The challenge was tough as the model needed to be privacy-friendly, crazy fast, and highly effective. In this post, we explain what motivated this project, namely developing an alternative to 'creepy' third-party data, and we discuss how we achieved the task by using state-of-the-art deep learning models. You can also read Vox Media's recent <a href='https://www.voxmedia.com/2020/2/26/21155010/its-not-who-you-are-but-what-you-are-consuming'>white paper</a> about the project.\",\n    authors: [\"Martin Laprise\"],\n    credits: \"Bottom trawling illustration from <i>Cold-water Coral Reefs: out of sight - no longer out of mind. UNEP-WCMC Biodiversity Series 22</i>\",\n    moreInfo: \"<a href='https://www.voxmedia.com/2020/2/26/21155010/its-not-who-you-are-but-what-you-are-consuming'>Vox Media announces Forte.</a>\",\n    by: \"Martin Laprise, Chief Scientist & Founder of Hectiq.AI\",\n    id: \"voxmedia\",\n    content: VoxMediaPost,\n  },\n  {\n    date: \"February 19, 2020\",\n    title: \"A decade of Data Science hypes: The Good Parts\",\n    excerpt: \"In this post, I look back at some of the trends & hypes that punctuated the Data Science world in the last years and take some notes on the lasting impacts they had.\",\n    frontImgUrl: \"/assets/img/photos/idea.jpg\",\n    frontImgUrlWide: \"/assets/img/photos/idea-wide.jpg\",\n    summary: \"In this post, I look back at some of the trends & hypes that punctuated the Data Science world in the last years and take some notes on the lasting impacts they had.\",\n    by: \"Martin Laprise, Chief Scientist & Founder of Hectiq.ai\",\n    authors: [\"Martin Laprise\"],\n    content: DecadeOfSciencePost,\n    id: \"decade-of-data-science\",\n  },\n  {\n    date: \"September 30, 2019\",\n    title: \"Hectiq.ai welcomes a new member\",\n    excerpt: \"Catherine Paulin is joining hectiq.ai to bring balance to the force.\",\n    frontImgUrlWide: \"\",\n    content: CatherineJoinsTeamPost,\n    dark: true,\n    id: \"catherine-joins-the-team\",\n  },\n  {\n    date: \"September 1, 2019\",\n    title: \"Hectiq.ai has a new blog\",\n    excerpt: \"We are proud to announce this new blog website! It will feature our recent achievements and news.\",\n    frontImgUrlWide: \"\",\n    id: \"new-website\",\n  }\n]\n\nexport default data;","import React from \"react\";\n\n\nfunction VoxMediaPost(props){\n  return (\n        <section class=\"pt-0 pt-md-0 pb-0\">\n          <div class=\"container\">\n            <div class=\"row justify-content-center\">\n              <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                <p>\n                  Have you ever felt like someone is spying on you all over the web? You shop for a pair of shoes online... and curiously enough Facebook Ads suggest that specific pair of shoes 10 min later. Not any pair of shoes: The EXACT same pair. What a coincidence! Turns out it’s not a coincidence at all. Welcome to the wonderful world of third-party cookies, real time bidding and data management platforms.\n                </p>\n                <h2 class=\"font-weight-bold mt-8\">\n                  Third-party vs first-party cookies\n                </h2>\n                <p class=\"mb-6\">\n                  If you have never heard of cookies, I would recommend that you watch this excellent video from Vox. It explains what a third-party cookie is and how it has been the core of the Ad industry for years.\n                </p>\n                <a href=\"https://www.youtube.com/watch?v=HFyaW50GFOs\" data-fancybox >\n                  <img src=\"https://img.youtube.com/vi/HFyaW50GFOs/maxresdefault.jpg\" class=\"img-fluid\" alt=\"...\"/>\n                </a>\n                <p class=\"mt-6\">\n                  A cookie is a little piece of data stored somewhere on your browser's device. Cookies were first invented to act as small and useful short term memory for a specific website. When you add an item to your cart, you’re expecting the site to remember it when you come back the next day. That short term memory is handled by a <i>first-party cookie</i>. It can only be accessed by the website itself. A third-party cookie can be set and accessed by a different website than the one it was created on. They are usually managed by ad networks or analytics tools.  And that’s exactly how they know you have placed a pair of yellow shoes in an online cart yesterday. \n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “A first-party cookie can only be accessed by the website that has set it. A third-party cookie can be set and accessed by a different website than the one it was created on.”\n                  </p>\n                </blockquote>\n                <p>\n                  That little piece of information (you shopping for a pair of shoes) is actually quite valuable. Ad networks collect all that data about you for ad retargeting. The next time they see you online, they can show you an ad specially tailored for you (spoiler: it involves shoes). \n                </p>\n                <p>\n                  Usage of third-party cookies raises serious concerns for user privacy and the good news is that the days of third-party cookies are numbered.\n                </p>\n                <p>\n                  With new regulation in place like General Data Protection Regulation in Europe (GPDR) and the new California Consumer Privacy Act (CCPA), using a third-party cookie while staying fully compliant will become more and more difficult. One of the key aspects of these regulations is the right to be forgotten. More crucially:\n                </p>\n                <ul class=\"list-unstyled mb-6 mt-7\">\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      Users can now refuse the sale of their personal data.\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                       Users are able to request deletion of their personal information at any time from a website. \n                    </p>\n                  </li>\n                </ul>\n                <p>\n                  Fulfilling those two simple requirements is nearly impossible with third-party data. For instance, when an ad network tracks your movements on your favorite shoes website, that information is automatically broadcasted and shared among a multitude of other systems. From that point on, the shoes website no longer controls where this data flows. Even if requested, it becomes practically impossible to delete this data or even to find where it went.\n                </p>\n\n                <p>\n                  Those issues have been known for some years now and most browsers (Edge, Chrome, Firefox) had the option to retire third-party cookies. Others like Apple, whom unsurprisingly doesn't own any Ad network, even disabled third-party cookies by default.\n                </p>\n\n                <p>\n                  Recently, Google announced their intentions to completely phase out third-party cookies in Chrome within two years. This announcement puts the final nail in the coffin for third-party cookies. Marketers and Brands now need to find other ways to make their marketing just as effective while respecting the new privacy rules. <span class=\"text-info\">Companies that still have not invested in a forward-looking plan to manage audience will be left behind. </span>\n                </p>\n                <h2 class=\"font-weight-bold mt-8\">\n                  Learning to know your audience using first-party contextual data only\n                </h2>\n\n                <p>\n                  Vox Media is one of the biggest and most respected digital media companies in the US. They have properties like The Verge, Vox, Eater, Polygon, Curbed, SB Nation, etc. They are receiving an incredible amount of visitors every day. Their system makes a great deal of decisions with every click. What content should they show the user? What ad content is the most relevant? And they need to answer those questions a thousand times per second.\n                </p>\n\n                <p>\n                  Months ago, Vox Media approached us with an interesting challenge. Would it be possible to leverage the latest AI methods to build a more privacy-friendly audience segmentation technology without relying on any third-party data? Instead, the prediction could be based on actions users took on their properties, or in other words, the first party-data. It could be the title of the clicked articles, the type of concent within the article, the time of the day, the level of engagement, the user's device, etc.\n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “ Would it be possible to leverage the latest AI methods to build a more privacy-friendly audience segmentation technology without relying on any third-party data? ”\n                  </p>\n                </blockquote>\n                <p>\n                  Ad retargeting by tracking users all over the web is similar to fishing using bottom trawling. It might work, but you’re gathering a whole lot of other data in the process which is highly problematic for a user's privacy. It turns out that if you want to learn more about your audience by using only first-party data, you need to be much smarter about how you’re using the few data points you have. \n                </p>\n\n                <figure class=\"figure pt-8 pb-8 pl-lg-8 pr-lg-8\">\n                  <img class=\"figure-img img-fluid rounded lift lift-lg\" src=\"/assets/img/photos/fishing.png\" alt=\"...\"/>\n                  <figcaption class=\"figure-caption text-center\">\n                    Doing ad retargeting by tracking users all over the web is similar to fishing using bottom trawling. It might work, but you’re gathering a whole lot of other data in the process which is highly problematic for a user's privacy.\n                  </figcaption>\n                </figure>\n                <p>\n                  Most of the data you have about the audience is how they are interacting with the content you placed on your website. For digital media companies, like Vox Media, this content is mostly texts, images, and videos. Thanks to the latest advance in deep learning, we are now much better at analyzing the content of images and texts than just a few years ago. Using Convolutional Neural Network, we can now really get a sense of what an image is all about. \n                </p>\n                <p>\n                  The same type of analysis can be achieved on texts using state-of-the-art Transformer models (like BERT, ELMO, GPT2, etc.) trained on huge amounts of data. Assuming that you can describe in great detail the nature of the content, the challenge is to turn users interactions with your content into interests and intentions.\n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “ As a content provider, the beauty of this approach is that the deep learning models are powered by your own content. ”\n                  </p>\n                </blockquote>\n                <p>\n                  We can illustrate this task using a classic example. You have a restaurant and a new client comes in. You have essentially two ways to know more about him. You can track him everywhere and note every restaurant he visits. This would be quite creepy and the equivalent of using a third-party cookie. Or you can use the first-party data approach and keep track of what he orders and likes in your own restaurant. If you have a deep understanding of all the meals served in your restaurant, you can do a pretty good job of predicting his interests. You can even add up other variables like the day and hour of the week the client visited to push further your understanding. \n                </p>\n                <p>\n                  As a content provider, the beauty of this approach is that the deep learning models are powered by your own content. As the amount of content increases in volume and diversity, the more refined your knowledge of your own audience will be.\n                </p>\n                <h2 class=\"font-weight-bold mt-8\">\n                  Toward a first-party segmentation system\n                </h2>\n\n                <p>\n                  A first-party segmentation system is designed to describe the audience interests and intents based on its viewed content. An example of these segments could be something specific like <i>Camera Enthusiast Currently Shopping</i>. In order to be effective, this system is required to fulfill a number of features: \n                </p>\n                <ul class=\"list-unstyled mb-7\">\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The model needs to learn from large volume of historical data.</span><br/>\n                      <span class=\"text-muted\">\n                      It must find and learn interesting patterns and \"natural\" segments in the data. For instance, it may find that a group of users visit the same content because of a similar interest. It will then be labelled as something like <i>Camera Enthusiast Currently Shopping</i>. \n                      </span>\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The predictions must be computed in real time.</span><br/>\n                      <span class=\"text-muted\">\n                      It must handle a high volume of requests (> 1k/sec) within a reasonable latency (~100ms). \n                      </span>\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The model should understand all content types.</span><br/>\n                      <span class=\"text-muted\">\n                      Content may come in any form like images, texts, numerical attributes, labels, etc.\n                      </span>\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      <span class=\"font-weight-bold text-primary\">The prediction should be valid on unseen data.</span><br/>\n                      <span class=\"text-muted\">\n                      For instance, if a user clicks on a recently published article about a new camera, the model should be able to assign the <i>Camera Enthusiast</i> segment even if the article is absent from the historical data. \n                      </span>\n                    </p>\n                  </li>\n                </ul>\n\n                <p>\n                  A natural way to model the interactions of the users with a set of content is by using the concept of graphs. In this setup we have two types of components: The users and the content. A user and a piece of content are connected if the user clicked on the content. For each content component, we convert raw data (texts and images) into compact attributes (a big vector), using recent deep learning models, and store them for later use. The interaction itself (the pageviews) may also have its own set of attributes.\n                </p>\n                <figure class=\"figure pt-8 pb-8 pl-lg-8 pr-lg-8\">\n                  <img class=\"figure-img img-fluid rounded\" src=\"/assets/img/photos/graph.png\" alt=\"...\"/>\n                  <figcaption class=\"figure-caption text-center\">\n                    A natural representation of the Vox Media dataset. Users are connected with contents they have seen. The model is trained to predict users segments based on this graph structure.\n                  </figcaption>\n                </figure>\n                <p>\n                  In this graph framework, the model task is to assign segments to users. This is done by crunching all the component features and even propagating information from users with similar records. All those attributes need to work together in a cohesive way to inform the final decision. These models, called Graph Neural Networks, are known to be extremely effective on a large variety of machine learning tasks. That's why state-of-the-art Graph Neural Networks are at the core the Vox Media’s first-party segmentation platform.\n                </p>\n\n                <figure class=\"figure pt-8 pb-8 pt-8 pb-8 pl-lg-10 pr-lg-10\">\n                  <img class=\"figure-img img-fluid rounded pl-lg-10 pr-lg-10\" src=\"/assets/img/photos/graph2.png\" alt=\"...\"/>\n                  <figcaption class=\"figure-caption text-center\">\n                    User segments are computed using the attributes of the contents they have seen. Higher level attributes can be built by looking deeper in the graph.\n                  </figcaption>\n                </figure>\n\n                <p>\n                  These models fulfill all the requirements and even more: \n                </p>\n\n                <ul class=\"list-unstyled mb-6 mt-7\">\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      The prediction does not require the actual identity of the user.\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      It is powered by the content owned by the platform, not the demographic user features.\n                    </p>\n                  </li>\n                  <li class=\"d-flex\">\n                    <div class=\"badge badge-rounded-circle badge-success-soft mt-1 mr-4\">\n                      <i class=\"fe fe-check\"></i>\n                    </div>\n                    <p>\n                      The segments are also perfectly inductive, meaning that the model is trained on historical data but the segments extend to unseen pieces of content.\n                    </p>\n                  </li>\n                </ul>\n            \n                <p>\n                  At the end, those first-party predictions are much more effective for the brand and more privacy-friendly than third-party cookies. And all this is happening many hundreds of times per sec; every time a user is interacting with a piece of content on Vox Media Network!\n                </p>\n                <blockquote class=\"blockquote mb-7 mt-7\">\n                  <p class=\"h2 mb-0 text-center text-primary-desat\">\n                    “        At the end, those contextual predictions are much more effective for the brand and more privacy-friendly than third-party cookies.\n                    ”\n                  </p>\n                </blockquote>\n                <p>\n                  Vox Media first-party segmentation platform is an impressive piece of technology and we’re are really proud, at Hectiq.AI, to have participated in its generation. Brands using Vox Media's new platform can now connect effectively to their audience and more importantly, they can do it in a privacy-friendly way.\n                </p>\n              </div>\n            </div> \n          </div> \n        </section>)\n}\n\n\nexport default VoxMediaPost;","import React from \"react\";\n\n\nexport default function DecadeOfSciencePost(props){\n\n  return (\n      <section class=\"pt-0 pt-md-0 pb-0\">\n        <div class=\"container\">\n          <div class=\"row justify-content-center\">\n            <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n              <p>\n                It’s the beginning of 2020 and like every new year, we see an abundance of the usual “year-in-review” blog posts. Looking back is not completely meaningless, but there is something deeply repetitive about those articles. This year I felt it was slightly different - because of the new decade, we saw a bunch of reviews of the previous one. Not all of them are great of course, but I think there is actually something more interesting that results from a look back over the past decade. \n              </p>\n              <p>\n                Ten years is long enough to see trends happening but still short enough to be experienced and remembered by a bunch of people. I thought it would be fun to go through some of the trends & hypes <strong class=\"text-primary\">I personally experienced</strong> as a Data Scientist / Data Engineer / CTO / Algo Guy or Machine Learning Engineer or whatever you want to call it over the past ten years.\n              </p>\n              <p>\n                Coincidently and on a more personal note, I also realized lately that I started doing data science fulltime in 2010. As a Physics PhD student for almost 5 years at that time (yes it's way too long !!!), I was eager to start something new, something more in line with my true passion for physics, numerical computation, software, the web..., a bunch of things that seem totally orthogonal to each other back then. At the time, working on web-related projects was not really viewed as hardcore software engineering and most importantly, no serious physicists were working on software. At least this opinion was part of the common wisdom of that time. \n              </p>\n              <p>\n                To be honest, I felt like my interest profile was a little bit off for a physicist. But then around 2008 something interesting started happening... for some reason people started searching for  people like me. They were looking for a strange mix of skills: software engineering, web tech knowledge, statistics, machine learning, etc. I was not really aware of this at that time, but an increasing amount of companies had started gathering a significant amount of data and they were looking for ways to leverage this in some form or other.\n              </p>\n\n              <h2 class=\"font-weight-bold mt-8\">\n                The infamous Data Scientist\n              </h2>\n\n              <p>\n                Naming something is often a good first step toward understanding something and a decade ago, some software engineers & product managers started realizing that they had a new type of “problem”. They were accumulating a lot of data and they started hiring people to handle & analyze this data, create statistical experiments, build data products, etc. It turns out that it’s really handy to have a job title when you’re looking to hire people, so they coined the term: <i>Data Scientist</i> with all it’s glorious ambiguity on purpose. That’s all it is.\n              </p>\n              <p>\n                When I first started working in startups, I was not really aware of this term nor was my employer, which made the “market” much less efficient. One of the trends I noticed pretty early one is the rise of this term <i>Data Scientist</i> as a job title to a point of almost laughable hype. Nevertheless, I think overall it was a good thing as it crystallized a genuine set of skills which lead to a much more efficient job market. People interested in those things can now find interesting jobs much more easily than a decade ago.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">Early 2009</h5>\n              <h2 class=\"font-weight-bold mt-0\">\n                Recommendation System\n              </h2>\n              <p>\n                From my perspective, recommendation systems were one the first big trends and big wins in Data Science. On September 18th, 2009, Netflix announced the winner of the Netflix prize. The challenge, announced 3 years earlier, was one of the first kaggle-like machine learning challenges and as such, it had a huge influence on the industry. Machine Learning is all about optimizing parameters: The parameters of models, the parameters of training, etc. The idea of treating the “model builder” just as another parameter in the process was a relatively new idea at the time, at least in a non-academic setting.  \n              </p>\n              <p>\n                The open nature of this challenge (and the $1M Grand Prize attached to it !) also lead to interesting contributions from a different set of people not normally engaged with this kind of work. That challenge was also where methods based on regularized Matrix Factorization (“MF”) were introduced for the first time in the context of collaborative filtering. All the winning methods (mostly big ensemble models) involved one or many MF models at some point or another. \n              </p>\n              <p>\n                Those methods are now part of most machine learning libraries and to this day, they are still the backbone of many recommendation engines. MF models were also one of the first to popularize the notion of embedding in a different context than in text analysis where they were initially used.\n              </p>\n              <img src=\"/assets/img/photos/recommandation.jpeg\" class=\"img-fluid pt-8 pb-8 pl-lg-11 pr-lg-11\" alt=\"...\"/>\n              <p>\n                In a way, recommendation models were (and still are) the perfect candidate to kick off the Data Science effort in a company. For many managers, the fact that a successful company would be willing to give $1M to improve their recommendation by 10% was an eye opener. It turns out that if you have a long tail of items to “sell” to a big audience, that audience's  discovery will be imperfect; if you have some basic analytics it’s pretty easy to quantify the financial impact of having better discovery by introducing a recommendation algorithm in the loop. In that sense, it was the perfect project to sell a new Data Science effort to executives. They could easily see a potential return on investment. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center text-primary-desat\">\n                  “Most successful projects involving machine learning essentially generate their own labels by some clever process, UX mechanic or by leveraging existing datasets”\n                </p>\n              </blockquote>\n              <p>\n                Another interesting aspect was the unsupervised nature of the models. As we are now discovering with modern deep learning models, the requirement for a huge volume of labeled data to train a supervised model can be a big problem for a project. “Classic” Machine Learning used back then was less “greedy” but it was nevertheless problematic. Having to manually label data in order to train and deploy a model in production can put a stop to a DS project in a spectacular fashion if it’s not carefully planned. \n              </p>\n              <p>\n                In my experience, most successful projects involving machine learning essentially “generate their own labels” by some clever process, UX mechanic or by leveraging existing datasets. Unsupervised recommendation models didn’t have that problem. From a product perspective it was “magic”: you fed in your historical data and the model takes care of the rest.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">2013 - 2015</h5>\n              <h2 class=\"font-weight-bold\">\n                Big Data!\n              </h2>\n              <p>\n                A long time ago, before companies like Google, Facebook or LinkedIn even existed,  data meant a very specific thing for a data analyst. If you would ask them back then what was “data”,  they would probably describe something similar to a row in a SQL database. Data is a bunch of rows, with some specific typed field that we can export to a beautiful CSV file and import it in Excel to do some kind of analysis with it. Then came companies like Google, who literally transform data into money. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center text-primary-desat\">\n                  “Then came companies like Google, who literally transform data into money. ”\n                </p>\n              </blockquote>\n              <p>\n                Needless to say that if you’re transforming data into money you’ll probably have a very different perspective of what data is and how to handle it. In 2004, Google published a hugely influential paper titled <a href=\"https://dl.acm.org/doi/abs/10.1145/1327452.1327492\">MapReduce: Simplified Data Processing on Large Clusters</a>. In this paper, Jeffrey Dean and Sanjay Ghemawat described in length the programming model used in one of the earlier versions of the data pipeline used in Google clusters. Technicality aside, the paper was also one of the first sneak peeks into their infrastructure and the philosophy behind it. \n              </p>\n              <img src=\"/assets/img/photos/solvecartoon.jpeg\" class=\"img-fluid pt-8 pb-8 mx-auto d-block\" alt=\"...\"/>\n              <p>\n                The “machine” they described was a completely different beast than most database specialists were using at the time. It was running on a bunch of distributed & cheap computers and the raw data (like web requests logs, crawled documents) were at the center of everything.\n              </p>\n              <p>\n                Apache Hadoop, one of the first open source implementations of the MapReduce approach was the poster child of the <i>Big Data</i> marketing craze at the time.  Around 2013-2015, the hype was in full swing. <span class=\"text-primary\">Exactly like Data Scientist, the term Big Data was a loosely defined term used mostly by non technical people</span>, but like other buzzword it was also useful to promote some interesting ideas. A lot of definitions of Big Data (because there are a lot !)  seem fixated on the volume of the data, but in my mind the core idea behind the concept and the main takeaway is what I call the Diogenes Syndrome Principle:\n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h3 mb-0 text-center text-secondary\">\n                  \"Log everything and, never, ever, delete data.\"\n                </p>\n              </blockquote>\n\n              <p>\n                The idea is that you never know the analysis that you’ll want to do in the future, so you save everything and structure the data later. Another more modern way to articulate this idea is what some refer to as the “log-oriented” architecture or the lambda architecture (which is similar in essence). In log-oriented architecture, like the one popularized by Jay Kreps (the creator of Apache Kafka), we acknowledge that the metrics or the objects we store in a database are in fact the result of a series of events. Those events, if they are saved somewhere and replayed to the system can effectively rebuild all the metrics from scratch. In that sense, the log events are a more fundamental piece of data then the aggregate metrics that we store in the db, so building the data pipeline around them make sense. Most modern data pipelines are now built using this principle with tools like Apache Kafka, Spark, Storm, etc. \n              </p>\n              <p>\n                Of course, for most sizable companies, this principle will lead to a significant volume and variety of data (raw event, text, images, etc..) which will have a significant impact on the engineering aspects... most SQL databases at the time were not equipped to deal with this. That’s why the number of different databases pretty much exploded at the same time. If you google the term <i>NoSQL</i> (yes … another buzzword) you will probably find a bunch of articles written during that inflation era. Most software engineering in the 90s would work mostly with a single type database, a big central SQL (<i>MySQL, Postgres, Oracle,</i> etc..). \n              </p>\n              <p>\n                Now most engineers and data scientists need to handle a wide variety of databases: Full text search databases for text, document-based  databases for documents, and so on covering key-values, time series, columnar, graphs, etc.  A side effect of this explosion of possibilities is what I would call database-FOMO (Fear Of Missing Out) where a bunch of curious engineers in a team can’t help themself…. They need to try the latest db tech out there. \n              </p>\n              <p>\n                There are a lot of things that can slowdown a software project and choosing an exotic database just for the sake of it is a really effective one. And if you want to slow down your project even more… mix them up ! A tip that I’ve found to work in many situations: when in doubt, simply use a good old Postgres database. However in the end, having all those databases for different types of data is obviously a good thing. Fortunately, it looks like we’re back to a more classic SQL way of thinking and lot of those databases (<i>Elasticsearch, Cassandra, Redshift, BigQuery</i>, etc..), even though they are using widely different technologies, are now offering an SQL-like querying layer.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">2010 - 2012</h5>\n              <h2 class=\"font-weight-bold\">\n                GPU Computing & CUDA\n              </h2>\n              <p>\n                My first contact with Data Science was related to GPU Computing. GPU Computing (and CUDA specifically) is all the rage these days, but people would be surprised to learn how \"old\" it is. I started poking around with CUDA around 2009 in order to accelerate a critical computation in my PhD project. In order to validate an hypothesis we had about the behavior of a really intense & short pulse of light propagating in a laser, we needed to numerically solve something called the nonlinear Schrödinger equation. There were well-established methods to do that, but it turns out that solving that equation on a 2009 CPU was pretty time consuming especially if it's part of an optimization process where you need to solve this equation a TON of times. Long story short, reimplementing our code to PyCUDA gave us a HUGE performance improvement. \n              </p>\n              <p>\n                During my journey into this GPU rabbit hole I started doing some consulting around it. My first client was a financial startup in San Diego looking for some help to optimize their computation pipeline using GPU. I decided to put on hold my thesis to work on this... and ... never went back of course :).\n              </p>\n              <p>\n                Going back to CUDA a little bit. GPU computing really had a huge impact on machine learning in the last years, it's essentially the technological backbone that allowed researchers to unlock the power of deep learning. Around 2007, I remember watching a talk by Geoffrey Hinton about restricted Boltzmann machines and deep autoencoders (on Google Video ... Youtube was still mostly about cat videos at the time). I was really impressed, but it became clear pretty quickly that this kind of model was really CPU intensive and worked mostly with tiny images in toy datasets. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center  text-primary-desat\">\n                  \"GPUs are essentially the technological backbone that allowed researchers to unlock the power of deep learning.\"\n                </p>\n              </blockquote>\n              <p>\n                In 2010, I also started talking with a research group at Columbia University using something called a \"convolutional neural network\" to detect houses in satellite imagery. Their model, trained on CPU, was taking weeks to train. We discussed the possibility of maybe accelerating this training using this new thing called CUDA but to be honest it was slightly over my head at the time. But this idea of using a GPU to train CNN turns out to be a great one. Most deep learning researchers and practitioners point out the beginning of the current \"deep learning craze\" to the year 2012, when Krizhevsky and Sutskever, two students in Hinton's research group, published their paper about AlexNet: a GPU-trained CNN that  became the State-of-the-art on Imagenet.\n              </p>\n              <p>\n                Fast forward to today, some people are slightly nervous about the fact that  most implementations of deep learning models are working exclusively on top of proprietary software like CUDA. To this day, there is no serious support for open technology like OpenCL in the most popular DL framework and when we read discussions in the GitHub issues of Tensorflow and PyTorch, it looks like it's not at all a priority for a number of reasons. \n              </p>\n              <p>\n                It's a bummer but we have to give Nvidia some credit. A decade ago, Deep Learning was not a thing yet and most of the other companies like AMD and Intel didn't care about scientific computing, at least in their consumer offering easily accessible to graduate students. When CUDA was first released, AMD didn't have any serious equivalent and Intel certainly had their MKL library but it was not freely available. So Nvidia made a huge bet at the time and they also shifted their product really quickly to answer this new emerging market around Deep Learning. Today, an increasing number of libraries used in machine learning are GPU accelerated and it is fair to say that it’s one of the big shifts that happened in the last decade.\n              </p>\n              <hr class=\"hr-md mt-7 mb-4\"/>\n              <h5 class=\"text-uppercase mt-8 text-info\">2012 - Now</h5>\n              <h2 class=\"font-weight-bold\">\n                The rise of Deep Learning & AI\n              </h2>\n              <p>\n                People into machine learning will know that a lot of the fundamental works and ideas currently used in Deep Learning had their root in the 2000s or even in the 90s. But I think it’s fair to say the last decade was really where things exploded and where DL had the most impact for the DS community. In 2013-2014, right after the first splash made by AlexNet, we could already feel the excitement, but DL was still an academic affair. Just installing Theano or Caffee, the two most popular DL libraries at the time, was not a straightforward task and training a model from scratch on ImageNet was a feat in itself.\n              </p>\n              <p>\n                It was definitely working, but the engineering aspect was so hazardous that it was really hard to see how you could leverage technologies like this in a production environment.  With the first release of Tensorflow (and Keras), things changed pretty quickly. \n              </p>\n              <p>\n                It was now possible to experiment with exotic neural network architecture more easily and more quickly. More importantly, deep learning researchers started using Tensorflow massively and this had a big impact for the commercial applications of deep learning. \n              </p>\n              <p>\n                It meant that researchers and machine learning engineers, implementing those solutions into real products, now had the same common language. \n              </p>\n              <p>\n                To this day, the rapidity at which ideas flow from research directly to commercial applications is one of the distinctive aspects of the Deep Learning / AI community. Part of it comes from the common tools (Tensorflow, PyTorch, Scikit-Learn, etc.) used by the researchers and the machine learning engineers working in companies.\n              </p>\n              <p>\n                This point reminds me of the value of good software engineering. For a Data Scientist, it’s often tempting to define yourself as “not an engineer”... I tried that in the past and it didn’t end well. It’s true that the main job of a DS is not always to put software directly in production, but in years of doing this I never encountered a successful machine learning or data science  project without having the DS themself taking care of a bunch of the engineering aspects. A training or an inference pipeline is an intricate process that can be broken in millions of different ways.The only way to move a project forward, into a real world application, is with the original Data Science or Machine Learning team taking care of the entire loop at least once. There is no such thing as “doing research” and passing it on to the engineering team… I never encountered this successfully in my entire life.\n              </p>\n              <p>\n                Another big trend in the last decade was the rise and the dominance of the PyData stack. Years ago using Python to do your scientific computation was more a matter of taste (or even faith!) than a logical decision. The main appeal was the language itself, the numpy/scipy libraries and the ability to bind legacy code (in Fortran or C) directly in a Python module. \n              </p>\n              <blockquote class=\"blockquote mb-7 mt-7\">\n                <p class=\"h2 mb-0 text-center text-primary-desat\">\n                  \"Years ago using Python to do your scientific computation was more a matter of taste (or even faith!) than a logical decision.\"\n                </p>\n              </blockquote>\n              <p>\n                Fast forward today, Python is THE dominant language in Data Science and Machine Learning and by a big margin. The entire PyData stack is a big ecosystem of libraries developed mostly independently by a bunch of people. Each one of them is the result of a team (or often a single person!) who decided to build something that could be used reliably by other people. \n              </p>\n              <p>\n                Building a sophisticated model or analysis is really cool, but when you can really trust it and use it in another software, model or analysis, then there is a huge compounding effect. I think this is the lessons we can learn from all those components we are using everyday: <i>numpy, scipy, pandas, scikit-learn, tensorflow, pytorch</i>, etc. Good software engineering is key.\n              </p>\n              <h3 class=\"font-weight-bold mt-8\">\n                Ok… but what’s the difference between Deep Learning and AI again???\n              </h3>\n              <p>\n                When I was still an undergraduate, one of the first books I read about the history of AI was <a href=\"https://www.amazon.ca/Ai-Tumultuous-History-Artificial-Intelligence/dp/0465029973\">AI: The Tumultuous History of the Search for Artificial Intelligence</a> by Daniel Crevier. This is a truly excellent book about the history of what we now call the “Good old Fashioned AI”. \n              </p>\n              <p>\n                Starting with the Dartmouth Workshop, the book goes through the birth of the discipline, Newell & Simon work on <i>Logic Theorist</i>, the perceptron, symbolic systems, expert systems etc. The book explains in great detail the tumultuous history of the AI field punctuated by a number of hype and disillusionment phases. After reading this you can understand why the word <i>artificial intelligence</i> was pretty much a taboo word for a number of years in many academic circles. It was tainted by a lot of unrealistic promises so most researchers decided to depart a little bit from that grand vision of automating complex human tasks. \n              </p>\n              <p>\n                During those years, researchers were essentially continuing doing all sorts of interesting things, but using different names: <i>computer vision, operational research, optimization, natural language processing, reinforcement learning,</i> etc. Before 2015, you would very rarely hear the term AI. But for some reason, the success of deep learning brought back that word to life. \n              </p>\n              <p>\n                It was actually quite strange to witness but I think it simply reflected the optimism of researchers and  machine learning practitioners that maybe this new approach, used in conjunction with the arsenal of modern computer science, could really lead to the automation of tasks that were impossible to do previously, and it turns out it was the case. This inflection didn’t happen immediately. But when deep learning started to be applied to a broader range of problems (<i>sentence-to-sentence</i> model in NLP, <i>Deep Q Network</i> in reinforcement learning, etc.) we clearly started to see the shift happen.\n              </p>\n              <h3 class=\"font-weight-bold mt-8\">\n                So what does this all mean for Data Science?\n              </h3>\n              <p>\n                There is common misconception in non-technical audiences (and even some technical audiences for that matter), about the current state of AI and the role played by deep learning models in those systems. Current deep learning models are really really good in perception tasks and pattern recognition. In most applications, deep learning models are used for detection, perception or pattern recognition.\n              </p>\n              <p>\n                More broadly speaking, they are really good at learning useful representations. After the first “perception” step, a simple prediction is usually done (classify an image, turn a wheel, find the most likely words, etc.) and then the traditional arsenal of modern computer science usually kicks-in. In most cases, statistical modelling is also required. This is true for self-driving cars, AI applied to health, fraud detection, etc. Hopefully, it turns out that there is a LOT of tasks where the automation is actually limited by this perception layer. So in those cases, current AI systems are really effective.\n              </p>\n              <p>\n                Current deep learning models require a ton of data. In a sense, they are the perfect complement to modern big data systems: They can digest an enormous amount of data and make sense of it. The latest <i>Transformer</i> models are the best example of this process. Language models, like BERT, ELMo, and GPT2 seem to have an infinite appetite for data. Every month we see  another paper on arxiv about a new transformer model trained on an even bigger dataset and the trend is pretty clear: The more the better! <i>Convolution Neural Networks</i> seem to have the same infinite appetite.\n              </p>\n              <p>\n                So does it mean we can only use deep learning models when we have a ton of labeled data? Well not exactly. Yes the vision model embedded in a Tesla car requires a ton of pre-annotated images to effectively detect cars, pedestrians and dogs. But once this model is trained and deployed into the wild it actually <strong class=\"text-info\">generates data and metrics</strong> that would have been impossible to gather otherwise. And the same is true for a lot of other applications.\n              </p>\n              <p>\n                For example, once you have an effective pose estimation model for humans, you can use it to gather a ton of other metrics in the live feed of a soccer game. This is the area of application where current neural networks are really good, and this is where there is the greatest misconception. Current neural networks models do not make complex human-like decisions, they make simple decisions and actually generate new data that can be analysed to make better complex decisions.\n              </p>\n              <p>\n                Exactly like the big data systems of some years ago gave us a new definition of data, new AI systems expand our definition of what is “data”. A raw feed of  log events in the 80s was not really a useful source of data because no system was powerful enough to ingest it into a simple row in a database. Modern data pipelines are now entirely based on log events. A raw video feed in the 90s was not a useful source of data for the same reason, but now we have models that give us the confidence that we can transform this video into useful metrics.\n              </p>\n              <p>\n                So AI, like all the other buzzwords before it, might fade into disuse, but the concepts and techniques it has brought forth into daily usage will remain with us for a very long time.\n              </p>\n            </div>\n          </div> \n        </div> \n      </section>)\n}","import React from \"react\";\n\n\nexport default function CatherineJoinsTeamPost(props){\n\n  return (<section class=\"pt-0 pt-md-0 pb-0\">\n            <div class=\"container\">\n              <div class=\"row justify-content-center\">\n                <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                  <p>Hectiq.ai team is happy to announce the arrival of Catherine as an Ai Research Scientist. She will work along us to pursue new industrial challenges while bringing balance to the force.\n                  </p>\n                  <img src=\"/assets/img/photos/catherine.jpg\" class=\"img-fluid rounded lift lift-lg mt-8 mb-8 mx-auto d-block w-lg-25\" alt=\"...\"/>\n                  <h5 class=\"text-uppercase mt-8 text-info\">Short Bio</h5>\n                  <p class=\"text-secondary\">\n                    Catherine completed her postgraduate studies at the University de Moncton, specializing in AI-based steganalysis. After acquiring her degree, Catherine worked as an AI researcher in different R&D departments (INO, Zilia), broadening and sharpening her proficiency with multiple frameworks. Her passion brought her naturally to hectiq.ai, where she gets to apply and adapt innovative methods in machine learning toward industrial problematics.\n                  </p>\n                </div>\n              </div> \n            </div> \n          </section>)\n\n}","import React from 'react';\n\n\nfunction Header(props){\n  return (<section class=\"pt-12 pt-md-14 pb-12 pb-md-15 bg-gray-900\" style={{marginTop:-83}}>\n            <div class=\"container\">\n              <div class=\"row justify-content-center\">\n                <div class=\"col-12 col-md-10 col-lg-7 text-center\">\n                  <h1 class=\"display-2 font-weight-bold text-white\">\n                    Blog posts\n                  </h1>\n                  <p class=\"lead text-white-75 mb-4\">\n                    Because we have something to say. \n                  </p>\n                </div>\n              </div>\n            </div>\n          </section>)\n}\n\nexport default Header;\n","import React from 'react';\n\nfunction NavBar(props){\n  return (<nav class={`navbar navbar-expand-lg ${(props.dark)? \"navbar-dark\": \"navbar-light\" }`}>\n            <div class={(props.boxes)? \"container\": \"container-fuild\"}>\n              <a class=\"navbar-brand\" href=\"/\">\n                <img src=\"/assets/img/brand.png\" class=\"navbar-brand-img\" alt=\"...\"/>\n              </a>\n\n              <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n                <span class=\"navbar-toggler-icon\"></span>\n              </button>\n\n              <div class=\"collapse navbar-collapse\" id=\"navbarCollapse\">\n\n                <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n                  <i class=\"fe fe-x\"></i>\n                </button>\n\n                <a class=\"navbar-btn btn btn-sm btn-secondary lift ml-auto\" href=\"https://www.hectiq.ai/\" data-toggle=\"smooth-scroll\" data-offset=\"0\">\n                  Contact\n                </a> \n              </div>\n            </div>\n          </nav>)\n}\n\nexport default NavBar;","import React from \"react\";\n\nfunction truncate(str, length) {\n    return str.length > length ? str.substring(0, length-3) + \"...\" : str;\n}\n\n\nfunction TextCard(props){\n  var colors = {\n    text: \"text-muted\",\n    bg: \"bg-white\",\n    title: \"text-dark\"\n\n  }\n  if (props.dark){\n    colors = {\n      text: \"text-white-50\",\n      bg: \"bg-secondary\",\n      title: \"text-light\"\n    }\n  }\n\n  var Container = \"div\";\n  if (props.content){\n    Container = \"a\";\n  }\n\n  return (<div class=\"col-12 col-sm-4 col-md-4 d-flex\">\n            <Container class={`card lift lift-lg shadow-light-lg mb-7 ${colors.bg}`} href={`/posts/${props.id}`}>\n              {(props.frontImgUrl)?\n                <img class=\"card-img-top mb-0 mt-0\" src={ props.frontImgUrl } alt=\"...\"/>\n              : null} \n              <div class=\"card-body my-auto\">\n                <h6 class={`text-uppercase mb-1 ${colors.text}`}>{props.date}</h6>\n                <h4 class={`mb-0 mb-2 ${colors.title}`}>{ props.title }</h4>\n                <p class={`mb-0 ${colors.text}`}> { truncate(props.excerpt, 100) }</p>\n              </div>\n            </Container>\n          </div>);\n}\n\nexport default TextCard;","import React from \"react\";\n\n\nfunction MainCard(props){\n  var colors = {\n    text: \"text-muted\",\n    bg: \"bg-white\",\n    title: \"text-dark\"\n\n  }\n  if (props.dark){\n    colors = {\n      text: \"text-white-50\",\n      bg: \"bg-secondary\",\n      title: \"text-light\"\n    }\n  }\n  return (<div class=\"col-12\">\n            <a class={`card lift lift-lg shadow-light-lg mb-7 ${colors.bg}`} href={`/posts/${props.id}`}>\n              {(props.frontImgUrl)?\n                <img class=\"card-img-top mb-0 mt-0\" src={ props.frontImgUrl } alt=\"...\"/>\n              : null} \n              <div class=\"card-body my-auto\">\n                <h6 class={`text-uppercase mb-1 ${colors.text}`}>{props.date}</h6>\n                <h4 class={`mb-0 mb-2 ${colors.title}`}>{ props.title }</h4>\n              </div>\n            </a>\n          </div>);\n}\n\nexport default MainCard;","import React from \"react\";\n\n\nexport default function Footer(props){\n  return (\n        <footer class=\"py-0 py-md-0 pb-10 {{ includes.classList }}\">\n          <div class=\"container border-top border-gray-300 pt-8\">\n            <div class=\"row  mb-4\">\n              <div class=\"col-6\">\n                <p class=\"text-gray-700 mb-2\">\n                  A blog by Hectiq.ai\n                </p>\n              </div>\n\n              <div class=\"col-6 ml-auto\">\n                <ul class=\"list-unstyled list-inline list-social mb-6 mb-md-0 text-right\">\n                  <li class=\"list-inline-item list-social-item mr-3\">\n                    <a href=\"https://twitter.com/HectiqAI\" class=\"text-decoration-none\">\n                      <img src=\"/assets/img/icons/social/twitter.svg\" class=\"list-social-icon\" alt=\"...\"/>\n                    </a>\n                  </li>\n                  <li class=\"list-inline-item list-social-item mr-3\">\n                    <a href=\"https://www.linkedin.com/company/10141301/\" class=\"text-decoration-none\">\n                      <img src=\"/assets/img/icons/social/linkedin.svg\" class=\"list-social-icon\" alt=\"...\"/>\n                    </a>\n                  </li>\n                </ul>\n              </div>\n\n            </div>\n          </div>\n        </footer>)\n}","import React from \"react\";\nimport MetaTags from 'react-meta-tags';\n\n\nexport default function Meta({title, ...props}){\n  let url = `https://blog.hectiq.ai/posts/${props.id}`\n\n  return (<MetaTags>\n            <title>{title}</title>\n            <meta property=\"og:site_name\" content=\"Hectiq.AI Blog\"/>\n            <meta property=\"og:type\" content=\"article\"/>\n            <meta property=\"og:title\" content={title}/>\n            <meta property=\"og:image\" content={props.metaImage}/>\n            <meta property=\"og:url\" content={url}/>\n            <meta name=\"twitter:card\" content={props.twitterCard}/>\n            <meta name=\"twitter:title\" content={title}/>\n            <meta name=\"twitter:description\" content={props.twitterDescription}/>\n            <meta name=\"twitter:url\" content={url}/>\n            <meta name=\"twitter:site\" content=\"@HectiqAI\"/>\n            <meta property=\"og:image:width\" content={props.metaImageWidth}/>\n            <meta property=\"og:image:height\" content={props.metaImageHeight}/>\n          </MetaTags>)\n}","import React from \"react\";\n\n\nfunction truncate(str, length) {\n    return str.length > length ? str.substring(0, length-3) + \"...\" : str;\n}\n\n\nfunction RowPost(props){\n  var colors = {\n    text: \"text-muted\",\n    bg: \"bg-white\",\n    title: \"text-dark\"\n\n  }\n\n  var Container = \"div\";\n  if (props.content){\n    Container = \"a\";\n  }\n\n\n  return (<div class=\"list-group-item d-flex align-items-center\">\n          <div class=\"mr-auto\">\n             <h6 class=\"text-uppercase mb-1 text-muted\">{ props.header }</h6>\n              <Container href={`/posts/${props.id}`}><h4 class=\"mb-0\">{ props.title }</h4></Container>\n              <p class={`mb-0 ${colors.text}`}> { truncate(props.excerpt, 120) }</p>\n          </div>\n          <div class=\"badge text-wrap ml-10 text-right\">\n            <p class=\"text-uppercase text-muted\">{ props.date }</p>\n          </div>\n        </div>);\n}\n\nexport default RowPost;","import * as React from \"react\";\n\nimport data from \"../posts/posts.react\";\n\nimport {NavBar, Header, TextCard, MainCard, RowPost, Footer} from \"../components\";\n\nclass HomePage extends React.Component {\n\n  render() {\n    const result = data.filter(o => !o.hide);\n    return (<div>\n              <NavBar boxes dark/>\n              <Header/>\n              <section class=\"py-8 py-md-11 mt-n10 mt-md-n14\">\n                <div class=\"container\">\n                  <div class=\"row\" >\n                    {(result.slice(0,1).map(({date, title, excerpt, ...props})=>{\n                      return (<MainCard date={date} title={title} excerpt={excerpt} {...props} />)\n                    }))}\n                    {(result.slice(1,4).map(({date, title, excerpt, ...props}, i)=>{\n                      return (<TextCard date={date} title={title} excerpt={excerpt} {...props} />)\n                    }))}\n                  </div>\n\n                  <div class=\"list-group list-group-flush mt-6\">  \n                    {(result.slice(4).map(({date, title, excerpt, ...props}, i)=>{\n                      return (<RowPost  date={date} title={title} excerpt={excerpt} {...props} />)\n                    }))}\n                  </div>\n\n                </div>\n              </section>\n\n              \n\n              <Footer/>\n            </div>)\n  }\n}\n\nexport default HomePage;","const metaTags = {\n  voxmedia: {\n    metaImage: \"/assets/img/photos/voxmediacollab-twitter.png\",\n    metaImageWidth: \"915\",\n    metaImageHeight: \"484\",\n    twitterCard: \"summary_large_image\",\n    twitterImage: \"/assets/img/photos/voxmediacollab-twitter.png\",\n    twitterDescription: \"Hectiq.AI completes an exciting collaboration with Vox Media for better understanding their audience. In this post, we discuss what motivated this project, and we explain how we developed a privacy-friendly, crazy fast, and highly effective deep learning model.\",\n    metaDescription: \"Hectiq.AI completes an exciting collaboration with Vox Media for better understanding their audience. In this post, we discuss what motivated this project, and we explain how we developed a privacy-friendly, crazy fast, and highly effective deep learning model.\",\n  },\n}\n\nexport default metaTags;","import {HomePage, PostPage} from \"./pages\"\n\n\n// Define your routes here for breadcrumb\nconst routes = [\n  { path: \"/\", name: \"Home\", strict: false, exact:true, isPrivate:true, Component: HomePage },\n  { path: \"/posts/:postid\", name: \"Home\", strict: false, exact:true, isPrivate:true, Component: PostPage },\n];\nexport default routes;","import * as React from \"react\";\nimport { Redirect } from \"react-router-dom\";\n\n\nimport data from \"../posts/posts.react\";\nimport metaTags from \"../posts/metatags.react\";\n\nimport {NavBar, Footer, Meta} from \"../components\";\n\nclass PostPage extends React.Component {\n  constructor(props){\n    super(props)\n    this.postId = this.props.match.params.postid\n    this.data  = data.find(o=>o.id === this.postId)\n  }\n  render() {\n    if ((this.data==null)||(this.data.content==null)||(this.data.hide)){\n      return (<Redirect to=\"/404\"/>)\n    }\n    return (<div>\n              <Meta title={this.data.title} {...metaTags[this.data.id]}/>\n\n              <NavBar boxes/>\n\n              {(this.data.frontWideImg)? \n                <section data-jarallax data-speed=\".8\" class=\"py-12 py-md-15 py-sm-0 bg-cover jarallax\" style={{backgroundImage: `url(${this.data.frontWideImg})`}}></section>\n                : null}\n\n              {/*Content*/}\n              <section class=\"pt-8 pt-md-11 pb-0\">\n                <div class=\"container\">\n                  <div class=\"row justify-content-center\">\n                    <div class=\"col-12 col-md-12 col-lg-10 col-xl-9\">\n                      <h6 class=\"text-uppercase mb-1 text-muted\">{this.data.date}</h6>\n                      <h1 class=\"display-4 font-weight-bold\">\n                        { this.data.title }\n                      </h1>\n                      {(this.data.by)? <p class=\"text-secondary  \">By {this.data.by}</p> : null}\n                      <p class=\"lead mb-7 text-muted\" dangerouslySetInnerHTML={{__html: this.data.summary}}>\n                      </p>\n                      <hr class=\"hr-md mb-7\"/>\n                    </div>\n                  </div>\n                </div>\n              </section>\n              {this.data.content()}\n\n              {/*Post footer*/}\n              <section class=\"pt-2 pt-md-11 pb-8\">\n                <div class=\"container \">\n                  {((this.data.authors!=null)||(this.data.credits!=null)||(this.data.moreInfo!=null))? <hr class=\"border-gray-300\"/> : null}\n                  <div class=\"list-group list-group-flush border-0 mb-1 mr-lg-12 ml-md-4 \">\n                    {(this.data.authors)? \n                      <div class=\"row\">\n                        <div class=\"col-sm-4 col-md-4 col-lg-2\">\n                          <p class=\"text-muted\">\n                            {(this.data.authors.length>1)? \"Authors\": \"Author\"}\n                          </p>\n                        </div>\n                        <div class=\"col-sm-8 col-md-8 col-lg-10\">\n                          <p class=\"font-size-sm text-muted \">\n                            {this.data.authors.join(', ')}\n                          </p>\n                        </div>\n                      </div>: null}\n\n                      {(this.data.credits)? \n                        <div class=\"row border-top pt-4 pb-4\">\n                          <div class=\"col-sm-4 col-md-4 col-lg-2\">\n                            <p class=\"text-muted \">\n                              Credits\n                            </p>\n                          </div>\n                          <div class=\"col-sm-8 col-md-8 col-lg-10\">\n                            <p class=\"font-size-sm text-muted mb-0\" dangerouslySetInnerHTML={{__html: this.data.credits}}>\n                            </p>\n                          </div>\n                        </div>: null}\n\n                      {(this.data.moreInfo)? \n                        <div class=\"row border-top pt-4 pb-4\">\n                          <div class=\"col-sm-4 col-md-4 col-lg-2\">\n                            <p class=\"text-muted \">\n                              Learn more\n                            </p>\n                          </div>\n                          <div class=\"col-sm-8 col-md-8 col-lg-10\">\n                            <p class=\"font-size-sm text-muted mb-0\" dangerouslySetInnerHTML={{__html: this.data.moreInfo}}>\n                            </p>\n                          </div>\n                        </div>: null}\n                  </div>\n                </div>\n              </section>\n\n              <Footer/>\n\n            </div>)\n  }\n}\n\nexport default PostPage;","import * as React from \"react\";\nimport PropTypes from 'prop-types';\n\nclass ErrorPage extends React.Component {\n\n  render() {\n    return (<div class=\"container d-flex flex-column\">\n              <div class=\"row align-items-center justify-content-center no-gutters min-vh-100\">\n                <div class=\"col-12 col-md-5 col-lg-4 py-8 py-md-11\">\n                  \n                  <h1 class=\"display-3 font-weight-bold text-center\">\n                    {this.props.title}\n                  </h1>\n\n                  <p class=\"mb-5 text-center text-muted\">\n                    {this.props.description}\n                  </p>\n\n                  <div class=\"text-center\">\n                    <a class=\"btn btn-primary\" href=\"/\">\n                      Back to safety\n                    </a>\n                  </div>\n\n                </div>\n              </div>\n            </div>)\n  }\n}\n\nErrorPage.propTypes = {\n  title: PropTypes.string.isRequired,\n  description: PropTypes.string.isRequired,\n}\n\n\nexport default ErrorPage;","import * as React from \"react\";\nimport ErrorPage from \"./ErrorPage.react\"\n\n\nfunction Error404Page(props){\n  let title = \"You should not be here.\",\n      description = \"This page does not exist\";\n\n  return (<ErrorPage title={title} description={description}/>)\n}\n\n\nexport default Error404Page;","// src/history.js\nimport { createBrowserHistory } from 'history';\nexport default createBrowserHistory();","import React from 'react';\n\nimport { Router, Route, Switch } from \"react-router-dom\";\nimport routes from \"./routes.js\";\n\nimport * as Errors from \"./pages/errors\"\nimport history from './history';\n\n\nclass App extends React.Component {\n\n    constructor(props) {  \n      super(props);\n      this.state = {}\n    };\n\n    render() {\n      \n      return (<Router history={history}>\n                <Switch>\n                  {routes.map(({path,  isPrivate, strict, exact, isDemoShop, Component}={}, key)=> {\n                    return (<Route exact={exact} strict={strict} path={path} render={(props) => {\n                                    return ( <Component {...props} />)}} />)\n                  })}\n                  <Route component={Errors.Error404} />\n                </Switch>\n              </Router>\n      );\n    }\n}\n\n\nexport default App;","// @flow\n\nimport React from \"react\";\nimport ReactDOM from \"react-dom\";\n\nimport App from \"./App.react\";\n\n\n\nconst rootElement = document.getElementById(\"root\");\n\nif (rootElement) {\n  ReactDOM.render(<App/>, rootElement);\n} else {\n  throw new Error(\"Could not find root element to mount to!\");\n}\n"],"sourceRoot":""}